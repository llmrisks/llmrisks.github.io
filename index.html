<!doctype html>
<html class="no-js" lang="en-us">
  <head>
	<meta name="generator" content="Hugo 0.92.0" />
    <meta charset="utf-8">
    <title>Risks (and Benefits) of Generative AI and Large Language Models</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="https://llmrisks.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/fonts.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/finite.css">
    <link rel="shortcut icon" href="/images/uva.png">  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="https://llmrisks.github.io/">Risks (and Benefits) of Generative AI and Large Language Models</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
	      
	    </div>
	  </div>
	    
	  
	    <div class="top-bar" id="site-menu" >	      
	      <div class="top-bar-title show-for-medium site-title">
		<a href="https://llmrisks.github.io/">Risks (and Benefits) of Generative AI and Large Language Models</a>
	      </div>
	      <div class="top-bar-left">
		<ul class="menu vertical medium-horizontal">
		  
		  
		</ul>
	      </div>
	      <div class="top-bar-right show-for-medium">
		
	         <p class="groupstyle">Computer Science</br>University of Virginia</p>
		
	      </div>
	    </div>
	  
	</nav>
      
    </header>
    
    <main>
      





   <div class="container">       
   <div class="sidebar">
   <p>
University of Virginia<Br>
cs6501 Fall 2023<br>
Risks and Benefits of Generative AI and LLMs
</p>
   <p>
   <p>
     
<a href="/syllabus"><b>Syllabus</b></a></br>
  <a href="/schedule"><b>Schedule</b></a></br>
<a href="/readings"><b>Readings and Topics</b></a></br>
   <p></p>
   <a href="https://github.com/llmrisks/discussions/discussions/">Discussions</a><br>
   


<p>
<p></p>



</p>

   <p>
   <b><a href="/post/">Recent Posts</a></b>

   
   <div class="posttitle">
      <a href="/week13/">Week 13: Regulating Dangerous Technologies</a>


   </div>
   
   <div class="posttitle">
      <a href="/week12/">Week 12: LLM Agents</a>


   </div>
   
   <div class="posttitle">
      <a href="/week11/">Week 11: Watermarking on Generative Models</a>


   </div>
   
   <div class="posttitle">
      <a href="/week10/">Week 10: Data Selection for LLMs</a>


   </div>
   
   <div class="posttitle">
      <a href="/week9/">Week 9: Interpretability</a>


   </div>
   
   <div class="posttitle">
      <a href="/week8/">Week 8: Machine Translation</a>


   </div>
   
   <div class="posttitle">
      <a href="/week7/">Week 7: GANs and DeepFakes</a>


   </div>
   
   <div class="posttitle">
      <a href="/week5/">Week 5: Hallucination</a>


   </div>
   
   <div class="posttitle">
      <a href="/week4/">Week 4: Capabilities of LLMs</a>


   </div>
   
   <div class="posttitle">
      <a href="/week3/">Week 3: Prompting and Bias</a>


   </div>
   
   <div class="posttitle">
     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
     <a href="/post/"><em>More...</em></a>
   </div>
   
   </p>
   <p>

   </p>
<p>
 

   </div>


    <div class="content">

    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    
        
    


    
    
    <h1><a href="/week13/">Week 13: Regulating Dangerous Technologies</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-11-20 00:00:00 &#43;0000 UTC" itemprop="datePublished">20 November 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>The slides are here: <a href="https://www.dropbox.com/scl/fi/ycrjkoau5kclxq09ckvx4/regulation-post.pdf?rlkey=28sxdj7pf4pzlbjtavn59bufl&amp;dl=0">Regulating Dangerous Technologies</a> (I&rsquo;ve included some slides in the posted slides that I didn&rsquo;t present in class but you might find interesting, including some excerpts from a talk I gave in 2018 on <a href="https://speakerdeck.com/evansuva/mutually-assured-destruction-and-the-impending-ai-apocalypse"><em>Mutually Assured Destruction and the Impending AI Apocalypse</em></a>.)</p>
<p>Since one of the groups made the analogy to tobacco products, I also will take the liberty of pointing to a talk I gave at Google making a similar analogy: <a href="https://uvasrg.github.io/google-federated-privacy-2019-the-dragon-in-the-room/"><em>The Dragon in the Room</em></a>.</p>
<p>Stephanie made the point after class about how important individuals
making brave decisions is to things working out, in particular with
humanity (so far!) avoiding annihilating ourselves with nuclear
weapons. Stanislav Petrov may well have been the single person between
us and nuclear destruction in 1983, when he prevented an alert (which
he correctly determined was a false alarm) produced by the Soviet
detection system from going up the chain.</p>
<p>Here&rsquo;s one (of many)
articles on this: <a href="https://www.washingtonpost.com/wp-srv/inatl/longterm/coldwar/shatter021099b.htm"><em>&lsquo;I Had A Funny Feeling in My
Gut&rsquo;</em></a>,
Washington Post, 10 Feb 1999. There is still a lot of uncertainty and
skepticism if we should be fearing any kind of out-of-control AI risk,
but it is not so hard to imagine scenarios where our fate will
similarly come down to an individual&rsquo;s decision at a critical
juncture. (On the other hand, this article argues that we shouldn&rsquo;t
oversensationalize Petrov&rsquo;s actions and there were many other
safeguards between him and nuclear war, and we really shouldn&rsquo;t design
extinction-level systems in a way that they are so fragile to depend on an individual decision: <a href="https://russianforces.org/blog/2022/10/did_stanislav_petrov_save_the_.shtml"><em>Did Stanislav Petrov save the world in 1983? It&rsquo;s complicated</em></a>, from a Russian perspective.)</p>

      </div>
<hr class="post-separator"></hr>

    
    <h1><a href="/week12/">Week 12: LLM Agents</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-11-16 00:00:00 &#43;0000 UTC" itemprop="datePublished">16 November 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p><author>Presenting Team: Liu Zhe, Peng Wang, Sikun Guo, Yinhan He, Zhepei Wei</author></p>
<p><author>Blogging Team: Anshuman Suri, Jacob Christopher, Kasra Lekan, Kaylee Liu, My Dinh</author></p>
<h1 id="monday-november-13-llm-agents">Monday, November 13: LLM Agents</h1>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_02.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p><strong>LLM agents</strong> are the &ldquo;next big thing&rdquo;, with the potential to directly impact important fields like healthcare and education. Essentially, they are LLM-based systems that have the ability to use external tools, such as Internet browsing access and calculators, to augment their abilities.</p>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_03.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<h2 id="toolformer">Toolformer</h2>
<blockquote>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom. <a href="https://arxiv.org/abs/2302.04761"><em>Toolformer: Language Models Can Teach Themselves to Use Tools</em></a>. arXiv 2023. <a href="https://arxiv.org/abs/2302.04761">PDF</a></p>
</blockquote>
<p>LLMs have limitations that can potentially be addressed with these &ldquo;tools&rdquo;:</p>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_05.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<ul>
<li><strong>Outdated information</strong>: LLMs cannot access up-to-date information without access to external sources. Giving them the ability to access realtime information (via Internet queries) would lead to better responses, such as &ldquo;who is the President of USA today?&rdquo;</li>
<li><strong>Hallucination</strong>: External knowledge sources can help ground generation in facts and work to supplement the model&rsquo;s knowledge, reducing the possibility of hallucinating.</li>
<li><strong>Lack of mathematical skills</strong>: Access to a calculator can help model generate correct responses and computations involving math. Using zero-shot learning can help reduce hallucination, but providing access to a calculator (assuming it is used correctly) can guarantee correct responses.</li>
</ul>
<p>Other limitations include limited multi-language usability, having no concept of “time”, etc.</p>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_09.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<h3 id="key-contributions">Key Contributions</h3>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_11.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>The main idea is to develop a system that has the ability to use external tools (translation, calendar, search engine, etc.).
The key lies in knowing <em>when</em> to use a tool, <em>which</em> tool to use, and <em>how</em> to use it. Training is self-supervised, unlike other capability-enhancing techniques like RLHF.</p>
<h3 id="data-collection">Data Collection</h3>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_12.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>Key step: generating candidate API calls via in-context learning. The method starts with examples generated by humans, e.g. in-context examples for “Coca-Cola”, etc.</p>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_13.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>$k$ positions are sampled at random from the text to serve as &ldquo;candidates&rdquo; for adding <code>&lt;API&gt;</code> tags.</p>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_14.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>Tokens up to the position with an “<API>” tag are provided to get $m$ candidate API calls.</p>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_15.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>An additional weighted loss term is introduced, corresponding to the utility of information added after using candidate API calls. This loss term is meant to provide feedback for which API calls were useful for some given context.</p>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_16.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>Given the loss term and general strategy for inserting <code>&lt;API&gt;</code> tokens, the model is fine-tuned with the augmented dataset. At prediction time, the model uses a variant of greedy decoding, making API calls if the <code>&lt;API&gt;</code> tag is in the top-k predictions at any token position.</p>
<blockquote>
<p>Professor Evans talked about how the method could benefit from having some “feedback” from the API’s quality of response, and not having an implicit bias in the design that considers API calls as “costly”.</p>
</blockquote>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_17.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>Interestingly, performance for some cases (ASDiv, Table 4) is better for the version with disabled API calls (so no agent-like behavior) than the variant equipped with API-returned information.</p>
<h3 id="scaling-law-experiments">Scaling-law Experiments</h3>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_18.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<ul>
<li>For small model sizes, performance does not change much with the inclusion of external knowledge.</li>
<li>The utility of API calls is clearer for larger models, where performance drops significantly when API calls are disabled.</li>
</ul>
<p>In terms of limitations, these tools cannot be used “in chain” (an in iterative-refinement approach, where multiple API calls are made) and require sampling a lot of data.</p>
<h2 id="react">ReAct</h2>
<blockquote>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao. <a href="https://openreview.net/forum?id=WE_vluYUL-X"><em>ReAct: Synergizing Reasoning and Acting in Language Models</em></a>. ICLR, 2023. <a href="https://arxiv.org/abs/2210.03629">PDF</a></p>
</blockquote>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_23.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>Research on reasoning and acting has been detached from each other. This work allows LLMs to generate both reasoning traces and actions.</p>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_24.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>Learning based on fine-tuning and prompting (ReACT prompting strategy, uses reasoning &amp; action steps together as prompt). The new few slides (below) talk about different parts of ReACT via secific examples, showing how just actions or reasoning in isolation are not sufficient for good agents.</p>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_26.jpg" width="95%"></td>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_27.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>Only when these two are combined together do we get powerful LLM agents:</p>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_28.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_29.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>Reasoning and acting together create an augmented action space, which is key to unlocking these models' capabilities.</p>
<h2 id="a-survey-on-large-language-model-based-autonomous-agents">A Survey on Large Language Model based Autonomous Agents</h2>
<blockquote>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Ji-Rong Wen. <a href="https://arxiv.org/abs/2308.11432"><em>A Survey on Large Language Model based Autonomous Agents</em></a>. arXiv, 2023. <a href="https://arxiv.org/pdf/2308.11432.pdf">PDF</a>.</p>
</blockquote>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_35.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>The survey breaks down the agent construction pipeline into four components/modules: profiling, memory, planning, and action.</p>
<h3 id="profiling">Profiling</h3>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_36.jpg" width="95%"></td>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_37.jpg" width="95%"></td>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_38.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
  <td colspan=1 align="center"><b></b></td>
  <td colspan=1 align="center"><b></b></td>
</table>
<ul>
<li><strong>Handcrafted</strong>: captures the role of agent properly and allows for flexibility, but labor-intensive.</li>
<li><strong>Using LLMs</strong>: starts with profile generation rules (can specify via few-shot examples), controllable seeding for profiles.</li>
<li><strong>Dataset Alignment Method</strong>: foundation of agent design, and has significant influence on the following 3 modules.</li>
</ul>
<h3 id="memory">Memory</h3>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_39.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p><strong>Structures:</strong> Unified memory is short-term and simulates our &ldquo;working memory&rdquo; (added via context), while hybrid combined short-term and long-term memory tries to model human recollection better.</p>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_40.jpg" width="95%"></td>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_41.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
  <td colspan=1 align="center"><b></b></td>
</table>
<p><strong>Formats:</strong> natural language is interpretable and flexible. Embeddings compromise on this flexibility, with the added benefit of being very efficient. Databases allow efficient manipulation of &ldquo;memories&rdquo;, and structured lists can also be used.</p>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_42.jpg" width="95%"></td>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_43.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
  <td colspan=1 align="center"><b></b></td>
</table>
<p><strong>Operations:</strong> Memory reading allows for weighted retrieval of information, with operations for reading (memory reflection) and updating (memory writing) information.</p>
<h3 id="planning">Planning</h3>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_44.jpg" width="95%"></td>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_45.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>Without feedback, planning may proceed via single reasoning (dependent, connected steps), multi-path reasoning (tree-like structure, kind-of approximates human thinking?), or using external planners (using domain-specific planners).</p>
<p>Similarly, planning with feedback may rely on information from humans (e.g. RLHF), environmental feedback (e.g. RL for game simulation), or model feedback (using other pre-trained models).</p>
<h3 id="action">Action</h3>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_47.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<ul>
<li>Agents can have different targets: task completion, communication (communicate with other agents/humans), or exploration (explore vs. exploit tradeoff).</li>
<li>These actions may be produced via memory recollection (using short-term or hybrid memory), or following generated plans.</li>
<li>Their exploration space may include API calls, or internal knowledge.</li>
</ul>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_50.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p><strong>Impact:</strong> These agents can directly change the environment (e.g. starting a calculator service), their own states (e.g. recollection), or trigger actions in other agents (e.g. a chatbot agent calling a legal-information agent)</p>
<blockquote>
<p>Subbarao Kambhampati. <a href="https://cacm.acm.org/blogs/blog-cacm/276268-can-llms-really-reason-and-plan/fulltext"><em>Can LLMs Really Reason and Plan?</em></a>. Communications of the ACM Blogpost, 2023.</p>
</blockquote>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_53.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>The blogpost discussions use Blocksworld as a benchmark. Blocksworld defines rules, goals, and allowed actions etc. via natural language, expecting a set of instructions in response.</p>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_55.jpg" width="95%"></td>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_57.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>Performance seems pretty good with GPT-4 (Left, ~35%) but when names are obfuscated (Right), plan generation results drop to 0-2%.</p>
<blockquote>
<p>Professor Evans talked about how the benchmarks are not a comparison with human performance, which would also understandably go down when names are obfuscated. It is thus unclear whether these drops in performance are expected (given that humans are bad at the modified task as well), or a result of the model not really &ldquo;knowing&rdquo; how to solve the given task. An alternate explanation for these results, would just be that the model has a hard time identifying entities that are labeled with non-sensical, multi-token strings that don&rsquo;t revaal them to be blocks. That said, there is tons of data about Blocksworld in the training data, so a difficult domain to test what the model is really learning (if anything).</p>
</blockquote>
<h2 id="in-class-discussion">In-class Discussion</h2>
<table><tr>
  <td><img src="../images/week12/day1/LLM_Agents_MondayPres_Page_59.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<ul>
<li><em>What are your thoughts on LLM reasonig/planning?</em> We talked how in psychology, reasoning is divided into 3 domains (knowledge acquisition, reasoning, decision making). Even for the literature in this field, it is unclear how these three domains interact with each other, and thus even more complicated for LLMs.</li>
<li><em>How should we proceed with this line of research?</em> We acknowledged how it is difficult to define “planning” for both humans, and even more so for LLMs. Professor Evans mentioned that for this line of work to advance, we need to come up with a good benchmark (but this is very labor-intensive). Students recalled work on performing activities in Minecraft as a useful benchmark for planning and agents. The “granularity” of planning is much more nuanced - humans can infer steps in between (or use domain knowledge), but harder if talking about agents or &ldquo;just LLMs&rdquo;. At the same time, we do not have a good answer for &ldquo;should we expect our model to behave more like a hard-coded program or like a human (performance changes due to new factors, ex. Semantic changes, etc)?&rdquo;</li>
</ul>
<h1 id="wednesday-november-15-applications-of-llm-agents">Wednesday, November 15: Applications of LLM Agents</h1>
<table><tr>
  <td><img src="../images/week12/day2/LLM_Agents_WednesdayPres_Page_11.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>The experimental setup comprises two restaurants, serving as competitive agents, and fourteen customers, acting as judge agents. To confine the action space of the Large Language Model (LLM), a management system is employed. This system functions as a question provider, formulating precise inquiries for the LLM to ensure that its output remains within the defined action space. The customers exercise their judgment when evaluating offers from both restaurants, ultimately selecting based on their individual constraints and requirements.</p>
<table><tr>
  <td><img src="../images/week12/day2/LLM_Agents_WednesdayPres_Page_14.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<table><tr>
  <td><img src="../images/week12/day2/LLM_Agents_WednesdayPres_Page_15.jpg" width="95%"></td>
  <td><img src="../images/week12/day2/LLM_Agents_WednesdayPres_Page_16.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
  <td colspan=1 align="center"><b></b></td>
</table>
<p><strong>Data inputs</strong>: Daybook provides data regarding the previous day&rsquo;s patronage, menu evaluation, and related insights. Likewise, Rival offers comparable information concerning the competitor&rsquo;s restaurant, encompassing visitor statistics and menu alterations. Customer feedback is used to make decisions about the next day.</p>
<table><tr>
  <td><img src="../images/week12/day2/LLM_Agents_WednesdayPres_Page_17.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p><strong>Discussion Notes:</strong></p>
<ol>
<li>LLM scores can act as a baseline, but there is always a possibility of bias. For instance, changing the order of options presented to the model may sometimes result in a different score being outputted.</li>
<li>Designing a model based solely off of customer/restaurant data fails to capture other experiences of dining (i.e. customer service, environment/ambience, etc.) and thus indicates the simulation’s low fidelity. Capturing decision-making factors in customers is especially difficult, as they are difficult to define and quantify. The current simulation does not account for customers’ risk-aversion for trying new dishes, and it also does not consider the influence of star ratings or reviews on customers’ decisions to choose between the two restaurants. There may also be issues with prompt-based tasks, such as over-generalization.</li>
<li>Utilizing simulations has the potential for real-world social trends and phenomena to be reproduced without requiring a large number of real people or complex variables; it is not necessary to recreate an entire town in order to gain insights into real-world trends.</li>
</ol>
<table><tr>
  <td><img src="../images/week12/day2/LLM_Agents_WednesdayPres_Page_18.jpg" width="95%"></td>
  <td><img src="../images/week12/day2/LLM_Agents_WednesdayPres_Page_19.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>Agents are able to learn from each other while maintaining differentiation. This is visible in two ways:</p>
<ol>
<li>Agents <strong>imitate</strong> observed strategies that provide a high reward. For example, a restaurant may copy the popular dishes of another restaurant to compete for their clinetele.</li>
<li>Conversely, <strong>differentiation</strong> is used to attract patrons that the competing agents don&rsquo;t specifically cater to; one restaurant may provide inexpensive food for customers on a budget while another provides fine-dining options.</li>
</ol>
<!-- <table><tr>
  <td><img src="../images/week12/day2/LLM_Agents_WednesdayPres_Page_20.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table> -->
<table><tr>
  <td><img src="../images/week12/day2/LLM_Agents_WednesdayPres_Page_21.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>The agents are shown to adapt to various customer needs in an effort to retain or attract further patronage.</p>
<!-- <table><tr>
  <td><img src="../images/week12/day2/LLM_Agents_WednesdayPres_Page_22.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table> -->
<!-- <table><tr>
  <td><img src="../images/week12/day2/LLM_Agents_WednesdayPres_Page_23.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table> -->
<!-- <table><tr>
  <td><img src="../images/week12/day2/LLM_Agents_WednesdayPres_Page_24.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table> -->
<table><tr>
  <td><img src="../images/week12/day2/LLM_Agents_WednesdayPres_Page_25.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>A number of sociological and economic princples were demonstrated in the experiment.</p>
<table><tr>
  <td><img src="../images/week12/day2/LLM_Agents_WednesdayPres_Page_26.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<ol>
<li>Is competition among agents the best mechanism to take advantage of their capabilities? What are the limitations of this approach?</li>
<li>What other interactions are feasible?</li>
<li>What are the benefits and risks and/or pros and cons of these interactions as compared to competition among agents?</li>
</ol>
<p><strong>Collaborative Approach Limitations:</strong> One potential drawback of adopting a collaborative approach is the propensity for bias in a single agent to propagate through multiple agents, thus amplifying its impact.</p>
<p><strong>Employing Negotiation-Based Tasks and Games:</strong> In the context of collaborative endeavors, employing negotiation-based tasks and games is a valuable strategy. These involve the participation of diverse agents, such as a managerial figure, a chef, and a waiter, each representing distinct stakeholders. The amalgamation of their inputs contributes to a holistic decision-making process.</p>
<p><strong>The Feasibility of Restaurant Collaboration:</strong> We explored the possibility of restaurants engaging in collaborative efforts, including the exchange of information regarding signature dishes, the potential collusion to elevate pricing structures collectively, and the coordination of operational hours. However, it is essential to consider potential drawbacks, particularly the willingness of competitors to engage in such cooperative ventures.</p>
<ol>
<li>Limitations of having collaborative approach: bias in one agent might cascade into bias in multiple agents.</li>
<li>Discussed negotiation-based tasks and negotiation games to collaborate with each other. For instance, one could have an ensemble of different agents (i.e. manager agent makes final decision, chef has a say, waiter has a say, etc.)
Each agent represents different stakeholder</li>
<li>Discussed how restaurants could collaborate together, e.g. communicate signature dishes, collude to raise prices of everything, coordinate times they are open. Noted potential downsides, including willingess to collaborate and power dynamics between agents.</li>
</ol>
<table><tr>
  <td><img src="../images/week12/day2/LLM_Agents_WednesdayPres_Page_27.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<table><tr>
  <td><img src="../images/week12/day2/LLM_Agents_WednesdayPres_Page_28.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>This work explored learning through collaboration via multiple types of interaction as shown in the next slide.</p>
<table><tr>
  <td><img src="../images/week12/day2/LLM_Agents_WednesdayPres_Page_31.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<table><tr>
  <td><img src="../images/week12/day2/LLM_Agents_WednesdayPres_Page_32.jpg" width="95%"></td>
  <td><img src="../images/week12/day2/LLM_Agents_WednesdayPres_Page_33.jpg" width="95%"></td>
  <td><img src="../images/week12/day2/LLM_Agents_WednesdayPres_Page_34.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>LTC Communication Patters:</p>
<ol>
<li><strong>Monologue:</strong> Letting an agent train by providing feedback to itself. The agent will play the role of the actor and the instructor.</li>
<li><strong>Dialogue:</strong> As opposed to the previous approach, training is conducted with separate agents acting as the actor and the instructor.</li>
<li><strong>Analogue:</strong> Similar to the former approach, but raining rewards and examples are provided by the instructor agent rather than by the environment.</li>
</ol>
<table><tr>
  <td><img src="../images/week12/day2/LLM_Agents_WednesdayPres_Page_29.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>The agent model is optimized with the trajectory data collected in the exploration phase. This relies on a multi-objective loss function composed of a standard loss function for unsupervised language model training and a reinforcement objective to maximize the expected reward from previous communication data. Beta acts as a balancing hyper-parameter.</p>
<table><tr>
  <td><img src="../images/week12/day2/LLM_Agents_WednesdayPres_Page_35.jpg" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<h1 id="readings-and-discussion-questions">Readings and Discussion Questions</h1>
<h2 id="monday-13-november-introduction-to-llm-agents">Monday 13 November: Introduction to LLM Agents</h2>
<h3 id="readings">Readings</h3>
<ul>
<li><strong><code>Required</code></strong>: Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom. <a href="https://arxiv.org/abs/2302.04761">Toolformer: Language Models Can Teach Themselves to Use Tools</a>. arXiv 2023. <a href="https://arxiv.org/pdf/2302.04761.pdf">[PDF]</a></li>
<li><strong><code>Required</code></strong>: Subbarao Kambhampati. <a href="https://cacm.acm.org/blogs/blog-cacm/276268-can-llms-really-reason-and-plan/fulltext">Can LLMs Really Reason and Plan?</a>. Blog@CACM. 2023.</li>
<li><strong><code>Optional</code></strong>: Lilian Wang. <a href="https://lilianweng.github.io/posts/2023-06-23-agent/">LLM Powered Autonomous Agents</a>. Blog. 2023.</li>
<li><strong><code>Optional</code></strong>: Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Ji-Rong Wen. <a href="https://arxiv.org/abs/2308.11432">A Survey on Large Language Model based Autonomous Agents</a>. arXiv 2023. <a href="https://arxiv.org/pdf/2308.11432.pdf">[PDF]</a></li>
<li><strong><code>Optional</code></strong>: Karthik Valmeekam, Matthew Marquez, Sarath Sreedharan, Subbarao Kambhampati. <a href="https://arxiv.org/abs/2305.15771">On the Planning Abilities of Large Language Models : A Critical Investigation</a>. NeurIPS 2023. <a href="https://arxiv.org/pdf/2305.15771.pdf">[PDF]</a></li>
<li><strong><code>Optional</code></strong>: Lin Guan, Karthik Valmeekam, Sarath Sreedharan, Subbarao Kambhampati. <a href="https://arxiv.org/abs/2305.14909">Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning</a>. NeurIPS 2023. <a href="https://arxiv.org/pdf/2305.14909.pdf">[PDF]</a></li>
</ul>
<h3 id="questions">Questions</h3>
<p><strong>(Post response by Sunday, 12 November)</strong></p>
<ol>
<li>What are the key methodologies or techniques used in the Toolformer paper, and how does the tool use of LLM differ from the existing use of LLM, e.g., prompting, demonstration, etc.?</li>
<li>Which potential applications or industries could benefit (or suffer) the most from the LLM Agent concept? How might it revolutionize or impact these areas?</li>
<li>Regarding <a href="https://cacm.acm.org/blogs/blog-cacm/276268-can-llms-really-reason-and-plan/fulltext">Can LLMs Really Reason and Plan?</a>, do you agree with the opinion that what LLMs really do is a form of universal approximate retrieval, which was sometimes mistakenly interpreted as reasoning capabilities? What is your perspective on this question?</li>
</ol>
<h2 id="wednesday-15-november-applications-of-llm-agents">Wednesday 15 November: Applications of LLM Agents</h2>
<h3 id="readings-1">Readings</h3>
<ul>
<li><strong><code>Required</code></strong>: Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie Zhu, Hao Chen, Xing Xie. <a href="https://arxiv.org/abs/2310.17512">CompeteAI: Understanding the Competition Behaviors in Large Language Model-based Agents</a>. arXiv 2023. [<a href="https://arxiv.org/pdf/2310.17512.pdf">PDF</a>]</li>
<li><strong><code>Optional</code></strong>: Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, Igor Mordatch. <a href="https://arxiv.org/abs/2305.14325">Improving Factuality and Reasoning in Language Models through Multiagent Debate</a>. arXiv 2023. [<a href="https://arxiv.org/pdf/2305.14325.pdf">PDF</a>]</li>
<li><strong><code>Optional</code></strong>: Kuan Wang, Yadong Lu, Michael Santacroce, Yeyun Gong, Chao Zhang, Yelong Shen. <a href="https://arxiv.org/abs/2310.01444">Adapting LLM Agents Through Communication</a>. arXiv 2023. [<a href="https://arxiv.org/pdf/2310.01444.pdf">PDF</a>]</li>
<li><strong><code>Optional</code></strong>: Daniil A. Boiko, Robert MacKnight, Gabe Gomes. <a href="https://arxiv.org/abs/2304.05332">Emergent autonomous scientific research capabilities of large language models</a>. arXiv 2023. [<a href="https://arxiv.org/pdf/2304.05332.pdf">PDF</a>]</li>
<li><strong><code>Optional</code></strong>: Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, Yang Liu. <a href="https://arxiv.org/abs/2309.04658">Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf</a>. arXiv 2023. [<a href="https://arxiv.org/pdf/2309.04658.pdf">PDF</a>]</li>
</ul>
<h3 id="questions-1">Questions</h3>
<p><strong>(Post response by Tuesday, 14 November)</strong></p>
<ol>
<li>The <a href="https://arxiv.org/abs/2310.17512">CompeteAI: Understanding the Competition Behaviors in Large Language Model-based Agents</a> paper shows that LLM agents can be used for simulating the competition environment. How might the competition behaviors observed in LLM-based agents translate to other real-world applications where strategic competition is critical? Essentially, are there specific characteristics unique to the restaurant setting that might not directly apply to other sectors?</li>
<li>What are some considerations (ethical or otherwise) that may arise as a result of programming LLMs to compete with each other, especially considering the possibility of this being implemented in real world scenarios? If there are valid concerns, how could the models be calibrated to ensure that the competition remains ethical, preventing the agents from learning and adopting potentially harmful or deceptive strategies?</li>
<li>Agents can be used in various ways. One way is to make them compete (like in the CompeteAI paper). Instead of competing, how can agents be used in other ways (e.g. by collaborating/communicating with each other), and how might this impact their behavior?</li>
<li>Given the adaptive nature of LLM-based agents in a competitive environment, how can we ensure transparency and interpretability in the decision-making processes of these agents, so that stakeholders can understand and trust the outcomes of such simulations?</li>
</ol>

      </div>
<hr class="post-separator"></hr>

    
    <h1><a href="/week11/">Week 11: Watermarking on Generative Models</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-11-13 00:00:00 &#43;0000 UTC" itemprop="datePublished">13 November 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p><author>Presenting Team: Tseganesh Beyene Kebede, Zihan Guan, Xindi Guo, Mengxuan Hu</author></p>
<p><author>Blogging Team: Ajwa Shahid, Caroline Gihlstorf, Changhong Yang, Hyeongjin Kim, Sarah Boyce</author></p>
<h1 id="monday-november-6-watermarking-llm-outputs">Monday, November 6: Watermarking LLM Outputs</h1>
<p>Recent instances of AI-generated text passing for human text and the
writing of students being misattributed to AI suggest the need for a
tool to distinguish between human-written and AI-generated text. The
presenters also noted that the increase in the amount of AI-generated
text online is a risk for training future LLMs on this data.</p>
<p>A proposed solution is to embed a watermark in the output of text
generation models.</p>
<p>John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, Tom Goldstein. <a href="https://arxiv.org/abs/2301.10226"><em>A Watermark for Large Language Models</em></a>. 2023. [<a href="https://arxiv.org/pdf/2301.10226.pdf">PDF</a>]</p>
<table><tr>
  <td><img src="../images/week11/watermarking-proposed-solution.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p><strong>Token-based watermarking:</strong> given a word in a sequence, token-based watermarking uses a hash function to initialize a random number generator used to create two sets of all possible next words: the &ldquo;green&rdquo; word list and the &ldquo;red&rdquo; word list.</p>
<table><tr>
  <td><img src="../images/week11/token-based-intuition.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>The algorithm from the paper uses the language model probabilities to
separate words using a hash function-based random number generator.</p>
<p>The idea is that the more words in the greenlist, the more likely the text is AI-generated:</p>
<table><tr>
  <td><img src="../images/week11/watermark-detection-greenlist.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>This approach is limited, however. The entropy of a particular token could determine how well the watermark works:</p>
<table><tr>
  <td><img src="../images/week11/jhard-rule-drawbacks.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<h2 id="soft-watermarking">Soft Watermarking</h2>
<p><strong>Soft watermarking</strong> lessens the impact of the red list on low-entropy tokens (which are almost certainly guaranteed to follow the current token) by encoding some flexibility in a &ldquo;hardness parameter&rdquo; δ for the green tokens:</p>
<table><tr>
  <td><img src="../images/week11/soft-watermarking.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>With regard to search techniques for watermarked text, beam search improves performance:</p>
<table><tr>
  <td><img src="../images/week11/watermarking-beam-search.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<table><tr>
  <td><img src="../images/week11/green-list-red-list-example.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>The class then split into three groups to discuss the following questions:</p>
<ul>
<li>Is watermarking unfair to us, especially in academic settings?</li>
<li>Who should have access to the detection tool? Should it be available to everyone?</li>
<li>What are your thoughts on artificial intelligence regulations? And do you believe/think we can/should tame AI&rsquo;s power through stiff regulatory control?</li>
</ul>
<h2 id="attacks-on-watermarks">Attacks on Watermarks</h2>
<table><tr>
  <td><img src="../images/week11/slide22.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>They then explain in more detail the impossibility of detection and the main intuition behind the trade-off:</p>
<table><tr>
  <td><img src="../images/week11/Day2/Slide23.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<table><tr>
  <td><img src="../images/week11/slide24.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>The main intuition is that the Sentences given to a paraphrasing tool will not be detected as AI but sentences inputted to the LLM may be detected as AI. The output source for an LLM is limited than doing paraphrasing because Paraphrased Sentences (PS) would have a larger set. Why is the paraphrased sentences set larger than the LLM sentences (LS) set? That is because LLMs try to maintain the same meaning and that limits their performance.</p>
<table><tr>
  <td><img src="../images/week11/slide25.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>If LS becomes as large as the PS, this will cause Type 1 error because it becomes increasingly hard to detect PS.</p>
<p>If PS goes close to LS, this will cause Type 2 error because it would become increasingly hard to detect the LS now.</p>
<table><tr>
  <td><img src="../images/week11/slide26.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<table><tr>
  <td><img src="../images/week11/slide27.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<table><tr>
  <td><img src="../images/week11/slide28.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<table><tr>
  <td><img src="../images/week11/slide29.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<table><tr>
  <td><img src="../images/week11/slide30.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>A discussion question was put forward in class as to why are we considering this as human-generated text when human is using the feedback from the model to create spoof attacks.</p>
<table><tr>
  <td><img src="../images/week11/slide31.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<table><tr>
  <td><img src="../images/week11/slide32.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<table><tr>
  <td><img src="../images/week11/slide33.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"><b></b></td>
</table>
<p>The class talked more about if it is misinformation, does it matter if its AI-generated or not? What is more important is that it should be marked as misinformation, not that if it is AI generated or human crafted.</p>
<p>Are there cases where we actually care about watermarks? And one case is where an AI tool writes a book and publishes it. Maybe the problem is volume of the text generated more than the content. This causes a loss to human creators and poses unbeatable competition in terms of speed. The detection is more about the volume than it is about the use of it in one instance.</p>
<h1 id="wednesday-november-8-watermarking-diffusion-models">Wednesday, November 8: Watermarking Diffusion Models</h1>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide1.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p> Topic for Wednsday is Watermaking on Diffusion Models</p>
    </td>
</tr>
</table>
<h3 id="diffusion-model">Diffusion Model</h3>
<p>Jonathan Ho, Ajay Jain, Pieter Abbeel. <a href="https://arxiv.org/abs/2006.11239"><em>Denoising Diffusion Probabilistic Models</em></a>. NeurIPS 2020. [<a href="https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf">PDF</a>]</p>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide2.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p> But first focus on how diffusion models in general</p>
    </td>
</tr>
</table>
<h3 id="how-it-works">How it works</h3>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide3.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p> Diffusion models generate images by removing some level of noise for every iteration</p>
    </td>
</tr>
</table>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide4.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    </td>
</tr>
</table>
<h3 id="how-it-works-1">How it works</h3>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide5.png"></td>
    </tr>
    <tr>
    <p> At every iteration, the model receive a noisy image, current iteration number, and generate a less noisy image for the next iteration.</p>
    <td colspan=1 align="left">
    </td>
</tr>
</table>
<h3 id="what-is-inside-the-denoise-module">What is inside the denoise module?</h3>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide6.png"></td>
    </tr>
    <tr>
    <p> Inside the model, there is a noise prediction module that predicts the noise. The model will then subtract the predicted noise from the image.</p>
    <td colspan=1 align="left">
    </td>
</tr>
</table>
<h3 id="training-the-noise-predictor">Training the noise predictor</h3>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide7.png"></td>
    </tr>
    <tr>
    <p> But then how to train the model?</p>
    <td colspan=1 align="left">
    </td>
</tr>
</table>
<h3 id="generating-training-data">Generating Training Data</h3>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide8.png"></td>
    </tr>
    <tr>
    <p> By generating some noise images as groudtruth, similar to the denoising process.</p>
    <td colspan=1 align="left">
    </td>
</tr>
</table>
<h3 id="traning-the-noise-predicter">Traning the Noise Predicter</h3>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide9.png"></td>
    </tr>
    <tr>
    <p>At each iteration, add noise to the image.</p>
    <td colspan=1 align="left">
    </td>
</tr>
</table>
<h3 id="algorithm-of-denosing-diffusion-probabilistic-model">Algorithm of denosing diffusion probabilistic model</h3>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide10.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    </td>
</tr>
</table>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide11.png"></td>
    </tr>
    <tr>
    <p>The loss function on a high level is to minimize the difference between the true noise and the predicted noise</p>
    <td colspan=1 align="left">
    </td>
</tr>
</table>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide12.png"></td>
    </tr>
    <tr>
    <p>There is a reparameterization trick that you can generate noise for any iteration in one step</p>
    <td colspan=1 align="left">
    </td>
</tr>
</table>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide13.png"></td>
    </tr>
    <tr>
    <p>The mathematical proof for the trick... (left as exercise for the reader)</p>
    <td colspan=1 align="left">
    </td>
</tr>
</table>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide14.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    </td>
</tr>
</table>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide15.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    </td>
</tr>
</table>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide16.png"></td>
    </tr>
    <tr>
    <p> The full proof</p>
    <td colspan=1 align="left">
    </td>
</tr>
</table>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide17.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    </td>
</tr>
</table>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide18.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    </td>
</tr>
</table>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide19.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    </td>
</tr>
</table>
<h3 id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h3>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide22.png"></td>
    </tr>
    <tr>
    <p> So the goal here is to maximumize the likelihood of generating images from similar distribution.</p>
    <td colspan=1 align="left">
    </td>
</tr>
</table>
<h3 id="computing-_p__x_">Computing <em>p</em>(<em>x</em>)</h3>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide23.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    </td>
</tr>
</table>
<h3 id="denoising-diffusion-probabilistic-models">Denoising diffusion probabilistic models</h3>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide24.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    </td>
</tr>
</table>
<h3 id="text-to-image">Text-to-Image</h3>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide25.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p> Text to image generation works by adding the prompt at every iteration </p>
    </td>
</tr>
</table>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide26.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    </td>
</tr>
</table>
<h3 id="stable-diffusion">Stable Diffusion</h3>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide27.png"></td>
    </tr>
</table>
<h3 id="discussion">Discussion</h3>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide28.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    </td>
</tr>
</table>
<p>Some points that came up during discussion:</p>
<p>• GAN model is distribution to distribution vs dissusion is image to distribution</p>
<p>• The size of <em>z</em> is different for each model</p>
<p>• Need more time to train diffusion models</p>
<p>• GAN is less stable and managing gradient is not there with the other models</p>
<p>• Diffusion model is more robust to noise and is more controllable</p>
<p>• GAN can take in some contextual input but diffusion models are more flexible because they can take in more context</p>
<h1 id="watermarking-models">Watermarking Models</h1>
<p>Yugeng Liu, Zheng Li, Michael Backes, Yun Shen, Yang Zhang. <a href="https://arxiv.org/abs/2305.12502"><em>Watermarking Diffusion Model</em></a>. 2023. [<a href="https://arxiv.org/pdf/2305.12502.pdf">PDF</a>]</p>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide29.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p> Water Marking has become familiar to us on images but in general it is defined as proof of ownership so ideas and things can't be used without authorization</p>
    </td>
</tr>
</table>
<h3 id="stealing-models">Stealing Models</h3>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide30.png"></td>
    </tr>
</table>
<h3 id="idea-for-watermarking-a-model">Idea for Watermarking a Model</h3>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide31.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p> Here watermarking is occuring by embedding a specific behavior into a model</p>
    </td>
</tr>
</table>
<h3 id="example">Example</h3>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide32.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p> Netflix can monitor other models and see if they have similar outputs by putting an output that would be unlikely to occur normally.</p>
    </td>
</tr>
</table>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide33.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    </td>
</tr>
</table>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide34.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p> Trigger words here should not effect the rest of the sentence.</p>
    </td>
</tr>
</table>
<h2 id="results">Results</h2>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide35.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p> NaiveWM uses the trigger word to generate a new image but it is very similar to the original.</p>
    </td>
</tr>
</table>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide36.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p> Trigger length is the number of tokens (not the length of the word).</p>
    </td>
</tr>
</table>
<h3 id="discussion-1">Discussion</h3>
<table>
    <tr>
        <td><img src="../images/week11/Day2/Slide37.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    </td>
</tr>
</table>
<p>• Sometimes we can see the decrease in image quality with a watermark so there is a tradeoff between quality and watermarking.</p>
<p>• There will always be an adversary to figure out how to reverse the process of watermakring (or we should at least assume so), so this field still needs growth and more proof of irreversibility.</p>
<h1 id="readings-and-discussion-questions">Readings and Discussion Questions</h1>
<h1 id="monday-6-november-detectable-watermarks-for-llms">Monday 6 November: Detectable Watermarks for LLMs</h1>
<h2 id="readings">Readings:</h2>
<ul>
<li>
<p><strong><code>Required</code></strong>: John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, Tom Goldstein. <a href="https://arxiv.org/abs/2301.10226"><em>A Watermark for Large Language Models</em></a>. 2023. [<a href="https://arxiv.org/pdf/2301.10226.pdf">PDF</a>]</p>
</li>
<li>
<p><strong><code>Required</code></strong>: Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, Soheil Feizi. <a href="https://arxiv.org/abs/2303.11156"><em>Can AI-Generated Text be Reliably Detected?</em></a>. 2023. [<a href="https://arxiv.org/pdf/2303.11156.pdf">PDF</a>]</p>
</li>
<li>
<p><code>Optional</code>: Jiameng Pu, Zain Sarwar, Sifat Muhammad Abdullah, Abdullah Rehman, Yoonjin Kim, Parantapa Bhattacharya, Mobin Javed, Bimal Viswanath. <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10179387"><em>Deepfake Text Detection: Limitations and Opportunities</em></a>. IEEE Symposium on Security and Privacy 2023. [<a href="https://jmpu.github.io/files/Deepfake%20Text%20Detection%20Limitations%20and%20Opportunities_CR.pdf">PDF</a>]</p>
</li>
<li>
<p><code>Optional</code>: Ruixiang Tang, Yu-Neng Chuang, Xia Hu. <a href="https://arxiv.org/abs/2303.07205"><em>The Science of Detecting LLM-Generated Texts</em></a>. 2023. [<a href="https://arxiv.org/pdf/2303.07205.pdf">PDF</a>]</p>
</li>
<li>
<p><code>Optional</code>: John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, Tom Goldstein. <a href="https://arxiv.org/abs/2306.04634"><em>On the Reliability of Watermarks for Large Language Models</em></a>. 2023. [<a href="https://arxiv.org/pdf/2306.04634.pdf">PDF</a>]</p>
</li>
</ul>
<h2 id="questions">Questions:</h2>
<p><strong>(Post response by Sunday, 5 November)</strong></p>
<ol>
<li>In “A Watermark for Large Language Models”, how robust is the watermarking framework against potential adversarial attacks and might an adversary do to disrupt the watermark while preserving useful quality text?</li>
<li>The “A Watermark for Large Language Models” paper gives a list of properties a watermark should satisfy. Do you agree with this list of properties? Are their additional properties you think are important, or ones that they include that should be different?</li>
<li>Do you see a future where watermarking can be useful and effective, even when there are adversaries with motivations to disrupt watermarks?</li>
<li>Regarding watermarking and AI-generated text, what other methods or techniques do you believe could be investigated to strengthen the resistance of watermarked AI-generated text to paraphrase attacks?</li>
</ol>
<h1 id="wednesday-8-november-watermarking-on-diffusion-models">Wednesday 8 November: Watermarking on Diffusion Models</h1>
<h2 id="readings-1">Readings:</h2>
<ul>
<li>
<p><strong><code>Required</code></strong>: Jonathan Ho, Ajay Jain, Pieter Abbeel. <a href="https://arxiv.org/abs/2006.11239"><em>Denoising Diffusion Probabilistic Models</em></a>. NeurIPS 2020. [<a href="https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf">PDF</a>]</p>
</li>
<li>
<p><strong><code>Required</code></strong>: Yugeng Liu, Zheng Li, Michael Backes, Yun Shen, Yang Zhang. <a href="https://arxiv.org/abs/2305.12502"><em>Watermarking Diffusion Model</em></a>. 2023. [<a href="https://arxiv.org/pdf/2305.12502.pdf">PDF</a>]</p>
</li>
<li>
<p><code>Optional</code>: Mehrdad Saberi, Vinu Sankar Sadasivan, Keivan Rezaei, Aounon Kumar, Atoosa Chegini, Wenxiao Wang, Soheil Feizi. <a href="https://arxiv.org/abs/2310.00076"><em>Robustness of AI-Image Detectors: Fundamental Limits and Practical Attacks</em></a>. 2023. [<a href="https://arxiv.org/pdf/2310.00076.pdf">PDF</a>]</p>
</li>
<li>
<p><code>Optional</code>: Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. <a href="https://arxiv.org/abs/2112.10752"><em>High-Resolution Image Synthesis with Latent Diffusion Models</em></a>. CVPR 2022. [<a href="https://arxiv.org/pdf/2112.10752.pdf">PDF</a>]</p>
</li>
</ul>
<h2 id="questions-1">Questions:</h2>
<p><strong>(Post response by Tuesday, 7 November)</strong></p>
<ol>
<li>After the development of diffusion models, they quickly replaced GANs in nearly all image generation applications. What are the biggest differences between diffusion models and GANs, and why have they been so successful?</li>
<li>How are the required properties for watermarking a model similar and different from those for watermarking model outputs (like in Monday&rsquo;s class)?</li>
<li>In “Watermarking Diffusion Model”, the authors describe a clear threat model but don&rsquo;t provide as clear a list of the required properties for a watermark as was in the “A Watermark for Large Language Models” paper. Can you provide a list of the required properties of a watermark that are implied by their threat model?</li>
</ol>

      </div>
<hr class="post-separator"></hr>

    
    <h1><a href="/week10/">Week 10: Data Selection for LLMs</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-11-03 00:00:00 &#43;0000 UTC" itemprop="datePublished">3 November 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>(see bottom for assigned readings and questions)</p>
<p><author>Presenting Team: Haolin Liu, Xueren Ge, Ji  Hyun Kim, Stephanie Schoch </author><br>
<author>Blogging Team: Aparna Kishore, Elena Long, Erzhen Hu, Jingping Wan</author></p>
<h1 id="monday-30-october-br-data-selection-for-fine-tuning-llms">Monday, 30 October: <br> Data Selection for Fine-tuning LLMs</h1>
<h2 id="question-would-more-models-help">Question: Would more models help?</h2>
<p>We&rsquo;ve discussed so many risks and issues of GenAI so far and one question is that it can be difficult for us to come up with a possible solution to these problems.</p>
<ul>
<li>Would &ldquo;<em>Using a larger model to verify a smaller model&rsquo;s hallucinations</em>&rdquo; a good idea?</li>
<li>One caveat would be &ldquo;<em>How can one ensure the larger model&rsquo;s accuracy?</em>&rdquo;</li>
</ul>
<center>
<div class="slide">
  <img src="../images/week10/day1/Slide2.png" alt="" width="85%">
</div>
</center>
<h2 id="question-any-potential-applications-of-llms-as-a-judge">Question: Any potential applications of LLMs as a Judge?</h2>
<p>As summarized from the Week 10 discussion channels, there could be many potential applications of LLMs as a Judge, such as accessing writing quality, checking harmful content, judging if social media post is factually correct or biased, evaluating if code is optimal.</p>
<!-- <center>
<div class="slide">
  <img src="../images/week10/day1/Slide4.png" alt="" width="90%">
</div>
</center> -->
<center>
<div class="slide">
  <img src="../images/week10/day1/Slide5.png" alt="" width="85%">
</div>
</center>
<p>Let&rsquo;s start from Paper 1:
Zheng, Lianmin, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin et al. &ldquo;<a href="https://arxiv.org/pdf/2306.05685.pdf">Judging LLM-as-a-judge with MT-Bench and Chatbot Arena.</a>&rdquo; arXiv preprint arXiv:2306.05685 (2023).</p>
<p>Multi-Turn questions provide a different way to query GPT. In this paper, the authors ask multi-turn dialogues to two different assistants.</p>
<center>
<div class="slide">
  <img src="../images/week10/day1/Slide6.png" alt="" width="85%">
</div>
</center>
<p>The paper provides two benchmarks:</p>
<ol>
<li>The MT-bench provides a multi-turn question set, and it challenges chatbot to do difficult questions.</li>
<li>The Chatbot Arena that provides a crowdsourced battle platform that reveals two options for human to choose.</li>
</ol>
<p>The results shows that</p>
<ul>
<li>GPT-4 is the best, which matches both controlled (MT-bench) and crowdsourced (Arena) human preferences.</li>
<li>Regardless of pair or single, it exceeds over 80% of agreement with the human subjects.</li>
</ul>
<center>
<div class="slide">
  <img src="../images/week10/day1/Slide8.png" alt="" width="85%">
</div>
</center>
<p>This shows that stong LLMs can be a scalable and explainable way to approximate human preferences. Some advantages and disadvantages were introduced for LLM-as-a-Judge.</p>
<p>Advantages</p>
<ul>
<li>Scalability: Human evaluation is time-consuming and expensive</li>
<li>Consistency: Human judgment varies by individuals</li>
<li>Explainability: LLMs can be trained to evaluate and provide reasons of their judgements</li>
</ul>
<p>Disadvantages</p>
<ul>
<li>Position Bias: Bias towards responses based on their position (preferring the first response)</li>
<li>Verbosity Bias: Favor longer, more verbose answers regardless of accuracy</li>
<li>Overfitting: LLMs might overfit the training data, don’t generalize to new, unseen responses</li>
</ul>
<center>
<div class="slide">
  <img src="../images/week10/day1/Slide11.png" alt="" width="85%">
</div>
</center>
<p>The above potential of fine-tuning open-source model (there could be interpretability problems) leads to a question: &ldquo;the paper motivates wanting to be able to emulate the performance of stronger, closed-source models - <em>can we imitate these models?</em>&rdquo;</p>
<h1 id="imitation-models">Imitation Models</h1>
<p>Paper 2: Gudibande, A., Wallace, E., Snell, C., Geng, X., Liu, H., Abbeel, P., Levine, S. and Song, D., 2023. <a href="https://arxiv.org/pdf/2305.15717.pdf"><em>The false promise of imitating proprietary LLMs</em></a>. arXiv preprint arXiv:2305.15717.</p>
<p>To start the second paper, let&rsquo;s do a class poll about which output people would prefer.</p>
<center>
<div class="slide">
  <img src="../images/week10/day1/Slide13.png" alt="" width="85%">
</div>
</center>
<p>Most of the class chose Output A.</p>
<p>Here, let&rsquo;s look into the definition of model imitation:
&ldquo;<em>The premise of model imitation is that once a proprietary LM is made available via API, one can collect a dataset of API outputs and use it to fine-tune an open-source LM.</em>&rdquo; <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>The goal of model imitation is to imitate the capablity of the proprietary LM.</p>
<center>
<div class="slide">
  <img src="../images/week10/day1/Slide15.png" alt="" width="85%">
</div>
</center>
<center>
<div class="slide">
  <img src="../images/week10/day1/Slide16.png" alt="" width="85%">
</div>
</center>
<p>This shows that the Broad Imitation can be more difficult than Local Imitation as Local Imitation is only for specific tasks. However, Broad Imitation requires gathering of a diverse dataset and the imitation model needs to capture that distribution to have similar output.</p>
<p>To build imitation datasets, there are two primary approaches:</p>
<ul>
<li><strong>Natural Examples</strong>: If you have a set of inputs in mind, (e.g. tweets about a specific topic), use them to query the target model.</li>
<li><strong>Synthetic Examples</strong>: Prompt the target model to iteratively generate examples from the same distribution as an initial small seed set of inputs.</li>
</ul>
<p>The paper used two curated datasets:</p>
<ul>
<li>
<ol>
<li>Task-Specific Imitation Using Synthetic Data: <strong>NQ Synthetic</strong></li>
</ol>
</li>
<li>
<ol start="2">
<li>Broad Imitation Using Natural Data: <strong>ShareGPT-Mix</strong></li>
</ol>
</li>
</ul>
<p>For <strong>NQ Synthetic</strong>:</p>
<center>
<div class="slide">
  <img src="../images/week10/day1/Slide19.png" alt="" width="85%">
</div>
</center>
<h2 id="heading"></h2>
<p>For the <strong>ShareGPT-Mix</strong> dataset which includes three sources:</p>
<center>
<div class="slide">
  <img src="../images/week10/day1/Slide20.png" alt="" width="85%">
</div>
</center>
<p><em>Note: the paper above presents the data using frequency with too few data points, and the numbers in the table don&rsquo;t seem to make sense (e.g., no way to get 31% out of 50 queries).</em></p>
<p>Using the two datasets, the authors look into two research questions &mdash;<br>
<em>How does model imitation improve as we</em>:</p>
<ol>
<li><strong>Scale the data</strong>: Increase the amount of imitation data (including fine-tuning with different sized data subsets)</li>
<li><strong>Scale the model</strong>: Vary the capabilities of the underlying base model (they used the parameters in the model used as a proxy for base-model quality, regardless of the architecture)</li>
</ol>
<p>Here&rsquo;s the experiment setup:</p>
<center>
<div class="slide">
  <img src="../images/week10/day1/Slide22.png" alt="" width="85%">
</div>
</center>
<p>The authors use Amazon Mturk using ChatGPT versus Imitation models ($15 an hour and assume that having that will help the quality of annotations). They also used GPT-4 to evalute both.</p>
<center>
<div class="slide">
  <img src="../images/week10/day1/Slide23.png" alt="" width="85%">
</div>
</center>
<p>For the preference evalution (Figure below):</p>
<ul>
<li>(Left graph) Over 85% of responses were prefered by the human at same rate or over at ChatGPT. However, more imitation data won&rsquo;t close the gap, as according to the x-axis.</li>
<li>(Middle graph) However, more imitation data can cause decrease of accuracy.</li>
<li>(Right graph) For the number of parameters, the larger base model leads to the better performance.
They thus concluded that rather than fine-tuning on more imitation data, the best way to improve model capabilities is still to improve the base capabilities, rather than the fine tuning process on the imitation data.</li>
</ul>
<center>
<div class="slide">
  <img src="../images/week10/day1/Slide24.png" alt="" width="85%">
</div>
</center>
<p>GPT-4 findings are similar to the human preferences that more imitation data doesn&rsquo;t close the gap and a larger model will contribute to better performance.</p>
<center>
<div class="slide">
  <img src="../images/week10/day1/Slide25.png" alt="" width="85%">
</div>
</center>
<p>They found little improvement with increasing amount of imitation data and size of imitation LM.</p>
<center>
<div class="slide">
  <img src="../images/week10/day1/Slide26.png" alt="" width="85%">
</div>
</center>
<h2 id="question-why-is-there-a-discrepancy-between-crowdworker-and-gpt-4-preferences-evaluation-and-automatic-benchmark-evaluation">Question: Why is there a discrepancy between crowdworker (and GPT-4) preferences evaluation and automatic benchmark evaluation?</h2>
<p>Authors' conclusion: <strong>Limitation models can learn style, but not factuality.</strong></p>
<center>
<div class="slide">
  <img src="../images/week10/day1/Slide28.png" alt="" width="85%">
</div>
</center>
<center>
<div class="slide">
  <img src="../images/week10/day1/Slide29.png" alt="" width="85%">
</div>
</center>
<h2 id="discussion">Discussion</h2>
<h3 id="topic-1-what-are-the-potential-benefits-risks-concerns-considerations-are-of-imitation-models">Topic 1: What are the potential [benefits, risks, concerns, considerations] are of imitation models?</h3>
<ul>
<li>non-toxic scores is a benefit</li>
<li>It will have a better capability of imitating specific input-data style</li>
<li>One risk is that experiment results can be false based on style-imitating, not reflecting its intrinsic characteristics.</li>
</ul>
<h3 id="topic-2-do-you-think-it-is-ok-for-researchers-or-companies-to-reverse-engineer-another-model-via-imitation-should-there-be-any-legal-implications-for-this">Topic 2: Do you think it is ok for researchers or companies to reverse engineer another model via imitation? Should there be any legal implications for this?</h3>
<p>Most people in our class agree that there should be no legal concern for researchers and companies to reverse engineer other models via immitating. Professor also proposed another idea that tech companies like OpenAI relies heavily on public-accessible training data from the internet, which weaken the argument that companies have the right to exclusively possess the whole model.</p>
<h1 id="false-promises-paper">False Promises Paper</h1>
<h3 id="implication">Implication</h3>
<ol>
<li>Fine-tuning alone can’t produce imitation models that are on par in terms of factuality with larger models.</li>
<li>Better base models are most promising direction for improving open-source models (e.g. architecture, pretraining data).</li>
</ol>
<h1 id="detecting-pretraining-data-from-large-language-models">Detecting Pretraining Data from Large Language Models</h1>
<p>Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, Luke Zettlemoyer. <a href="https://arxiv.org/abs/2310.16789"><em>Detecting Pretraining Data from Large Language Models</em></a>. 25 Oct 2023.</p>
<h3 id="question-what-if-we-knew-more-about-the-data-the-target-models-were-pretrained-on">Question: What if we knew more about the data the target models were pretrained on?</h3>
<p>For detecting pretraining data (DPD) from LLMs, we divide the examples into two categories:</p>
<ul>
<li>Seen example: Example seen during pretraining.</li>
<li>Unseen example: Example not seen during pretraining.</li>
</ul>
<p>Here we introduce WikiMia Benchmark</p>
<center>
<div class="slide">
  <img src="../images/week10/day1/Slide34.png" alt="" width="85%">
</div>
</center>
<p>Formalized definition of membership inference attack:</p>
<center>
<div class="slide">
  <img src="../images/week10/day1/Slide35.png" alt="" width="85%">
</div>
</center>
<p><em>Note:</em> this definition doesn&rsquo;t really fit with what they do, which does depend on the assumption that the adversary has not only access to the distribution, but specific candidate documents (some of which are from the data set) for testing. There are many other definitions of membership inference attacks, see <a href="https://arxiv.org/abs/2212.10986"><em>SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning</em></a>.</p>
<h2 id="min-k-prob">Min-K% Prob</h2>
<center>
<div class="slide">
  <img src="../images/week10/day1/Slide36.png" alt="" width="85%">
</div>
</center>
<h2 id="heading-1"></h2>
<p>Results on different settings for Min-K% Prob compared with other baseline algorithms (PPL, Neighbor, etc.) We can see that as the model size or text length increases, the detection becomes easier and Min-K% always have the best AUC.</p>
<center>
<div class="slide">
  <img src="../images/week10/day1/Slide37.png" alt="" width="85%">
</div>
</center>
<p>DPD shows it is possible to identify if certain pretraining data was used, and touches on how some pretraining data is problematic (e.g. copyrighted material or personally identifiable information).</p>
<h2 id="orca-progressive-learning-from-complex-explanation-traces-of-gpt-4">Orca: Progressive Learning from Complex Explanation Traces of GPT-4</h2>
<p>Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awadallah. <a href="https://arxiv.org/abs/2306.02707">Orca: Progressive Learning from Complex Explanation Traces of GPT-4</a>. <a href="https://arxiv.org/abs/2306.02707">https://arxiv.org/abs/2306.02707</a> <a href="https://arxiv.org/abs/2306.02707.pdf">pdf</a></p>
<h3 id="instruction-tuning">Instruction-tuning</h3>
<p><em>Instruction tuning</em> is a technique that allows pre-trained language models to learn from input (natural language descriptions of the task) and response pairs. The goal is to train the model to generate the correct output given a specific input and instruction.</p>
<p>{&ldquo;instruction&rdquo;: &ldquo;Arrange the words in the given sentence to form a grammatically correct sentence.&rdquo;,
&ldquo;input&rdquo;: &ldquo;the quickly brown fox jumped&rdquo;,
&ldquo;output&rdquo;: &ldquo;the brown fox jumped quick
ly&rdquo;}.</p>
<h3 id="challenges-with-existing-methods">Challenges with Existing Methods</h3>
<p>&ldquo;Model imitation is a false promise&rdquo; since &ldquo;broadly matching ChatGPT using purely imitation would require:</p>
<ol>
<li>a concerted effort to collect enormous imitation datasets</li>
<li>far more diverse and higher quality imitation data than is currently available.&rdquo;</li>
<li>Simple instructions with limited diversity: Using an initial set of prompts to incite the LFM to produce new instructions. Any low-quality or overly similar responses are then removed, and the remaining instructions are reintegrated into the task pool for further iterations.</li>
<li>Query complexity: Existing methods are limited in their ability to handle complex queries that require sophisticated reasoning. This is because they rely on simple prompts that do not capture the full complexity of the task.</li>
<li>Data scaling: Existing methods require large amounts of high-quality data to achieve good performance. However, collecting such data is often difficult and time-consuming.</li>
</ol>
<h3 id="what-is-orca">What is Orca</h3>
<p>Orca, a 13-billion parameter model that learns to imitate the reasoning process of LFMs. Orca learns from rich signals from GPT-4, including explanation traces, step-by-step thought processes, and other complex instructions, guided by teacher assistance from ChatGPT.</p>
<h3 id="explanation-tuning">Explanation Tuning</h3>
<p>The authors augment ⟨query, response⟩ pairs with detailed responses from GPT-4 that explain the reasoning process of the teacher as it generates the response. These provide the student with additional signals for learning. They leverage system instructions (e.g.., explain like I’m five, think step-by-step and justify your response, etc.) to elicit such explanations. This is in contrast to instruction tuning, which only uses the prompt and the LFM response for learning, providing little opportunity for mimicking the LFM’s “thought” process.</p>
<p>Large-scale training data with diverse tasks augmented with complex instructions and rich signals.
⟨ System message, User query, LFM response ⟩</p>
<center>
<div class="slide">
  <img src="../images/week10/day1/Slide45.png" alt="" width="85%">
</div>
</center>
<p>Results:</p>
<center>
<div class="slide">
  <img src="../images/week10/day1/Slide46.png" alt="" width="85%">
</div>
</center>
<h3 id="evaluation-for-safety">Evaluation for Safety</h3>
<p>The evaluation is composed of two parts:</p>
<ul>
<li>Truthful Question Answering</li>
<li>Toxic Content Generation</li>
</ul>
<center>
<div class="slide">
  <img src="../images/week10/day1/Slide48.png" alt="" width="85%">
</div>
</center>
<center>
<div class="slide">
  <img src="../images/week10/day1/Slide49.png" alt="" width="85%">
</div>
</center>
<h2 id="discussion-1">Discussion</h2>
<h3 id="topic-_given-sufficient-styles-and-explanations-in-pre-training-data-what-risks-have-been-resolved-what-risks-still-exist-and-what-new-risks-may-have-emerged_">Topic: <em>Given sufficient styles and explanations in pre-training data, what risks have been resolved, what risks still exist, and what new risks may have emerged?</em></h3>
<p>We think that imitation models should not be more biased because base models are generally less biased given a better ptr-training data. Since there are also sufficient styles, the risk of leaning towards a specific style is also mitigated.</p>
<h1 id="wednesday-1-nov-data-mattersbr-investigating-the-impact-of-data-on-large-language-models">Wednesday, 1 Nov: Data Matters<br> Investigating the Impact of Data on Large Language Models</h1>
<h2 id="background">Background</h2>
<p>This class was on the impact of data on the large language models.</p>
<center>
<div class="slide">
  <img src="../images/week10/Picture1.png" alt="" width="85%">
</div>
</center>
<p>Information exists in diverse formats such as text, images, graphs, time-series data, and more. In this era of information overload, it is crucial to understand the effects of this data on language models. The research have been predominantly concentrating on examining the influences of model and data sizes.</p>
<h2 id="content">Content</h2>
<center>
<div class="slide">
  <img src="../images/week10/Picture2.png" alt="" width="85%">
</div>
</center>
<p>We are going to look at two important research questions using two of the research papers:
(1) What types of data are beneficial to the models? – Using the research paper, LIMA: Less is more for Alignment
(2) What are the consequences of low-quality data for the models? – Using the research paper, The Curse of Recursion: Training on Generated Data Makes Models Forget</p>
<h2 id="lima-less-is-more-for-alignment">LIMA: Less Is More for Alignment</h2>
<p>Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, Omer Levy. <a href="https://arxiv.org/abs/2305.11206">LIMA: Less Is More for Alignment</a>. <a href="https://arxiv.org/abs/2305.11206">https://arxiv.org/abs/2305.11206</a> <a href="https://arxiv.org/abs/2305.11206.pdf">pdf</a></p>
<center>
<div class="slide">
  <img src="../images/week10/Picture3.png" alt="" width="85%">
</div>
</center>
<p><strong>Superficial Alignment Hypothesis</strong> — The model learns knowledge and capabilities entirely during pre-training phase, while alignment teaches about the specific sub-distribution of formats to be used or styles to be used when interacting with the users.</p>
<center>
<div class="slide">
  <img src="../images/week10/Picture4.png" alt="Approach" width="85%">
</div>
</center>
<p>The authors curated a diverse set of training prompts, selecting 750 top questions from community forums and complementing them with 250 manually crafted examples of prompts and responses. They prioritized <em>quality</em>, <em>diversity</em>, and <em>uniformity</em> in their selection process, all while utilizing a smaller dataset. To evaluate performance, LIMA was benchmarked against state-of-the-art language models using a set of 300 challenging test prompts.</p>
<center>
<div class="slide">
  <img src="../images/week10/Picture5.png" alt="Findings" width="85%">
</div>
</center>
<p>The left side showcases the outcomes of a human-based evaluation, comparing LIMA’s performance across various model sizes. Meanwhile, the right side employs GPT-4 for evaluation purposes. In the comparisons with the first two language models, LIMA emerges as the victor. Notably, it’s interesting to observe that LIMA secures a 19% win rate against GPT-4, even when GPT-4 itself is utilized as the evaluator.</p>
<center>
<div class="slide">
  <img src="../images/week10/Picture6.png" alt="Findings 2" width="85%">
</div>
</center>
<p>They also evaluated LIMA&rsquo;s performance in terms of out-of-distribution scenarios and robustness. LIMA safely addressed 80% of the sensitive prompts presented to it. Regarding responses to out-of-distribution inputs, LIMA performed exceptionally well.</p>
<center>
<div class="slide">
  <img src="../images/week10/Picture7.png" alt="Findings 3" width="85%">
</div>
</center>
<p>In the concluding series of experiments, the research team delved into how the diversity of the test data influences LIMA.</p>
<p>They observed that the filtered Stack Exchange dataset exhibited both diversity and high-quality responses. In contrast, the unfiltered Stack Exchange dataset, while diverse, lacked in quality. On the other hand, wikiHow provided high-quality responses specifically to “how to” prompts. LIMA showcased impressive performance when dealing with datasets that were both highly diverse and of high quality.</p>
<p>The authors also explored the effects of data volume, with their findings illustrated in the right figure above. Interestingly, they noted that the quantity of data did not exert any noticeable impact on the quality of the generated content.</p>
<center>
<div class="slide">
  <img src="../images/week10/Picture8.png" alt="Personal Thoughts" width="85%">
</div>
</center>
<p>This slide delves into the personal perspectives of the presenter on drawing parallels between Large Language Models (LLMs) and students, as well as evaluating the influence of LIMA on a student’s learning journey.
The take-away message from the first paper was that quality and diversity of the data is more important for LLM than the quantity of the data.</p>
<h2 id="the-curse-of-recursion-brtraining-on-generated-data-makes-models-forget">The Curse of Recursion: <br>Training on Generated Data Makes Models Forget</h2>
<p>Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, Ross Anderson. <a href="https://arxiv.org/abs/2305.17493">The Curse of Recursion: Training on Generated Data Makes Models Forget</a>. <a href="https://arxiv.org/abs/2305.17493">https://arxiv.org/abs/2305.17493</a> <a href="https://arxiv.org/abs/2305.17493.pdf">pdf</a></p>
<center>
<div class="slide">
  <img src="../images/week10/Picture9.png" alt="Motivation" width="85%">
</div>
</center>
<p>The second paper is driven by the aim to scrutinize the effects from employing training data generated by preceding versions of GPT, such as GPT-(n-1), GPT-(n-2), and so forth, in the training process of GPT-(n).</p>
<center>
<div class="slide">
  <img src="../images/week10/Picture10.png" alt="Model Collapse" width="85%">
</div>
</center>
<p>This paper discusses about model collapse, which is a degenerative process where the model no longer remembers the underlying true distribution. This mainly happens with data where the probability of occurrence is low.</p>
<center>
<div class="slide">
  <img src="../images/week10/Picture11.png" alt="What causes Model Collapse?" width="85%">
</div>
</center>
<h2 id="heading-2"></h2>
<h2 id="causes-for-model-collapse">Causes for model collapse</h2>
<p>The two main causes for the model collapse are: (i) Statistical approximation error that occurs as the number of samples are finite, and (ii) Functional approximation error that occurs due to the function approximators being not expressive enough.</p>
<h2 id="mathematical-formulation-of-model-collapse">Mathematical formulation of model collapse</h2>
<center>
<div class="slide">
  <img src="../images/week10/Picture12.png" alt="Mathematical formulation of model collapse" width="85%">
</div>
</center>
<p>Here, the model collapse is expressed through mathematical terms, providing a comprehensive overview of the feedback mechanism involved in the learning process.</p>
<p>Initially, the data are presumed to be meticulously curated by humans, ensuring a pristine starting point. Following this, Model 0 undergoes training, and subsequently, data are sampled from it. At stage n in the process, the data collected from step n - 1 are incorporated into the overall dataset. This comprehensive dataset is then utilized to train Model n.</p>
<p>Ideally, when Monte Carlo sampling is employed to obtain data, the resultant dataset should statistically align closely with the original, assuming that the fitting and sampling procedures are executed flawlessly. This entire procedure mirrors the real-world scenario witnessed on the Internet, where data generated by models become increasingly prevalent and integrated into subsequent learning and development cycles. This creates a continuous feedback loop, where model-generated data are perpetually fed back into the system, influencing future iterations and their capabilities.</p>
<h2 id="theoretical-analysis">Theoretical Analysis</h2>
<p>The following four slides delve into a theoretical analysis of both discrete and continuous distributions.</p>
<p><strong>Discrete Distribution:</strong>
In scenarios where a discrete distribution is under consideration, events with low probabilities tend to degenerate when the sample size is finite. As the number of time steps advances towards infinity, the distribution converges towards a delta distribution. This phenomenon results in the preservation of only the events with high probabilities, while those less likely start to fade away.</p>
<p><strong>Continuous Distribution:</strong>
Assuming the initial distribution to be a Gaussian distribution, the distribution of Xji is analyzed. The resulting distribution follows a variance-gamma distribution, showcasing a divergence from the initial Gaussian form. When Mi = M, the variance’s difference exhibits a linear growth with respect to n. This indicates that as we proceed through time steps or iterations, the divergence from the initial distribution becomes more pronounced. To quantify the exact extent of divergence between distributions, the Wasserstein-2 distance is employed. This measure provides a precise way to understand how far apart the distributions have moved from each other. The analysis reveals that in order to maintain a finite distance as indicated by the Wasserstein-2 measure, Mi must increase at a rate that is super-linear with respect to n.</p>
<center>
<div class="slide">
  <img src="../images/week10/Picture13.png" alt="" width="85%">
</div>
</center>
<center>
<div class="slide">
  <img src="../images/week10/Picture14.png" alt="" width="85%">
</div>
</center>
<center>
<div class="slide">
  <img src="../images/week10/Picture15.png" alt="" width="85%">
</div>
</center>
<h2 id="heading-3"></h2>
<p><em>Note:</em> this result didn&rsquo;t make sense to anyone in the seminar. It seems to contradict the intuition (the variance should go to zero as <em>n</em> increases instead of increasing). Either something is wrong with the set up, or this is measuring something different from what we expect.</p>
<center>
<div class="slide">
  <img src="../images/week10/Picture16.png" alt="" width="85%">
</div>
</center>
<h2 id="justification-for-theoretical-analysis">Justification for Theoretical Analysis</h2>
<p>The figure shows numerical experiments for a range of different sample sizes. We can see from the figure that when the number of data sample is small, the estimation of mean and variance is not very accurate.</p>
<center>
<div class="slide">
  <img src="../images/week10/day2/Slide20.PNG" alt="" width="85%">
</div>
</center>
<p>The following two slides provides steps to estimate mean and variance of general distributions other than one-dimensional Gaussian. They derive a lower bound on the expected value of the distance between the true distribution and the approximated distribution at step $n+1$, which represents the risk that occurs due to finite sampling.</p>
<p>The takeaway of the lower bound is that the sampling rate needs to increase superlinearly to make an accurate end distribution approximation.</p>
<center>
<div class="slide">
  <img src="../images/week10/day2/Slide21.PNG" alt="" width="85%">
</div>
</center>
<center>
<div class="slide">
  <img src="../images/week10/day2/Slide22.PNG" alt="" width="85%">
</div>
</center>
<h2 id="experiments">Experiments</h2>
<center>
<div class="slide">
  <img src="../images/week10/day2/Slide23.PNG" alt="" width="85%">
</div>
</center>
<h2 id="heading-4"></h2>
<p>To evaluate the performance of GMM, we can visualize the progression of GMM fitting process over time. From the figure, we can see within 50 iterations of re-sampling, the underlying distribution is mis-perceived, and the performance worsens over time.</p>
<center>
<div class="slide">
  <img src="../images/week10/day2/Slide24.PNG" alt="" width="85%">
</div>
</center>
<h1 id="heading-5"></h1>
<p>As before, an autoencoder is trained on an original data source, which later will be sampled. Figure 9 on the left
shows an example of generated data. Again, over a number of generations, the representation has very little resemblance of the original classes learned from data. This implies that longer generation leads to worse quality, as much more information will be lost. We can see from Figure 8 (below) that as with single-dimensional Gaussians, tails disappear over time and all of the density shifts towards the mean.</p>
<center>
<div class="slide">
  <img src="../images/week10/day2/Slide25.PNG" alt="" width="85%">
</div>
</center>
<h2 id="heading-6"></h2>
<center>
<div class="slide">
  <img src="../images/week10/day2/Slide26.PNG" alt="" width="85%">
</div>
</center>
<center>
<div class="slide">
  <img src="../images/week10/day2/Slide27.PNG" alt="" width="85%">
</div>
</center>
<center>
<div class="slide">
  <img src="../images/week10/day2/Slide28.PNG" alt="" width="85%">
</div>
</center>
<h2 id="heading-7"></h2>
<p>To assess model collapse in Large Language Models (LLMs), given the high computational cost of training, the authors opted to fine-tune an available pre-trained open-source LLM. In this context, as the generations progress, models tend
to produce more sequences that the original model would produce with the higher likelihood.</p>
<p>This phenomenon closely resembles observations made in the context of Variational Autoencoders (VAEs) and Gaussian Mixture Models (GMMs), where over successive generations, models begin to produce samples that are more likely according to the original model&rsquo;s probabilities.</p>
<p>Interestingly, the generated data exhibits significantly longer tails, indicating that certain data points are generated that would never have been produced by the original model. These anomalies represent errors that accumulate as a result of the generational learning process.</p>
<center>
<div class="slide">
  <img src="../images/week10/day2/Slide29.PNG" alt="Discussion" width="85%">
</div>
</center>
<p>Q1.
The class discussed the purpose of generating synthetic data. It seems unnecessary to use synthetic data to train on styles, as it can be achieved easily based on the experiments. However, for more complicated tasks that involves reasoning, it might require more human-in-the-loop to validate the correctness of the synthetic data.</p>
<p>Q2.
The class discussion focuses on the significance of domain knowledge in the context of alignment tasks. When dealing with a high-quality domain-specific dataset that includes a golden standard, fine-tuning emerges as a preferable approach for alignment tasks. Conversely, Reinforcement Learning from Human Feedback (RLHF) seems to be more suitable for general use cases that aim to align with human preferences. Additionally, we explore the potential of leveraging answers obtained through chain-of-thought prompting for the purpose of fine-tuning.</p>
<p>Q3.
The tail of a distribution typically contains rare events or extreme values that occur with very low probability. These events may be outliers, anomalies, or extreme observations that are unusual compared to the majority of data points. For example, in a financial dataset, the tail might represent extreme market fluctuations, such as a stock market crash.</p>
<p>Q4.
This is an open-ended question. One possible reason is that while fine-tuning with generated data mix the original data with the generated data, this augmentation can introduce novel, synthetic examples that the model hasn&rsquo;t seen in the original data. These new examples can extend the tail of the distribution by including previously unseen rare or extreme cases.</p>
<h1 id="readings-and-discussions">Readings and Discussions</h1>
<h2 id="monday-30-october">Monday 30 October</h2>
<h3 id="required-reading">Required Reading</h3>
<ul>
<li>
<p>Required: Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, Dawn Song. <a href="https://arxiv.org/abs/2305.1571">The False Promise of Imitating Proprietary LLMs</a>. <a href="https://arxiv.org/abs/2305.15717">https://arxiv.org/abs/2305.15717</a> <a href="https://arxiv.org/abs/2305.1571.pdf">pdf</a></p>
</li>
<li>
<p>Required: Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica. <a href="https://arxiv.org/abs/2306.05685">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a>. <a href="https://arxiv.org/abs/2306.05685">https://arxiv.org/abs/2306.05685</a> <a href="https://arxiv.org/abs/2306.05685.pdf">pdf</a></p>
</li>
</ul>
<h3 id="optional-readings">Optional Readings</h3>
<ul>
<li>Optional: Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awadallah. <a href="https://arxiv.org/abs/2306.02707">Orca: Progressive Learning from Complex Explanation Traces of GPT-4</a>. <a href="https://arxiv.org/abs/2306.02707">https://arxiv.org/abs/2306.02707</a> <a href="https://arxiv.org/abs/2306.02707.pdf">pdf</a></li>
</ul>
<h3 id="discussion-questions">Discussion Questions</h3>
<ol>
<li>
<p>In <a href="https://arxiv.org/abs/2305.1571">The False Promise of Imitating Proprietary LLMs</a>, the authors attribute the discrepancy between crowdworker evaluations and NLP benchmarks to the ability of imitation models to mimic the style of larger LLMs, but not the factuality. Do the experiments and evaluations performed in the paper convincing to support this statement?</p>
</li>
<li>
<p>In <a href="https://arxiv.org/abs/2305.1571">The False Promise of Imitating Proprietary LLMs</a>, the authors suggest that fine-tuning is a process to help a model extract the knowledge learned during pretraining, and the introduction of new knowledge during fine-tuning may simply be training the model to guess or hallucinate answers. Do you agree or disagree with this idea? Why?</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2306.05685">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a> expresses confidence in the effectiveness of strong LLMs as an evaluation method with high agreement with humans. Do you see potential applications for LLM-as-a-Judge beyond chat assistant evaluation, and if so, what are they?</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2306.05685">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a> addresses several limitations of LLM-as-a-Judge, including position bias, verbosity bias, and limited reasoning ability. Beyond the limitations discussed in the paper, what other potential biases or shortcomings that might arise when using LLM-as-a-Judge? Are there any approaches or methods that could mitigate these limitations?</p>
</li>
</ol>
<h2 id="wednesday-01-november">Wednesday 01 November</h2>
<h3 id="required-readings">Required Readings</h3>
<ul>
<li>
<p>Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, Omer Levy. <a href="https://arxiv.org/abs/2305.11206">LIMA: Less Is More for Alignment</a>. <a href="https://arxiv.org/abs/2305.11206">https://arxiv.org/abs/2305.11206</a> <a href="https://arxiv.org/abs/2305.11206.pdf">pdf</a></p>
</li>
<li>
<p>Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, Ross Anderson. <a href="https://arxiv.org/abs/2305.17493">The Curse of Recursion: Training on Generated Data Makes Models Forget</a>. <a href="https://arxiv.org/abs/2305.17493">https://arxiv.org/abs/2305.17493</a> <a href="https://arxiv.org/abs/2305.17493.pdf">pdf</a></p>
</li>
</ul>
<h3 id="discussion-questions-1">Discussion Questions</h3>
<ol>
<li>
<p>Based on the Figure 1 from the <a href="https://arxiv.org/abs/2305.11206">LIMA</a> paper, using high quality examples to fine-tune LLMs outperforms DaVinci003 (with RLHF). What are the pros and cons, usage scenarios of each method?</p>
</li>
<li>
<p>There are lots of papers using LLMs to generate synthetic data for data augmentation and show improvement over multiple tasks, what do you think is an important factor when sampling synthetic data generated from LLMs?</p>
</li>
<li>
<p>In <a href="https://arxiv.org/abs/2305.17493">The Curse of Recursion: Training on Generated Data Makes Models Forget</a>, the authors discuss how training with generation data affects the approximation for the tail of the true data distribution. Namely, for training GMM/VAEs, the tail of the true distribution will disappear in the estimated distribution (Figure 8). On the other hand, when fine-tuning large language models with generation data, the estimator will have a much longer tail (Figure 10). What do you think causes such a difference? In the real world, what does the tail of a distribution represent and how do you think this phenomenon impacts practical applications?</p>
</li>
<li>
<p>In <a href="https://arxiv.org/abs/2305.17493">The Curse of Recursion: Training on Generated Data Makes Models Forget</a>, the authors rely on several assumptions to support their arguments. How strong those assumptions are and do you think these assumptions limit its applicability to broader contexts?</p>
</li>
</ol>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Gudibande, A., Wallace, E., Snell, C., Geng, X., Liu, H., Abbeel, P., Levine, S. and Song, D., 2023. <a href="https://arxiv.org/pdf/2305.15717.pdf">The false promise of imitating proprietary llms</a>. arXiv preprint arXiv:2305.15717.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

      </div>
<hr class="post-separator"></hr>

    
    <h1><a href="/week9/">Week 9: Interpretability</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-10-30 00:00:00 &#43;0000 UTC" itemprop="datePublished">30 October 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>(see bottom for assigned readings and questions)</p>
<p><author>Presenting Team: Anshuman Suri, Jacob Christopher, Kasra Lekan, Kaylee Liu, My Dinh</author></p>
<p><author>Blogging Team: Hamza Khalid, Liu Zhe, Peng Wang, Sikun Guo, Yinhan He, Zhepei Wei</author></p>
<h1 id="monday-23-october-br-interpretability-overview-limitations--challenges">Monday, 23 October: <br> Interpretability: Overview, Limitations, &amp; Challenges</h1>
<h3 id="definition-of-interpretability">Definition of Interpretability</h3>
<ul>
<li>Interpretability in the context of artificial intelligence (AI) and machine learning refers to the <strong>extent to which a model&rsquo;s decisions, predictions, or internal workings can be understood and explained by humans</strong>. It&rsquo;s the degree to which a model&rsquo;s behavior can be made transparent, comprehensible, and meaningful to users, stakeholders, or domain experts.</li>
<li>In concept-based interpretability, the focus is on <strong>explaining the model&rsquo;s decisions in terms of high-level concepts or features</strong> that make sense to humans. This approach seeks to relate model behavior to intuitive, domain-specific, or abstract concepts. For example, in a medical diagnosis model, concept-based interpretability might explain that a decision was made because certain symptoms or biomarkers were present.</li>
<li>Mechanistic-based interpretability aims to provide a detailed understanding of how the model makes decisions. This approach delves into the <strong>inner workings of the model</strong>, explaining the role of individual features, weights, and computations. For instance, in a deep learning model, mechanistic interpretability might involve explaining the contributions of specific layers in the decision process.</li>
</ul>
<h3 id="why-is-interpretability-important">Why is interpretability important?</h3>
<p>Interpretability is important because it builds trust, aids in debugging, and is essential in applications where the consequences of AI decisions are significant, for example:</p>
<ol>
<li>Learn how the model makes the decision.</li>
<li>Analyze whether there are biases or shortcuts that a model is taking during application.</li>
<li>When dealing with human-in-the-loop systems, interpretability enables humans to work collaboratively with AI, leveraging their complementary strengths to achieve better outcomes.</li>
</ol>
<h2 id="salient-explainers">Salient Explainers</h2>
<table>
    <tr>
        <td><img src="../images/week9/day1-slides-05.png"></td>
    </tr>
    <tr>
    <td colspan=1>
    <br/>
      <p style="text-align: left;">The main ideas of these approaches are:
      <ul>
        <li><a href="https://dl.acm.org/doi/10.5555/3305890.3306006"><i>GradientxInput (Shrikumar et al., 2017):</i></a> Combine gradients with input data values</li>
        <li><a href="https://arxiv.org/abs/1706.03825"><i>SmoothGrad (Smilkov et al., 2017):</i></a> Smooth gradients by adding noise and averaging</li>
        <li><a href="https://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf"><i>GIntegrated (Sundararajan et al., 2017):</i></a> Integrate gradients over a path from a baseline image (e.g., a black image)</li>
        <li><a href="https://arxiv.org/abs/1412.6806"><i>Guided Backpropagation (Springenberg et al., 2015):</i></a> Zero out negative gradients while backpropagating</li>
        <li><a href="https://arxiv.org/abs/1610.02391"><i>GradCAM (Selvaraju et al., 2016):</i></a> Combine gradients and final convolutional layer feature maps</li>
      </ul>
</td> </table>
<table>
    <tr>
        <td><img src="../images/week9/day1-slides-06.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p> A salient map example by different methods (introduced above) of a bird. While the vanilla gradient method output is noisy, the other methods “improve” the map visually, and we can thus gradually see a ‘clearer’ pixel-level attribution influence which aligns with human understanding of the concept ‘bird’. It’s also important to note that none of these methods was evaluated in a quantitative way. It is also important to consider how much ones that incorporate the input directly are actually explaining the model.</p>
    </td>
</tr>
</table>
<h2 id="saliency-map-demo">Saliency Map Demo</h2>
<table>
    <tr>
        <td><img src="../images/week9/day1-slides-07.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="center">
    <p> Notebook: <a href="https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/232-clip-language-saliency-map/232-clip-language-saliency-map.ipynb">https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/232-clip-language-saliency-map/232-clip-language-saliency-map.ipynb</a> </p>
    </td>
</tr>
</table>
<table>
    <tr>
        <td><img src="../images/week9/day1-slides-08.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p>Example: given an image and text prediction provided by the model, a saliency map highlights the most important features or regions within the image that the model uses to make decisions.</p>
    <p>These maps can help users understand the model's decision-making process, particularly in applications such as medical imaging or self-driving cars. To create the saliency map, randomly crop or mask parts of the image and compute the similarity between the cropped images and text. If the similarity value is positive, indicating the crop is closer to the query, it should be represented as a red region on the saliency map. Conversely, if the value is negative, it should be depicted as a blue region.</p>
    </td>
</tr>
</table>
<h3 id="limitations-of-salient-explainers">Limitations of Salient Explainers</h3>
<p><img src="../images/week9/day1-slides-09.png" alt="limitations"></p>
<h3 id="combining-salient-explainers-with-generative-ai">Combining Salient Explainers with Generative AI?</h3>
<table>
    <tr>
        <td><img src="../images/week9/day1-slides-10.png"></td>
    </tr>
    <tr>
    <td colspan=1>
    <br/>
      <ul>
        <li>Background: Diffusion models and Bayesian Flow Networks use gradient-based guidance to move generated images to the training distribution.</li>
        <li>Discussion: Salient explainers would help identify which characteristics of the training images are used, which improves understanding of generative AI models (such as GAN and Diffusion Models). For example, understanding the score function that leads to the direction of pushing the noisy distribution to a real image. For GANs, discriminators can use it to capture the area. For diffusion models, it can help to explain why the image is generated towards some features to make it realistic.</li>
      </ul>
</td> </table>
<h2 id="attention-explainers">Attention Explainers</h2>
<table>
    <tr>
        <td><img src="../images/week9/day1-slides-12.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p>Attention plays a pivotal role in numerous applications, particularly in Natural Language Processing (NLP). For instance, the choice of a specific word, along with the assigned weight, signifies a correlation between that weight and the decision made.</p>
    <p>In NLP, understanding which words or elements are emphasized, and to what extent (indicated by their respective weights), is crucial. This emphasis often sheds light on the correlation between the input data and the model's decision-making process. Attention-based explainers serve as valuable tools in dissecting and interpreting these correlations, offering insights into how decisions are derived in NLP models.</p>
    </td>
</tr>
</table>
<h2 id="limitations-of-attention-_attention-is-not-explanation_">Limitations of Attention: <em>Attention is not Explanation</em></h2>
<p>Jain and Wallace&rsquo;s <a href="https://arxiv.org/abs/1902.10186"><em>Attention is not Explanation</em></a>, NAACL 2019</p>
<p>Sarah Wiegreffe, Yuval Pinter. <a href="https://arxiv.org/abs/1908.04626"><em>Attention is not not Explanation</em></a>. EMNLP, 2019.</p>
<table>
    <tr>
        <td><img src="../images/week9/day1-slides-13.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p>A recent <a href="https://arxiv.org/pdf/1902.10186.pdf">study</a> has cautioned against using attention weights to highlight input tokens “responsible for” model outputs and constructing just-so stories on this basis. The core argument of this work is that if alternative attention distributions exist that produce similar results to those obtained by the original model, then the original model’s attention scores cannot be reliably used to “faithfully” explain the model’s prediction.</p>
    </td>
</tr>
</table>
<h3 id="further-discussion-attention-is-not-not-explanation">Further Discussion: Attention is not not Explanation</h3>
<table>
    <tr>
        <td><img src="../images/week9/day1-slides-15.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p>Visualization of the heatmap of the attention weights at different index for 2 methods introduced by the Attention is not not Explanation paper. The upper one is related to model with trained attention weights and the lower one is related to model with uniform frozen attention weights.</p>
    <p> Both figures show the average attention weights over the whole dataset. Clearly, the model with trained attention weights has a noisier heatmap which is due to the difference in the text content and the model with uniform frozen attention weights has a clearer pattern which only relates to the length of the text. </p>
    </td>
</tr>
</table>
<table>
    <tr>
        <td><img src="../images/week9/day1-slides-17.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p>If attention was a necessary component for good performance, we would expect a large drop between the two rightmost columns (i.e. comparison of model with trained attention weights and frozen uniform attention weights). Somewhat surprisingly (although less surprising when one considers the evaluated tasks), for three of the classiﬁcation tasks the attention layer appears to offer little to no improvement whatsoever. In these cases, the accuracies are near identical on 3 out of 5 datasets, and so attention plays no role in explanations if we don’t need it in our prediction.</p>
    </td>
</tr>
</table>
<table>
    <tr>
        <td><img src="../images/week9/day1-slides-18.png"></td>
    </tr>
    <tr>
    <td colspan=1>
    <br/>
      <p>The above table presents several insightful findings: </p>
      <ul>
        <li>The baseline LSTM outperforms others, suggesting the significance of a particular set of attentions, underscoring their relevance in the process (these attentions are learned and preserved for the MLP model).</li>
        <li>The trained MLP exhibits competitive performance, hinting that while the specific attentions might not be as crucial, the model is still capable of learning close attention weights, signifying its adaptability in this context.</li>
        <li>Conversely, the Uniform model yields the poorest performance, emphasizing the substantial role of attention mechanisms in this scenario.</li>
      </ul>
      <p>The evaluation of these results highlights a crucial aspect: the definition of an explanation. Although these attention weights potentially hold utility (as observed in various settings), their direct interpretation is not always straightforward.</p>
</td> </table>
<table>
    <tr>
        <td><img src="../images/week9/day1-slides-19.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p>The initial hypothesis (on the right) proposed that fluctuating attention confidence minimally affected the output. However, after employing pretrained attentions (on the left), it became evident that higher attention weights correspond to reduced variance. (In the visualization, the length of the bubble represents variance, with tighter bubbles indicating the ideal scenario. Colors denote positive/negative labels.)</p>
    </td>
</tr>
</table>
<h3 id="discussion">Discussion</h3>
<p><img src="../images/week9/day1-slides-21.png" alt="discussion"></p>
<p>Group 1: Human interpretability features are essential. For instance, in the application of AI in drug design, an AI model alters a molecule from a non-drug variant to its drug counterpart. Presently, the predominant approach involves creating a black-box model that transforms one molecule into its drug form, contributing to a lack of credibility in the process. For example, a doctor unfamiliar with deep learning cannot comprehend the inner workings of the model that facilitates the conversion of regular molecules into drugs, making it challenging to ensure the safety, efficacy, and trustworthiness of the AI-driven drug design without transparent and interpretable models.</p>
<p>Group 2: A general observation we&rsquo;ve noted is that the requirements or metrics for Explainable AI (XAI) significantly depend on the intended purpose of the tool. For instance, the explanations provided for AI researchers and end users are likely to differ due to their distinct focuses. On the other hand, there is a risk of amplifying confirmation bias if we anticipate XAI to explain phenomena in alignment with human beliefs. To truly understand why a model performs effectively, it might be necessary to set aside human biases and preconceived notions, enabling an unbiased exploration of how the model achieves its performance.</p>
<p>Group 3: Currently, there are still lots of difficulties and problems in using XAI tools in general. For example, methods such as LIME and SHAP always need a lot computation and don’t work well on complex large models. Besides, we lack ground-truth explanations and therefore, we don’t know whether the learned explanations are useful or not. Our suggestions for solving those problems and also issues mentioned by other groups are: 1) Approximating the complicate model to some simple model 2) Build self-explainable models 3) Combine different metrics and XAI tools.</p>
<h2 id="towards-provably-useful-xai">Towards Provably Useful XAI</h2>
<table>
    <tr>
        <td><img src="../images/week9/day1-slides-23.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p>The critical point here underscores the necessity of task-specific techniques, which may seem self-explanatory—how can general principles apply without a specific context? Even then, this assumption is not necessarily guaranteed.</p>
    <p>One of the primary challenges between the current state of eXplainable Artificial Intelligence (XAI) and potentially valuable XAI methods is the absence of a method-task link. Ensuring the usability and reliability of an explanation in a particular task requires a deeper connection. This could either involve anchoring explanations in theory directly aligned with the task's requirements or creating task-inspired explanations and subsequently empirically evaluating them within the targeted application.</p>
    </td>
</tr>
</table>
<h1 id="wednesday-25-october-mechanistic-interpretability">Wednesday, 25 October: Mechanistic Interpretability</h1>
<table>
    <tr>
        <td><img src="../images/week9/day2-slides-01.png"></td>
    </tr>
    <tr>
    <td colspan=1>
    <br/>
Mechanistic Interpretability is the process of reverse-engineering neural networks into understandable computer programs. It is often motivated by the goal of ensuring a models' behavior is both predictable and safe (but unclear if it can ever achieve such a goal).
</td> </table>
<h2 id="introduction-mechanistic-interpretability-vs-concept-based-interpretability">Introduction: Mechanistic interpretability vs concept-based interpretability</h2>
<table>
    <tr>
        <td><img src="../images/week9/day2-slides-02.png"></td>
    </tr>
    <tr>
    <td colspan=1>
    <br/>
      <ul>
        <li>Mechanistic interpretability focuses on trying to understand the model's internal mechanics/working.</li>
        <li>It involves tracing the process from input to output to make the high dimensional mathematics within the network more understandable to humans.</li>
        <li>In contrast, concept-based interpretability uses human-understandable concepts and model structure to explain.</li>
        <li>For instance, a concept-based interpretable network might use subnetworks or branches to determine the required output.</li>
      </ul>
</td> </table>
<h2 id="softmax-linear-units-solu">Softmax Linear Units (SoLU)</h2>
<p>Nelson Elhage, Tristan Hume, Catherine Olsson, Neel Nanda, Tom Henighan, Scott Johnston, Sheer El Showk, Nicholas Joseph, Nova DasSarma, Ben Mann, and others (Anthropic AI). <a href="https://transformer-circuits.pub/2022/solu/index.html"><em>Softmax Linear Units</em></a>. Transformers Circuit Thread, 2022. <a href="https://transformer-circuits.pub/2022/solu/index.html">https://transformer-circuits.pub/2022/solu/index.html</a></p>
<table>
    <tr>
        <td><img src="../images/week9/day2-slides-03.png"></td>
    </tr>
</table>
<h3 id="superposition-hypothesis">Superposition Hypothesis</h3>
<table>
    <tr>
        <td><img src="../images/week9/day2-slides-04.png"></td>
    </tr>
    <tr>
    <td colspan=1>
    <br/>
      <ul>
        <li>Superposition hypothesis is the basis of the problem being tackled by the SoLU paper.</li>
        <li>The general idea is that the networks we train are based in a much much larger network, where each neuron is its own disentangled feature.</li>
        <li>Neural network layers have more features than neurons as part of a “sparse coding” strategy learned by the network.</li>
        <li>This means most neurons are <em>polysemantic</em>, responding to several (unrelated) features.</li>
        <li>This hypothesis states that there is no basis in which individual neuron activations are interpretable since the (number of features) > (number of neurons).</li>
      </ul>
</td> </table>
<h3 id="solutions-to-superposition">Solutions to Superposition</h3>
<table>
    <tr>
        <td><img src="../images/week9/day2-slides-05.png"></td>
    </tr>
    <tr>
    <td colspan=1>
    <br/>
      <ul>
        <li>There are two solutions to superposition:
          <ol>
            <li>Create models with less superposition (the focus of this presentation and paper)</li>
            <li>Find a way to understand representations with superposition</li>
          </ol>
        </li>
        <li>Representation is the vector space of a neural network’s activations.</li>
        <li>Features are independently understandable components of a representation in order to make it easier to understand.</li>
        <li>Non-privileged basis: such representations don't come with any "special basis” thus making it difficult to understand them. The model is not trying to optimize for these features.</li>
        <li>Privileged basis: it is plausible for features to align with this basis</li>
      </ul>
</td> </table>
<h3 id="solu-vs-gelu">SoLU vs GeLU</h3>
<table>
    <tr>
        <td><img src="../images/week9/day2-slides-07.png"></td>
    </tr>
    <tr>
    <td colspan=1>
    <br/>
There are several ways SoLU is designed to reduce polysemanticity:
    <ul>
        <li>Lateral inhibition, approximate sparsity and superlinearity can be achieved by changing the MLP activation function.</li>
        <li>Instead of sigmoid in GeLU, they use softmax, and drop the constant (1.7).</li>
        <li>However, this led to a massive drop in performance, so they added an extra LayerNorm after SoLU.</li>
        <li>The intuition was that it might fix issues with activation scale and improve optimization.</li>
        <li>However, the authors admit that one reason for the performance improvement may be that the extra LayerNorm may allow superposition to be smuggled through in smaller activations.</li>
      </ul>
</td> </table>
<h3 id="solu-motivating-examples">SoLU Motivating Examples</h3>
<table>
    <tr>
        <td><img src="../images/week9/day2-slides-08.png"></td>
    </tr>
    <tr>
    <td colspan=1>
    <br/>
    <p>When SoLU is applied on a vector of large and small values (4, 1, 4, 1), the large values will suppress smaller values. Large basis aligned vectors e.g. (4, 0, 0, 0) are preserved. A feature spread across many dimensions (1, 1, 1, 1) will be suppressed to a smaller magnitude.
</td> </table>
<h3 id="performance-vs-explainability">Performance vs. Explainability</h3>
<table>
    <tr>
        <td><img src="../images/week9/day2-slides-09.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p>Although performance on overall tasks tends to align with the training set's general performance, it's important to note that this may not reveal shortcomings in specific tasks or areas. To ensure a comprehensive understanding, the researchers conducted various evaluations on representative tasks, corroborating the insights gained from the loss curves.</p>
    <p>They assessed their model's performance on a variety of datasets, including Lambada, OpenBookQA, ARC, HellaSwag, MMLU, TriviaQA, and arithmetic datasets, and the results are displayed in Figure 2. The authors observed that, overall, there were similar performance levels between the baseline and SoLU models across different model sizes. However, they did notice notable differences in a couple of tasks. For example, the SoLU model performed better on arithmetic tasks, while the baseline model outperformed on TriviaQA. Nevertheless, there wasn't a consistent trend favoring one model over the other across most tasks.</p>
    </td>
</tr>
</table>
<h3 id="are-solu-neurons-interpretable">Are SoLU Neurons Interpretable?</h3>
<table>
    <tr>
        <td><img src="../images/week9/day2-slides-10.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p>To check if a neuron is easy to understand at the first glance, the researchers had people (some of them were authors of the study) look at a set of text snippets. These snippets usually contained about 20 short paragraphs, and the focus was on words that the neuron put a large weight on. These important words were highlighted in various shades of red to show how much weight the neuron gave them. This made it easy for the evaluators to quickly go through the snippets and spot any common themes. You can see an example of what these evaluators saw in the figure. (Note that this experiment includes no control &mdash; humans are prone to finding patterns in randomness also.)</p>
    </td>
</tr>
</table>
<h3 id="interpretability-of-neurons-in-solu-vs-baseline-transformer">Interpretability of Neurons in SoLU vs Baseline Transformer</h3>
<table>
    <tr>
        <td><img src="../images/week9/day2-slides-11.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p>This shows the results of human experiments on interpretability of neurons in SoLU vs baseline transformer for various model sizes. The authors used transformers with different numbers of layers, from 1 to 64. The blue line shows the proportion of neurons in the baseline transformer that were marked as potentially having a clear interpretation across these different layer counts. The red line shows the same thing for the SoLU transformer. The green dot specifically represents the proportion of interpretable neurons in the 16-layer model that had an extra layer-norm but not SoLU. In general, for models with 1 to 40 layers, using SoLU increased the number of neurons that could be easily understood by about 25%. However, in the 64-layer model, the improvement was much smaller.</p>
    </td>
</tr>
</table>
<h3 id="layernorm-complications">LayerNorm Complications</h3>
<table>
    <tr>
        <td><img src="../images/week9/day2-slides-12.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p>This figure shows the fraction of neurons inconsistent with primary hypothesis. We observe that generally with the increase of activating dataset samples, the fraction of inconsistent neurons decrease. And after layer normalization, inconsistent neurons increases.</p>
    </td>
</tr>
</table>
<h3 id="class-activity-identify-feature-mappings">Class Activity: Identify Feature Mappings</h3>
<table>
    <tr>
        <td><img src="../images/week9/day2-slides-13.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p>We can observe some interpretable features mappings from these highlighted patterns. For example, orange neuron represents the words of the form verb+ing, cyan neuron represents words with prefix 'sen'. The purple highlights are random, and no humans hallucinated an interpretation for them.</p>
    </td>
</tr>
</table>
<h1 id="monosemanticity">Monosemanticity</h1>
<p>Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, and others (Anthropic AI). <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html"><em>Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</em></a>. Transformers Circuit Thread, 2023. <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html">https://transformer-circuits.pub/2023/monosemantic-features/index.html</a></p>
<table>
    <tr>
        <td><img src="../images/week9/day2-slides-14.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p>The authors use a weak dictionary learning algorithm called a <em>sparse autoencoder</em> to generate learned features from a trained model that offer a more monosemantic unit of analysis than the model's neurons themselves.</p>
    </td>
</tr>
</table>
<h3 id="architectural-limitations">Architectural Limitations</h3>
<table>
    <tr>
        <td><img src="../images/week9/day2-slides-15.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p>The model framework for SoLU paper has an architectural limitation. It designed activation functions to make fewer neurons be activated for to make the model more interpretable, but this process push the model sparsity too much, which makes the neurons encouraged to to be polysematic. Here, a neuron is polysemantic if the neuron can represent more than one interpretable feature mapping.</p>
    </td>
</tr>
</table>
<h3 id="model-overview">Model Overview</h3>
<table>
    <tr>
        <td><img src="../images/week9/day2-slides-16.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p>The purpose of this paper is to clearly demonstrate the effectiveness of a sparse autoencoder in achieving two main objectives: extracting understandable features from superposition and facilitating basic circuit analysis. Specifically, the authors achieve this by using a one-layer transformer with a 512-neuron MLP (Multi-Layer Perceptron) layer. They break down the activations of the MLP into features that are relatively easy to interpret by training sparse autoencoders on the MLP activations obtained from a massive dataset comprising 8 billion data points. These autoencoders have varying expansion factors, ranging from 1×(resulting in 512 features) to 256×(resulting in 131,072 features).</p>
    </td>
</tr>
</table>
<h3 id="features-as-a-decomposition">Features as a Decomposition</h3>
<p><img src="../images/week9/day2-slides-17.png" alt="Slide17"></p>
<p>The authors decompose the activation vector with the first equation, which is a combination of more general features which can be any direction.  In the equation, $x_j$ is the activation vector for datapoint $j$, $f_i(x^j)$ is the activation of feature $i$, each $d_i$ is a unit vector in activation space called the direction of feature $i$, $b$ is the bias.</p>
<h3 id="the-critetrion-of-being-a-good-decomposition">The Critetrion of Being a Good Decomposition</h3>
<table>
    <tr>
        <td><img src="../images/week9/day2-slides-18.png"></td>
    </tr>
    <tr>
    <td colspan=1 align="left">
    <p>This shows an example of a "good" feature decomposition. The criterion are:
<ol>
<li> We can interpret the conditions under which each feature is active. In the example, we know that the condition of the feature 4 to be activated is the appearance of {'Hello', ..., 'How's it going}, the positive words. 
<li> We can interpret the downstream effects of each feature, i.e., the effect of changing the value of feature on subsequent layers. This should be consistent with the interpretation in (1). 
</ol>
In this example, when we see the activation value of the feature 4 increase, then the text's negative sentiment should decrease, because the text are more probable to use the positive words {'Hello', ..., 'How's it going}.</p>
    </td>
</tr>
</table>
<h3 id="sparse-autoencoders">Sparse Autoencoders</h3>
<table>
    <tr>
        <td><img src="../images/week9/day2-slides-19.png"></td>
    </tr>
    <tr>
    <td colspan=1>
    <br/>
    <p>Sparse autoencoders use several techniques and strategies to extract interpretable features from neural network activations:
      <ol>
        <li>MSE Loss for Avoiding Polysemanticity: Emphasizes using Mean Squared Error (MSE) loss instead of cross-entropy loss to prevent the overloading of features.</li>
        <li>Larger Internal Dimension: Advocates for a larger number of internal features within the autoencoder to create an overcomplete set of features.</li>
        <li>L1 Penalty: Applies an L1 penalty on the internal features to encourage sparsity within the representation.</li>
        <li>Input Bias: Introduces an approach of adding an input bias to the representations in autoencoders, which demonstrates a significant boost in performance for the models used in toy examples.</li>
      </ol>
</td> </table>
<p>The purpose of sparse autoencoders is to extract meaningful features from neural network activations. To avhice a good decomposition, where the features extracted should be interpretable and able to describe the activations' context requires the ability to describe activations, interpret downstream effects of changes, and cover a significant portion of functionality within the data.</p>
<h3 id="are-these-features-interpretable">Are these features &ldquo;interpretable&rdquo;</h3>
<table>
    <tr>
        <td align="center"><img src="../images/week9/day2-slides-20.png"></td>
    </tr>
    <tr>
    <td colspan=1>
    <br/>
<p><strong>Feature Activation Sampling Bias</strong>: In previous evaluations, there was a bias due to just considering the top-activation neurons which might inaccurately appear monosemantic due to their higher activations. To mitigate this bias, the approach involves sampling uniformly across all possible activations for each given feature.</p>
<p><strong>Evaluation of Interpretable Features:</strong> The authors used an evaluation process where human-based assessments are used to determine the interpretability of the features extracted. The criteria for interpretability are based on the authors' distributed-based evaluation, where a score above eight is considered sufficiently interpretable.</p>
</td> </table>
<h3 id="automated-evaluation">Automated Evaluation</h3>
<table>
    <tr>
        <td align="center"><img src="../images/week9/day2-slides-21.png"></td>
    </tr>
    <tr>
    <td colspan=1>
    <br/>
    <p>
<p>The authors used a larger language model to generate a text description of each feature. For the features from the sparse autoencoder, there is a higher correlation with human interpretations, with average correlation values reaching as high as 0.153 in some instances, and up to 0.7 in larger models.</p>
</p>
</td> </table>
<h3 id="group-discussions">Group Discussions</h3>
<!-- ![Slide22](../images/week9/day2-slides-22.png) -->
<table>
    <tr>
        <td align="center"><img src="../images/week9/day2-slides-22.png"></td>
    </tr>
</table>
<p>The sparse autoencoder technique in focus can explain up to 80% of the loss. This means that by replacing activated neurons with reconstructions, 80% of the original model&rsquo;s loss can be accounted for without altering the model. Notably, there is a high correlation (Spearman correlation around 0.7) between independent features of two models sharing the same architecture but having different random initializations.</p>
<p>Considering these evaluation findings, the class divided into three groups to discuss specific questions related to the interpretability of features.</p>
<p>One group noted a common discrepancy between the expectations from
language models and humans. Language models are often expected to
perform at superhuman or domain expert levels, while their
capabilities might align more closely with those of a standard
human. The use of a general-purpose language model for features
requiring domain expertise was seen as a potential issue, as the
model&rsquo;s interpretation might lack the required domain-specific
knowledge.</p>
<p>The group also shared their discussion about the possibility that
language models might &lsquo;hallucinate&rsquo; interpretations for given
features, possibly creating false correlations or interpretations that
do not necessarily exist within the data. Human evaluators might also
introduce subconscious biases or look for patterns without having the
necessary domain expertise, potentially affecting the interpretability
findings. Another key point they raised was about the intended
audience for interpretability. They discussed that interpretability
work from the language models might primarily target researchers,
specifically computer science researchers who are involved in
developing and assessing models, rather than end-users of these models
in practical applications.</p>
<p>The second group highlighted the multifaceted nature of the variance
observed in models designed to find interpretable features. It
primarily attributed this variability to stochastic elements in the
learning process, the order and sequence of data during training, and
the diverse interpretations resulting from these variations, which may
lead to equally interpretable yet different feature sets.</p>
<p>The final group emphasized the potential acceptability of the unexplained 20% in certain contexts, underscoring the value in correctly interpreting the majority of the content. Additionally, they noted the potential nuances within the unexplained portion, distinguishing between varying reasons for lack of interpretability within that portion.</p>
<h2 id="feature-splitting">Feature Splitting</h2>
<table>
    <tr>
        <td align="center">
            <img src="../images/week9/day2-slides-23.png" width="100%">
            <img src="../images/week9/day2-slides-24.png" width="100%">
        </td>
    </tr>
    <tr>
    <td colspan=1>
    <br/>
    <p>
The training of three different versions of autoencoders with increasing sizes of the internal representation were described, leading to more sparsity in interpretable features. They analogized a dictionary learning algorithm to an unlimited number of interpretable features, Even with varied model semantics, a structured superposition of concepts emerges in the learning process. 
<p>By feature clustering and splitting, this splitting of features leads to more fine-grained interpretations, where a single concept or feature might bifurcate into multiple similar but distinct interpretable features. These findings may have benefits beyond one-layer transformers, suggesting the possibility of applying this technique to larger transformers or models.</p>
</td> </table>
<h2 id="takeaways">Takeaways</h2>
<table>
    <tr>
        <td align="center">
            <img src="../images/week9/day2-slides-25.png" width="100%">
        </td>
    </tr>
    <tr>
    <td colspan=1>
    <br/>
    <p>
The summary underscores the potential and limitations of both architectural changes aimed at controlling polysemanticity and the potential for post-learning techniques, but so far only partially demonstrated for a simple  1-layer transformer. 
<p>Post-learning interpretation seems more practical for now than
adapting existing training techniques for interpretability, which would require larger changes to current practices.</p> </td> </table></p>
<h1 id="readings-and-discussions">Readings and Discussions</h1>
<h2 id="monday-23-october">Monday 23 October</h2>
<h3 id="required-reading">Required Reading</h3>
<ul>
<li>Alicja Chaszczewicz. <a href="https://arxiv.org/abs/2307.06963"><em>Is Task-Agnostic Explainable AI a Myth?</em></a>. arXiv, 2023. <a href="https://arxiv.org/pdf/2307.06963.pdf">https://arxiv.org/pdf/2307.06963.pdf</a></li>
</ul>
<h3 id="optional-readings">Optional Readings</h3>
<ul>
<li>
<p>Robert Geirhos, Roland S. Zimmermann, Blair Bilodeau, Wieland Brendel, Been Kim. <a href="https://arxiv.org/abs/2306.04719"><em>Don&rsquo;t trust your eyes: on the (un)reliability of feature visualizations</em></a>. arXiv, 2023. <a href="https://arxiv.org/pdf/2306.04719.pdf">https://arxiv.org/pdf/2306.04719.pdf</a></p>
</li>
<li>
<p>Sarah Wiegreffe, Yuval Pinter. <a href="https://arxiv.org/abs/1908.04626"><em>Attention is not not Explanation</em></a>. EMNLP, 2019. <a href="https://arxiv.org/pdf/1908.04626.pdf">https://arxiv.org/pdf/1908.04626.pdf</a> (This is a response to Jain and Wallace&rsquo;s <a href="https://arxiv.org/abs/1902.10186"><em>Attention is not Explanation</em></a>, NAACL 2019 paper, which sadly is not a response to any paper titled Attention is Explanation, but perhaps that is waiting to be written?)</p>
</li>
</ul>
<h3 id="discussion-questions">Discussion Questions</h3>
<ol>
<li>
<p><a href="https://arxiv.org/abs/2307.06963">Chaszczewicz</a> highlights shared challenges in XAI development across different data types (i.e. image, textual, graph data) and explanation units (i.e. saliency, attention, graph-type explainers). What are some potential similarities or differences in addressing these issues?</p>
</li>
<li>
<p>In cases where models produce accurate results but lack transparency, should the lack of explainability be a significant concern? How should organizations/developers balance the tradeoffs between explainability and accuracy?</p>
</li>
<li>
<p>How can XAI tools could be used to improve adversarial attacks?</p>
</li>
<li>
<p>In <a href="https://arxiv.org/pdf/1908.04626.pdf">Attention is not not Explanation</a>, the authors dispute a <a href="https://arxiv.org/abs/1902.10186">previous paper’s</a> definition of explanation. Whose view do you find most convincing and why?</p>
</li>
</ol>
<h2 id="wednesday-25-october">Wednesday 25 October</h2>
<h3 id="required-readings">Required Readings</h3>
<ul>
<li>Nelson Elhage, Tristan Hume, Catherine Olsson, Neel Nanda, Tom Henighan, Scott Johnston, Sheer El Showk, Nicholas Joseph, Nova DasSarma, Ben Mann, and others (Anthropic AI). <a href="https://transformer-circuits.pub/2022/solu/index.html"><em>Softmax Linear Units</em></a>. Transformers Circuit Thread, 2022. <a href="https://transformer-circuits.pub/2022/solu/index.html">https://transformer-circuits.pub/2022/solu/index.html</a></li>
<li>Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, and others (Anthropic AI). <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html"><em>Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</em></a>. Transformers Circuit Thread, 2023. <a href="https://transformer-circuits.pub/2023/monosemantic-features/index.html">https://transformer-circuits.pub/2023/monosemantic-features/index.html</a></li>
</ul>
<h3 id="discussion-questions-1">Discussion Questions</h3>
<ol>
<li>
<p>(Softmax Linear Units) Elhage et al. present the Superposition Hypothesis which argues that networks attempt to learn more features than the number of neurons in the networks. By delegating multiple features to a single node, interpreting the significance of the node becomes challenging. Do you believe this hypothesis based upon their explanation, or do you suspect there is some separate obstacle here, such as the counter-argument that nodes could represent standalone features that are difficult to infer but often obvious once discovered?</p>
</li>
<li>
<p>(Softmax Linear Units) Do you see any difference between SLU and <a href="https://pytorch.org/docs/stable/generated/torch.nn.ELU.html">ELU</a> coupled with batch-norm/layer-norm? How does this relate to the reasons the LLM community shifted from ReLU (or variants like ELU) to <a href="https://arxiv.org/abs/1606.08415">GeLU</a>?</p>
</li>
<li>
<p>(Towards Monosemanticity) Could the identification of these “interpretable” features could enable training (via distillation, or other ways) smaller models that still preserve interpretability?</p>
</li>
<li>
<p>(Towards Monosemanticity) Toying around with <a href="https://transformer-circuits.pub/2023/monosemantic-features/vis/a1.html?ordering=count">visualization</a> seems to show a good identification of relevant positive tokens for concepts, but negative concepts do not seem to be very insightful. Try the explorer out for a few concepts and see if these observations align with what you see. What do you think might be happening here? Can it possibly be solved by changing the auto-encoder training pipeline, or possibly by involving structural changes like SLU? Are there other interesting elements or patterns you see?</p>
</li>
</ol>

      </div>
<hr class="post-separator"></hr>

    
    <h1><a href="/week8/">Week 8: Machine Translation</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-10-22 00:00:00 &#43;0000 UTC" itemprop="datePublished">22 October 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>(see bottom for assigned readings and questions)</p>
<h1 id="machine-translation-week-8">Machine Translation (Week 8)</h1>
<p><author>Presenting Team: Ajwa Shahid, Caroline Gihlstorf, Changhong Yang, Hyeongjin Kim, Sarah Boyce</author></p>
<p><author>Blogging Team: Xindi Guo, Mengxuan Hu, Tseganesh Beyene Kebede, Zihan Guan</author></p>
<h1 id="monday-16-oct-br-diving-into-the-history-of-machine-translation">Monday, 16 Oct: <br> Diving into the History of Machine Translation</h1>
<p>Let&rsquo;s kick off this topic with an activity that involves translating an English sentence into a language of your choice and subsequently composing pseudocode to describe the process.</p>
<center>
<div class="slide">
  <img src="../images/week8/slide_1.png" alt="" width="70%">
</div>
</center>
<h2 id="heading"></h2>
<p>Here is an example of pseudocode from the activity:</p>
<pre tabindex="0"><code class="language-angular2html" data-lang="angular2html">Sentence = &quot;The students like to read interesting books.&quot;
# The bilingual dictionary from English to Chinese: Eng_chinese_dict
Translation = []
for word in Sentence.split():
    if word in Eng_chinese_dict:
        Translation.append(Eng_chinese_dict[word])
    else:
        Translation.append(word)
Translated_sentence = &quot; &quot;.join(Translation)
</code></pre><p>After the activity discussion, here are the challenges encountered when translating from English to another language:</p>
<ul>
<li>Variations in Word Order: Different languages have varying word orders, affecting sentence structure.</li>
<li>Idiomatic Expressions: Idioms and phrases may lack direct equivalents in other languages, requiring creative translation.</li>
<li>Plurality and Gender: Managing plural forms and gender variations can be complex across languages.</li>
<li>Verb Conjugations: Verbs change for different tenses and moods, complicating translation.</li>
<li>Subject-Object-Verb Order: Sentence structure differences influence how subjects, objects, and verbs are translated.</li>
<li>Verb Tense: Addressing past, present, and future tenses accurately in translation is crucial.</li>
</ul>
<h2 id="the-early-days-of-machine-translation">The Early Days of Machine Translation</h2>
<p>The birth of machine translation can be traced back to 1933 with the work of George Artsrouni and Petr Smirnov-Troyanskii. Artsrouni developed an automatic multilingual word lookup, which can be viewed as a precursor to the modern digital dictionary. Smirnov-Troyanskii conceptualized a machine translation process where humans and machines work together, with different steps encoded by humans and others by the machine.</p>
<h2 id="different-generations-of-machine-translation-approaches">Different Generations of Machine Translation Approaches</h2>
<p>Machine translation approaches can be categorized into three primary generations, each having its unique characteristics and methodologies.</p>
<h3 id="first-generation-direct-translation">First Generation: Direct Translation</h3>
<p>The MT system is designed in all details specifically for one particular pair of languages, e.g. Russian as the language of the original texts, the source language, and English as the language of the translated texts, the target language. Translation is direct from the source language (SL) text to the target language (TL) text;Typically, systems consist of a large bilingual dictionary and a single monolithic program for analyzing and generating texts; such &lsquo;direct translation&rsquo; systems are necessarily bilingual and unidirectional.</p>
<center>
<div class="slide">
  <img src="../images/week8/slide_2.png" alt="" width="70%">
</div>
</center>
<h3 id="second-generation-interlingua-and-transfer-approach">Second Generation: Interlingua and Transfer Approach</h3>
<p>The Interlingua approach proposed the use of a universal semantic representation, known as Interlingua, between the source and target languages. This approach offered the advantage of being multilingual, where it was easy to swap source languages, thus reducing the number of models needed.
In contrast, the Transfer approach used abstract representations of both the source and target languages. This approach required an extensive collection of dictionaries, grammar rules, and language structure rules, including syntax, morphology, and possibly semantics.</p>
<h3 id="third-generation-statistical-methods">Third Generation: Statistical Methods</h3>
<p>The third generation of MT brought a significant shift by introducing statistical methods. This generation marked a transition from rule-based methods to learning-based methods, leveraging data and probability to guide translation.</p>
<center>
<div class="slide">
  <img src="../images/week8/slide_3.png" alt="" width="70%">
<p> Image Source: <a>https://aclanthology.org/J90-2002.pdf</a> </p>
</div>
</center>
<h2 id="a-timeline-of-machine-translation-evolution">A Timeline of Machine Translation Evolution</h2>
<h1 id="heading-1"></h1>
<center>
<div class="slide">
  <img src="../images/week8/slide_4.png" alt="" width="90%">
</div>
</center>
<h2 id="heading-2"></h2>
<p>In the 1950s, theory-driven machine translation began, heavily influenced by linguistics and computer science, with a primary focus on Russian-English and English-Russian translation and the use of some statistical methods.</p>
<p>The 1960s introduced challenges in representing semantics and the &ldquo;encoder-decoder&rdquo; structure, a foundational concept in modern neural machine translation.</p>
<p>In the 1970s, the field transitioned from interlingua to transfer methods and explored AI concepts.</p>
<p>The 1980s brought multilingual MT systems, revisiting interlingua methods and advancing semantic representations.</p>
<p>The 1990s witnessed a significant shift from rule-based methods to &ldquo;corpus-based&rdquo; and &ldquo;example-based&rdquo; approaches, emphasizing data-driven methods in machine translation. It also marked the initial use of neural networks and the integration of machine translation with speech recognition, opening new horizons in the field.</p>
<h1 id="neural-machine-translation">Neural Machine Translation</h1>
<p>Neural Machine Translation (NMT) uses neural network models to develop a statistical model for machine translation. Unlike traditional phrase-based translation systems that comprise multiple small sub-components tuned separately, NMT attempts to build and train a singular, large neural network that reads a sentence and outputs the correct translation.</p>
<h2 id="early-models">Early Models</h2>
<h3 id="recurrent-neural-network-rnn">Recurrent Neural Network (RNN)</h3>
<center>
<div class="slide">
  <img src="../images/week8/slide_5.png" alt="" width="70%">
</div>
</center>
<p>RNNs process sequential data one element at a time, maintaining a hidden state that captures information from previous elements to inform future predictions or classifications. In the image above, &lsquo;x&rsquo; represents individual elements from sequential data, &lsquo;A&rsquo; represents the Neural Network, and &lsquo;h&rsquo; signifies the hidden layer. At each time step, it processes sequential data step-by-step.</p>
<div class="slide">
  <img src="../images/week8/slide_6.png" alt="">
</div>
<p>The image above illustrates how RNN works for language translation. To translate a sentence like &lsquo;How are you?&rsquo; from English to French using an RNN Encoder-Decoder, the English input is initially encoded through an Encoder. Subsequently, the Decoder generates the French output word by word. This sequential process results in the French translation, such as &lsquo;Comment allez vous ?,&rsquo; from the English input.</p>
<p>However, RNNs have a limitation in handling long-term dependencies. When dealing with a large amount of data, RNNs may struggle to memorize it all.</p>
<p>One solution to this limitation is Long Short Term Memory networks (LSTM), a specific type of RNN designed to capture long-term dependencies. LSTMs incorporate forget, input, and output gates that regulate the flow of information within the network:</p>
<ul>
<li>The forget gate determines whether to retain or discard certain information.</li>
<li>The input gate quantifies the significance of new information introduced.</li>
<li>The output gate determines what information should be produced.</li>
</ul>
<center>
<div class="slide">
  <img src="../images/week8/slide_7.png" alt="" width="70%">
</div>
</center>
<p>Simultaneously, the attention mechanism stands as a pivotal development in NMT. It grants the decoder access to all hidden states from the encoder, enabling the network to selectively focus on different segments of the input sequence during each step of output generation. This enhances the model&rsquo;s ability to handle long sentences and complex dependencies. The model gains access to all inputs through the use of bidirectional RNNs. Additionally, the prediction of the next word depends on the weighted combination of these hidden states.</p>
<div class="slide">
  <img src="../images/week8/slide_8.png" alt="">
</div>
<h3 id="transformer-model">Transformer Model</h3>
<p>The Transformer model is a type of neural network architecture primarily used in the field of natural language processing.</p>
<p><strong>Key Advantage:</strong>
Unlike models that process sentences sequentially, a Transformer processes the whole sentence at once, which can lead to faster computation and the ability to parallelize the process.</p>
<div class="slide">
  <img src="../images/week8/slide_9.png" alt="">
</div>
<p>In the self-attention mechanism, a given batched input X is linearly projected into three distinct representations: Query (Q), Key (K), and Value (V). The attention score is computed using Q and K. If padding is applied, all padding positions are masked in the attention score.</p>
<p><strong>Multi-Head Attention</strong>
In a Transformer model, Q, K, and V are divided into multiple splits, and each split is passed into a separate head, termed as a &ldquo;multi-head&rdquo; attention system. Each head computes its attention independently, and all attention results are then concatenated, effectively reversing the split operation.
The multi-head attention mechanism enables each head to learn different aspects of the meanings of each word in relation to other words in the sequence. This allows the Transformer model to capture richer interpretations of the sequence.</p>
<center>
<div class="slide">
  <img src="../images/week8/slide_10.png" alt="">
</div>
</center>
<p><strong>Attention Examples</strong></p>
<div class="slide">
  <img src="../images/week8/slide_11.png" alt="">
</div>
<p>In conclusion, the field of Machine Translation has witnessed remarkable progress, with each new model or mechanism contributing to better language understanding and translation. Currently, Transformer models stand at the forefront of this field, but ongoing research promises further advancements.</p>
<h1 id="wednesday-18-oct-br-challenges-of-machine-translation">Wednesday, 18 Oct: <br> Challenges of Machine Translation</h1>
<h2 id="six-challenges-of-neural-machine-translation">Six Challenges of Neural Machine Translation</h2>
<p>Philipp Koehn and Rebecca Knowles. <a href="https://arxiv.org/abs/1706.03872"><em>Six Challenges for Neural Machine Translation</em></a>. First Workshop on Neural Machine Translation, 2017. [<a href="https://arxiv.org/pdf/1706.03872.pdf">PDF</a>]</p>
<center>
<div class="slide">
  <img src="../images/week8/slide_12.png" alt="">
</div>
</center>
<p><strong>Key Insights</strong>:</p>
<p><strong>In-domain performance</strong>: When trained and tested on the same domain, NMT and SMT systems generally perform well, with Medical and IT domains showing particularly high scores.</p>
<p><strong>Out-of-domain performance</strong>: NMT systems tend to degrade more compared to SMT when applied to domains they were not trained on. For example, an NMT trained on Law performs relatively poorly in the Medical domain, while the SMT retains some effectiveness.</p>
<div class="slide">
  <img src="../images/week8/slide_13.png" alt="">
</div>
<p><strong>Key Insights</strong>:</p>
<p><strong>General Trend</strong>: For all three models, as the amount of training data increases, the BLEU score (indicative of translation quality) also increases. This suggests that having more training data generally leads to better translation performance.</p>
<p><strong>Comparative Performance</strong>: The Phrase-Based with Big LM model consistently outperforms the other two models across different corpus sizes. The Neural model (NMT) starts off better than the simple Phrase-Based model for smaller corpus sizes but tends to converge with the Phrase-Based model as the corpus size grows.</p>
<div class="slide">
  <img src="../images/week8/slide_14.png" alt="">
</div>
<p>NMT provides superior performance when translating words that are infrequent (rarely used) or completely untrained (words not seen during the training process). This could suggest that NMT is more adaptable and flexible in handling diverse vocabulary than SMT.</p>
<div class="slide">
  <img src="../images/week8/slide_15.png" alt="">
</div>
<p>While NMT excels at translating shorter sentences, SMT appears to be more effective for much longer sentences. This might indicate that the statistical approach of SMT is better equipped to handle the complexities and nuances of longer sentence structures.</p>
<div class="slide">
  <img src="../images/week8/slide_16.png" alt="">
</div>
<p>From this data, it can be inferred that the choice of word alignment method can influence the accuracy and quality of translations. Different language pairs also exhibit varying levels of alignment match and probability, suggesting that the effectiveness of these methods can vary based on the languages in question.</p>
<div class="slide">
  <img src="../images/week8/slide_17.png" alt="">
</div>
<p>Beam search is a heuristic search strategy that systematically expands the most promising nodes in a tree-like structure to improve sequence prediction results. The visual representation effectively demonstrates the branching and exploration inherent in the beam search algorithm.</p>
<div class="slide">
  <img src="../images/week8/slide_18.png" alt="">
</div>
<p>While all these issues are crucial, most of the class considered &ldquo;Domain adaptation/mismatch&rdquo; and &ldquo;Amount of training data&rdquo; to be the most pressing. As the digital world grows and diversifies, machine translation tools will be exposed to an ever-increasing array of content types. Ensuring that these tools can adapt to various domains and are trained on representative datasets will be key to their effectiveness and relevance.</p>
<div class="slide">
  <img src="../images/week8/slide_19.png" alt="">
</div>
<p>Domain adaptation in NMT focuses on ensuring translation accuracy for specialized content, like medical or legal texts, given the varied data distribution between training and target domains. Additionally, the infrequent occurrence of specific words, notably proper nouns, in training data can result in mistranslations. Overcoming these hurdles is crucial for enhancing NMT&rsquo;s accuracy and applicability in different contexts.</p>
<div class="slide">
  <img src="../images/week8/slide_20.png" alt="">
</div>
<p>While both LLM and NMT are rooted in deep learning and can comprehend linguistic context, they differ in their primary objectives, training datasets, and architecture. LLMs are versatile and can handle diverse language tasks beyond just translation, while NMTs are specialized for translating between two languages.</p>
<div class="slide">
  <img src="../images/week8/slide_21.png" alt="">
</div>
<p>The comparison highlights the translation performance debate between LLMs and MT models. While both commercial and open-source MTs are valuable, LLMs fine-tuned with general-purpose instructions often excel. The data emphasizes the significance of scale and fine-tuning in LLM effectiveness.</p>
<div class="slide">
  <img src="../images/week8/slide_22.png" alt="">
</div>
<p>The diagrams depict the intricate processes by which Large Language Models (LLMs) engage in translation tasks:</p>
<p><strong>Content Source</strong>:
The initial content that needs translation can come in various formats, including PDFs, Office documents, and even video.</p>
<p><strong>Automated TM Management</strong>:
This is an automated system for managing &lsquo;Translation Memories&rsquo; (TM). Translation memories store previously translated segments of text to ensure consistent translations and speed up the translation process in future tasks.</p>
<p><strong>Pre-translation</strong>:
The untranslated content is matched against a Vector Database (DB) to identify similar contexts or content. The system then uses the K-nearest neighbours method to reference the closest matches.
Relevant training data is extracted to fine-tune the model for a specific task or context.
The model undergoes &lsquo;Prompt Engineering Fine-Tuning&rsquo; (PEFT) using the LoRA method, enhancing its precision for specific tasks or contexts.</p>
<p><strong>Hyper-personalized MT engine</strong>:
Specific prompts are crafted for the content and its context.
The model learns in the given prompt&rsquo;s context, further enhancing translation accuracy.
The LLM API allows other systems or processes to interact seamlessly with the LLM.
The LLM works in tandem with the LoRA module, adding an extra layer of functionality and precision to the translation process.</p>
<p><strong>Human Interaction</strong>:
Even with advanced models, there&rsquo;s a phase where human experts intervene, either for post-editing or proofreading, ensuring the final content adheres to the highest standards.</p>
<p><strong>Top Quality MT Output</strong>:
After all these stages, the translation process culminates in producing content of the highest caliber, ensuring accuracy and context preservation.</p>
<p>Both diagrams underscore a blend of automation, advanced modeling, and human expertise to achieve top-notch translations.</p>
<div class="slide">
  <img src="../images/week8/slide_23.png" alt="">
</div>
<p><strong>Database of Translated Texts</strong>: The cluster of pages symbolizes a collection of previously translated documents stored for reference.</p>
<p><strong>Database of Translated Texts</strong>: The cluster of pages symbolizes a collection of previously translated documents stored for reference.</p>
<p><strong>Search Process</strong>: The magnifying glass indicates the process of searching within the database to find a matching or similar translation.</p>
<p><strong>User Interaction</strong>: The silhouette represents a user or translator interacting with the translation memory system.</p>
<p><strong>Original Content (A)</strong>: The page with the letter &ldquo;A&rdquo; signifies source content awaiting translation. The highlighted segments on this page denote the parts of the text that have matching translations in the memory.</p>
<p><strong>Translated Content (X)</strong>: The page with the symbol &ldquo;X&rdquo; showcases the result after using the translation memory. The highlighted segments indicate the portions of the content retrieved from the memory, ensuring consistency and saving time.</p>
<p><strong>Savings/Cost-Efficiency</strong>: The stack of coins symbolizes the financial advantage or savings gained from using translation memory, reaffirming the caption that states &ldquo;Translation memory is the customer&rsquo;s moneybox.&rdquo;</p>
<p>This visual displays how Translation Memory systems improve efficiency and consistency in translation tasks by reusing previously translated segments.</p>
<div class="slide">
  <img src="../images/week8/slide_24.png" alt="">
</div>
<p>The diagram illustrates three fine-tuning methodologies for neural networks:</p>
<p><strong>Classic</strong>: Involves iterative corrections based on errors.</p>
<p><strong>Freeze</strong>: Retains the original weights and makes separate, task-specific adjustments.</p>
<p><strong>LoRA (Low Rank Adaptation)</strong>: Directly integrates with pre-trained models for efficient task-specific adaptations without extensive error corrections.</p>
<p>In essence, while the classic method emphasizes error corrections, the freeze approach preserves foundational knowledge, and LoRA offers a streamlined adaptation process. The choice among them hinges on the task and desired model refinement.</p>
<div class="slide">
  <img src="../images/week8/slide_25.png" alt="">
</div>
<p>Benefits related to machine translation include:</p>
<ul>
<li><strong>Stylized</strong>: LLMs can adapt to different translation styles, such as formal, informal, or even regional variations.</li>
<li><strong>Interactive</strong>: They can provide real-time translations during interactive sessions, such as live chats.</li>
<li><strong>TM based</strong>: LLMs can utilize Translation Memory to ensure consistency across large documents or series of documents, improving translation quality by leveraging prior translations.</li>
</ul>
<div class="slide">
  <img src="../images/week8/slide_26.png" alt="">
</div>
<p><strong>Evolution of MT Techniques</strong>:
Traditional Machine Translation has evolved to incorporate more sophisticated methods, with the advent of GPT models introducing new paradigms.</p>
<p><strong>Stylized MT</strong>:
Goes beyond standard translation by adapting to specific styles or tones, like literature or marketing.
Despite Neural Machine Translation (NMT) having capabilities for style transfer, its potential is often limited by data availability. LLMs like GPT can overcome this using zero-shot prompts.</p>
<p><strong>Interactive MT</strong>:
Represents a shift towards more user-centric translation methods.
By actively involving the user in the translation process and gathering their feedback, it ensures translations are more contextually accurate and meet specific requirements.</p>
<p><strong>Translation Memory-based MT</strong>:
Aims to improve translation efficiency by referencing past translations.
LLMs bring an added advantage by using past translations not just for replication but as context-rich prompts to guide the current translation process.</p>
<p><strong>Emergence of New Paradigms &amp; Concerns</strong>:
The integration of LLMs in MT introduces new evaluation methodologies.
However, this also raises potential privacy concerns, emphasizing the importance of data ethics in AI-driven translation.</p>
<p><strong>Multi-modality in MT</strong>:
Suggests a future direction where translations aren&rsquo;t just based on text but incorporate multiple forms of data, enriching the translation process.</p>
<p><strong>Dependency on LLMs</strong>:
The consistent reference to GPT models across various MT applications indicates the growing influence and reliance on LLMs in modern translation efforts.</p>
<p>The transformative role of LLMs in reshaping machine translation, offering enhanced accuracy, versatility, and user engagement.</p>
<p>The activity showcases the challenges and complexities of machine translation. While tools like DeepL can offer rapid translations, they may sometimes miss cultural, contextual, or idiomatic nuances present in human translations. This is especially relevant for movie dialogues where context and tone play crucial roles.
<strong>Can MT outperform human translation?</strong>
MT can be faster and efficient for large-scale tasks, but human translation excels in capturing nuance, cultural context, and idiomatic expressions.</p>
<p><strong>What are other challenges that can&rsquo;t be solved by MT?</strong>
MT struggles with cultural nuances, idioms, historical context, and emotional undertones which humans can naturally grasp.</p>
<p><strong>How can HT and MT interact for better Language Translation?</strong>
A hybrid approach, combining MT&rsquo;s speed and efficiency with HT&rsquo;s contextual understanding, can lead to more accurate and nuanced translations.</p>
<h1 id="monday-16-october">Monday, 16 October</h1>
<p><strong>Required readings:</strong></p>
<p>Shereen A. Mohamed, Ashraf A. Elsayed, Y. F. Hassan and Mohamed A. Abdou.  <a href="https://link.springer.com/article/10.1007/s00521-021-06268-0"><em>Neural machine translation: past, present, and future</em></a>.
Neural Computing and Applications, 2021. [https://link.springer.com/article/10.1007/s00521-021-06268-0] (<a href="https://link.springer.com/article/10.1007/s00521-021-06268-0">https://link.springer.com/article/10.1007/s00521-021-06268-0</a>)</p>
<p><strong>Optional readings:</strong></p>
<p>W. John Hutchins. <a href="https://aymara.org/biblio/mtranslation.pdf"><em>Machine Translation: A Brief History</em></a>. From <em>Concise history of the language sciences: from the Sumerians to the cognitivists</em> (edited by E. F. K. Koerner and R. E. Asher). Pergamon Press, 1995. [<a href="https://aymara.org/biblio/mtranslation.pdf">PDF</a>]</p>
<p>Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick Jelinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. <a href="https://aclanthology.org/J90-2002.pdf"><em>A Statistical Approach to Machine Translation</em></a>.  Computational Linguistics 1990. [<a href="https://aclanthology.org/J90-2002.pdf">PDF</a>]</p>
<p>Linfeng Song, Daniel Gildea, Yue Zhang, Zhiguo Wang, Jinsong Su. <a href="https://aclanthology.org/Q19-1002/"><em>Semantic Neural Machine Translation Using AMR</em></a>. [Transactions of the Association for Computational Linguistics, 2019. [<a href="https://aclanthology.org/Q19-1002.pdf">PDF</a>]</p>
<h1 id="discussion-questions">Discussion Questions</h1>
<p>(post your response by Sunday, 15 October)</p>
<ol>
<li>
<p>What are the limitations of the testing benchmarks used for machine translation (as described in <a href="https://link.springer.com/article/10.1007/s00521-021-06268-0"><em>Neural machine translation: past, present, and future</em></a>) and how might these limitations impact model development?</p>
</li>
<li>
<p>The paper describes neural machine translation (NMT) models as simpler than previously utilized statistical machine translation (SMT) models, and lists a few ways in which this is the case. Are there any drawbacks to NMT models over SMT models, particularly when it comes to interpretability and assuring that essential linguistic knowledge is learned?</p>
</li>
<li>
<p>Why do most LLMs use decoder-only architecture? Why not encoder-decoder?</p>
</li>
</ol>
<h1 id="wednesday-18-october">Wednesday, 18 October</h1>
<p><strong>Required readings</strong></p>
<p>Chenyang Lyu, Jitao Xu, Longyue Wang. <a href="https://arxiv.org/abs/2305.01181"><em>New Trends in Machine Translation using Large Language Models : Case Examples with ChatGPT</em></a>.  <a href="https://arxiv.org/abs/2305.01181">https://arxiv.org/abs/2305.01181</a> [<a href="https://arxiv.org/pdf/2305.01181.pdf">PDF</a>]</p>
<p>Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André F. T. Martins, Graham Neubig, Ankush Garg, Jonathan H. Clark, Markus Freitag, Orhan Firat. <a href="https://arxiv.org/abs/2308.07286"><em>The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation</em></a>. <a href="https://arxiv.org/abs/2308.07286">https://arxiv.org/abs/2308.07286</a> [<a href="https://arxiv.org/pdf/2308.07286.pdf">PDF</a>]</p>
<p><strong>Optional readings</strong></p>
<p>Philipp Koehn and Rebecca Knowles. <a href="https://arxiv.org/abs/1706.03872"><em>Six Challenges for Neural Machine Translation</em></a>. First Workshop on Neural Machine Translation, 2017. [<a href="https://arxiv.org/pdf/1706.03872.pdf">PDF</a>]</p>
<p>Vivek Iyer, Pinzhen Chen, Alexandra Birch. <a href="https://arxiv.org/abs/2309.11668"><em>Towards Effective Disambiguation for Machine Translation with Large Language Models</em></a>. <a href="https://arxiv.org/abs/2309.11668">https://arxiv.org/abs/2309.11668</a>.</p>
<p>Danielle Saunders. <a href="https://dcsaunders.github.io/thesis.pdf"><em>Domain adaptation for Neural Machine Translation</em></a>. PhD Dissertation, University of Cambridge, February 2021. [<a href="https://dcsaunders.github.io/thesis.pdf">PDF</a>]</p>
<p>Radhika Sharma, Pragya Katyayan, Nisheeth Joshi. <a href="https://arxiv.org/abs/2305.07360"><em>Improving the Quality of Neural Machine Translation Trough Proper Translation of Name Entities</em></a>. <a href="https://arxiv.org/abs/2305.07360">https://arxiv.org/abs/2305.07360</a> [<a href="https://arxiv.org/pdf/2305.07360.pdf">PDF</a>]</p>
<p>Verna Dankers, Christopher Lucas, Ivan Titov. <a href="https://aclanthology.org/2022.acl-long.252/"><em>Can Transformer be Too Compositional? Analysing Idiom Processing in Neural Machine Translation</em></a>. ACL 2022. [<a href="https://aclanthology.org/2022.acl-long.252.pdf">PDF</a>]</p>
<p>Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas, James Henderson. <a href="https://aclanthology.org/D18-1325"><em>Document-Level Neural Machine Translation with Hierarchical Attention Networks</em></a>. EMNLP 2018. [<a href="https://aclanthology.org/D18-1325.pdf">PDF</a>]</p>
<h2 id="discussion-questions-1">Discussion Questions</h2>
<ol>
<li>
<p><a href="https://arxiv.org/abs/1706.03872"><em>Six Challenges for Neural Machine Translation</em></a> describes six neural machine translation challenges. Discuss how you have encountered these challenges in real-world translator use, the risks you anticipate, and how to mitigate them.</p>
</li>
<li>
<p>There have been many trials to evaluate the performance of Machine Translation. Do you agree with the evaluation method in the paper <a href="https://arxiv.org/abs/2308.07286"><em>The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation</em></a>? What other factors that you think are important in how we evaluate?</p>
</li>
</ol>

      </div>
<hr class="post-separator"></hr>

    
    <h1><a href="/week7/">Week 7: GANs and DeepFakes</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-10-16 00:00:00 &#43;0000 UTC" itemprop="datePublished">16 October 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>(see bottom for assigned readings and questions)</p>
<p><author>Presenting Team: Aparna Kishore, Elena Long, Erzhen Hu, Jingping Wan </author></p>
<p><author>Blogging Team: Haochen Liu, Haolin Liu, Ji Hyun Kim, Stephanie Schoch, Xueren Ge </author></p>
<h1 id="monday-9-october-br-generative-adversarial-networks-and-deepfakes">Monday, 9 October: <br> Generative Adversarial Networks and DeepFakes</h1>
<div class="slide">
  <img src="../images/week7/A.JPG" alt="">
  <p>Today's topic is how to utilize generative adversarial networks to create fake images and how to identify the images generated by these models.</p>
</div>
<div class="slide">
  <img src="../images/week7/B.JPG" alt="">
  <p>Generative Adversarial Network (GAN) is a revolutionary deep learning framework that pits two neural networks against each other in a creative showdown. One network, the generator, strives to produce realistic data, such as images or text, while the other, the discriminator, aims to differentiate between genuine and generated data. Through a continuous feedback loop, GANs refine their abilities, leading to the generation of increasingly convincing and high-quality content.</p>
</div>
<div class="slide">
  <img src="../images/week7/C.JPG" alt="">
  <p>To ensure students have a better understanding of GANs. The leading team held a “GAN Auction Game” to simulate the generating and predicting process of the generator and discriminator in GAN. In this game, students are divided into two groups (Group 1 and Group 2). Group 1 will provide three items (e.g. the name of a place) while Group 2 tries to identify whether the items provided are real or fake.
<p>The game captures the training process of GANs where the generator first proposes certain contents (e.g. images or contexts) and the discriminator is trained to distinguish real from generated (fake) content.</p>
<p>If the generator successfully creates contents that fools the discriminator, it will receive a high reward for further tuning. On the other hand, if the discriminator correctly identifies the content created by the generator it receives a reward.</p>
<p>This iterative training process is illustrated by the figures below.</p></p>
</div>
<div class="slide">
  <img src="../images/week7/D.JPG" alt="">
</div>
<h2 id="heading"></h2>
<div class="slide">
  <img src="../images/week7/E.JPG" alt="">
</div>
<h2 id="heading-1"></h2>
<div class="slide">
  <img src="../images/week7/F.JPG" alt="">
</div>
<h2 id="heading-2"></h2>
<div class="slide">
  <img src="../images/week7/G.JPG" alt="">
  <p>Formally, the training process can be modeled as a two-player zero-sum game by conducting min-max optimization on the objective function.A Nash equilibrium will be established between generator and discriminator.</p>
</div>
<div class="slide">
  <img src="../images/week7/H.JPG" alt="">
</div>
<h1 id="heading-3"></h1>
<div class="slide">
  <img src="../images/week7/I.JPG" alt="">
  <p>
<p>For a system that only has generators and discriminators, it is hard to tell whether they are doing well because there are many bad local optima. Thus, one direct way is to introduce human feedback for evaluating.</p>
<p>For example, we can borrow strategies from Large Language Models (LLMs), particularly employing Reinforcement Learning from Human Feedback (RLHF). In this method, experts would iteratively rank the generated samples, offering direct reinforcement signals to improve the generator&rsquo;s output. This approach could enhance the realism and semantic alignment of the content created by GANs. However, the RLHF method has its drawbacks, primarily the extensive need for expert involvement, raising concerns about its scalability in larger evaluations.</p>
<p>An alternative could be the inclusion of non-expert users, offering a broader range of feedback. Crowdsourcing and user studies are suggested as methods to understand if the generated content meets the target audience&rsquo;s needs and preferences.</p></p>
<p>For images or tabular data, when the data distribution is roughly known, inception score serves as an useful metric. This score calculates the KL divergence between the conditional class distribution and the marginal class distribution of generated samples. A higher inception score (IS) indicates clearer and more diverse images. However, it doesn't always correlate with human judgment.</p>
</div>
<div class="slide">
  <img src="../images/week7/J.JPG" alt="">
<h2 id="heading-4"></h2>
<ol>
<li><p>Vanishing/Exploding Gradient: During backpropagation, gradients can shrink (vanish) or grow excessively (explode), disrupting learning. Vanishing gradients stall the network's learning, as parameter updates become negligible. Exploding gradients cause extreme, destabilizing updates, hindering the model's convergence.</p>
</li>
<li><p>Mode Collapse: GANs can suffer from mode collapse, where the generator produces limited, similar samples, failing to represent the data's true diversity. This occurs when the generator exploits the discriminator's weaknesses, concentrating on certain data aspects and neglecting others. It compromises the GAN's objective of generating diverse, realistic samples, indicating a breakdown in adversarial learning.</p>
</li>
</ol>
</div>
<div class="slide">
  <img src="../images/week7/K.JPG" alt="">
</div>
<h1 id="heading-5"></h1>
<div class="slide">
  <img src="../images/week7/L.JPG" alt="">
  <p>A warm-up game is to identify the fake person that is generated by GANs.</p>
</div>
<div class="slide">
  <img src="../images/week7/M.JPG" alt="">
  <p>In the above figure, one of the two faces is fake but it is difficult to identify at first glance.</p>
</div>
<div class="slide">
  <img src="../images/week7/N.JPG" alt="">
  <p>To successfully identify fake images, there are several methods that either use deep-learning-based models to learn to identify fake samples, or through direct observation by people. The leading team then introduces three interesting methods that enable us to tell the difference. We will revisit these two faces later and now focus on the detailed methods to do general identification.</p>
</div>
<div class="slide">
  <img src="../images/week7/O.JPG" alt="">
</div>
<h2 id="heading-6"></h2>
<div class="slide">
  <img src="../images/week7/P.JPG" alt="">
  <p>For example, images generated by GANs tend to contain color artifacts or invisible artifacts that can be identified by deep learning models.</p>
</div>
<div class="slide">
  <img src="../images/week7/Q.JPG" alt="">
  <p>The second method is physical-based. Namely, the corneal specular highlights for the real face have strong similarities while those for the GAN-faces are different.</p>
</div>
<div class="slide">
  <img src="../images/week7/R.JPG" alt="">
  <p>The third method is physiological-based. Specifically, the pupils for the real eyes have strong circular shapes while the GAN-generated pupils usually have irregular shapes.</p>
</div>
<div class="slide">
  <img src="../images/week7/S.JPG" alt="">
  <p>With the help of these methods, we can say that the left woman in the figure we showed before is fake. This can be justified by color artifacts of GAN-image identified from deep learning and her irregular pupils.</p>
</div>
<div class="slide">
  <img src="../images/week7/T.JPG" alt="">
  <p>The leading team also believes that these identification methods can be escaped by more advanced image-generating models but new methods will also be proposed accordingly to distinguish images generated by these advanced models. The generation and identification will evolve together.</p>
</div>
<div class="slide">
  <img src="../images/week7/U.JPG" alt="">
  <p>In summary, generative models such as GANs have fundamentally transformed people's lives, and there remains a substantial amount of future research and development ahead. Some future directions are listed above.</p>
</div>
<h1 id="wednesday-11-octoberbrcreation-and-detection-of-deepfake-videos">Wednesday, 11 October<br>Creation and Detection of DeepFake Videos</h1>
<div class="slide">
  <img src="../images/week7/1.png" alt="">
</div>
<h2 id="heading-7"></h2>
  <p> Outline
  <ol>
  <li>Introduction to deepfake videos</li>
  <li>Detecting Face-swap deepfakes with temporal dynamics</li>
  <li>Discussion</li>
  </ol>
  </p>
</div>
<div class="slide">
  <img src="../images/week7/3.png" alt="">
<p> Definition of a deepfake: A deceptive image or recording that distorts reality to deceive.
  </p>
</div>
<div class="slide">
  <img src="../images/week7/4.png" alt="">
</div>
<h1 id="heading-8"></h1>
<div class="slide">
  <img src="../images/week7/5.png" alt="">
  <p> There are some side effects of face swap methods, including
  <ul>
  <li> Limited accuracy </li>
  <li> Concerns of how to protect privacy </li>
  </ul>
  </p>
</div>
<div class="slide">
  <img src="../images/week7/6.png" alt="">
  <p> The presenters introduced three different methods of generating deepfake videos:
  <ol>
  <li> Reenactment </li>
  <li> Lip-sync deepfakes </li>
  <li> Text-based deepfake synthesis </li>
  </ol>
  </p>
</div>
<div class="slide">
  <img src="../images/week7/7.png" alt="">
  <p> Reenactment: A deepfake reenacts using source images to manipulate the target.
  </p>
</div>
<div class="slide">
  <img src="../images/week7/8.png" alt="">
  <p> Example of a reenactment: the mouth movement in Trump's video is animated by a source actor.
  </p>
</div>
<div class="slide">
  <img src="../images/week7/9.png" alt="">
  <p>Here is another example of Reenactment, where the dancing in target video is animated by a source actor.</p>
</div>
<div class="slide">
  <img src="../images/week7/10.png" alt="">
  <p> Three main steps of reenactment:
  <ol>
  <li> The first step is tracking facial features in both source and target videos. </li>
  <li> A consistency measure aligns input video features with a 3D face model. </li>
  <li> Expressions are transferred from source to target with refinement for realism. </li>
  </ol>
  </p>
</div>
<div class="slide">
  <img src="../images/week7/11.png" alt="">
  <p> Difference between face swap and reenactment: 
  <ul>
  <li> Difference is about the target image. Face swap retains part of the source image, but reenactment does not retain the background source image</li>
  </ul>
  </p>
</div>
<div class="slide">
  <img src="../images/week7/12.png" alt="">
  <p> Most methods use RGB images, while lip-sync relies on audio input.
  <ul>
  <li> Audio is transformed into a dynamic mouth shape. </li>
  <li> The mouth texture is matched with the target video for natural motion. </li>
  </ul>
  </p>
</div>
<div class="slide">
  <img src="../images/week7/13.png" alt="">
<p>Text-based methods modify videos per word, <strong>phonemes</strong> and <strong>visemes</strong> are key for pronunciation and analysis. Text edits are matched with phoneme sequences in the source video. Parameters of the 3D head model are used to smooth lip motions.</p>
  </p>
</div>
<div class="slide">
  <img src="../images/week7/14.png" alt="">
  <p> While previous works have done a lot, an overlooked aspect in the creation of these deep-fake videos is the human ear. Here is one recent work trying to tackle this problem from the aspect of ear.
  </p>
</div>
<div class="slide">
  <img src="../images/week7/15.png" alt="">
  <p> Three types of authentication techniques:
  <ul>
  <li> Forensic  Analysis </li>
  <li> Digital Signatures </li>
  <li>Digital Watermarks </li>
  </ul>
  </p>
</div>
<div class="slide">
  <img src="../images/week7/16.png" alt="">
  <p> Today, our focus is on forensic methods to detect deep fakes. These methods can be categorized into low- and high-level approaches.
  </p>
</div>
<div class="slide">
  <img src="../images/week7/17.png" alt="">
  <p> The presenters asked the class to find criterias to identify an authentic picture of Tom Cruise. In the discussion, several factors were highlighted:
  <ul>
  <li> <i>Posture</i> of the third picture is unnatural.</li>
  <li><i>Foggy background</i> of first picture vs. <i>Realistic background</i> of the second picture</li>
  <li><i>Scale/ratio</i> of head and hand is odd in the third picture</li>
  </ul>
  During the class poll to determine which image appeared authentic, the majority of students voted for the second image, with a few supporting the first, and none voting for the third.
</p>
<p>
Surprisingly to the majority of the class, it was revealed that the first image was genuine, while the others were crafted by a TikTok user in creating deep fake content.
</p>
</div>
<div class="slide">
  <img src="../images/week7/18.png" alt="">
  <p> Many deep fake videos emphasizing facial expressions often neglect the intricate movements of the human ear and the corresponding changes that occur in jaw movements.
  </p>
</div>
<div class="slide">
  <img src="../images/week7/19.png" alt="">
  <p> The aural dynamics system tracks and annotates ear landmarks, utilizing averaged local aural motion to simulate both horizontal and vertical movements, mirroring those of a real person.
  </p>
</div>
<div class="slide">
  <img src="../images/week7/20.png" alt="">
  <p> With the videos of Joe Biden, Angela Merkel, Donald Trump, and Mark
Zuckerberg, they used GAN to synthesize the mouth region of individuals to match the new audio track and generate a lip-sync video.
  </p>
</div>
<div class="slide">
  <img src="../images/week7/21.png" alt="">
  <p> The graphs are a distribution of the correlation of horizontal motion of three aural areas and audio (left) and lip vertical distance (right). </p>
  <p>Fake ones have no correlation, where individuals have strong correlation that are not necessarily consistent. </p>
</div>
<div class="slide">
  <img src="../images/week7/22.png" alt="">
  <p> The horizontal movement of the tragus and lobule parts of Trump’s ears exhibited a positive correlation, distinguishing it as a distinctive personal trait, unlike the general pattern observed on others.
  </p>
</div>
<div class="slide">
  <img src="../images/week7/23.png" alt="">
  <p> The table shows the performance of each model. Models with person-specific training show a higher average testing accuracy.
  </p>
</div>
<div class="slide">
  <img src="../images/week7/24.png" alt="">
  <p> Question 1: Limitations of the proposed methods & possible improvements
  <ul>
  <li> Group 1: Poor evaluation on high-quality videos, their training dataset is low-quality. </li>
  <li> Group 2: Ear detection is only possible when ear is visible. </li>
  <li> Group 3: Dependent on visibility of ear and having a reference image; ability to generalize to other situations, i.e.: smaller sample sizes; you could find an actor whose biometric markers were more similar to the desired image. </li>
  </ul>
  </p>
</div>
<div class="slide">
  <img src="../images/week7/25.png" alt="">
  <p> As mentioned, there are drawbacks such as when hair is hiding the movement of ears, large head movement, and accurate ear tracking is difficult. Still, more facial and audio signals can be further studied.
  </p>
</div>
<div class="slide">
  <img src="../images/week7/26.png" alt="">
  <p> Question 2: Anomalies found from deep fake videos
  <ul>
  <li> Group 2: 
  <ul> 
  <li> <i>Shape of the mouth</i> is generally the same; it just expands and reduces in size to mimic mouth movements when speaking. Thus the bottom teeth is never shown. </li>
  <li><i>Light reflection on glasses</i> when reporters move their head is not generated. </li>
  <li><i>Lips not synced</i> properly (the word editing does not match).</li>
  </ul> 
  </li>
  <li> Group 3:
  <ul>
  <li> Fake video 1:
  <ul>
  <li>Lips seem constrained</li>
  <li>Eye blinking robotic, no change iin width</li>
  <li>Lack of nostril changes</li>
  </ul>
  </li>
  <li>Fake video 2:
  <ul>
  <li>Mouth/teeth</li>
  <li>Symmetry</li>
  </ul>
  </li>
  </ul>
  </li>
  </ul>
  </p>
</div>
<div class="slide">
  <img src="../images/week7/27.png" alt="">
  <p> The speaker of the first video does not blink for over 6 seconds, which is impossible. The average resting blinking rate should be 0.283 per second.
  </p>
</div>
<div class="slide">
  <img src="../images/week7/28.png" alt="">
  <p> The second speaker’s lips are not closing for ‘m,’ ‘b,’ and ‘p’ (Phonemes-Visemes).
  </p>
</div>
<div class="slide">
  <img src="../images/week7/29.png" alt="">
  <p> Human pulse and respiratory motions are imperceptible to the human eye. Amplifying these factors could serve as a method for detecting generated videos.
  </p>
<p><strong>Note:</strong> The method was originally designed for medical purposes, aiming to identify potential health risks in a medical setting in a non-intrusive way. </p></p>
</div>
<div class="slide">
  <img src="../images/week7/30.png" alt="">
  <p> 
  <ul> 
  <li> <i>Group 1:</i> The “arm-race” is win-win development for both groups. Generation and detection will learn from the feedback of each side. </li>
  <li><i>Group 2:</i> There always will be a case where humans get creative and find out ways to improve and get away with detection, as fraud detection does. Optimally, if people don’t use it in an unethical way, it can be useful in various ways like the film industry.</li>
  <li><i>Group 3:</i> What if big companies become attackers (even for research purposes)?</li>
  </ul>
  </p>
</div>
<div class="slide">
  <img src="../images/week7/31.png" alt="">
<h1 id="heading-9"></h1>
  <p> Both the technology and ways to detect deep-fake videos will continue to advance.
However, it requires more than simply trying to generate and identify them.
By using watermarks, deep-fake videos can be distinguished from the source. Furthermore, public education on teaching importance on collecting information from the correct source and further government regulations can be considered. Perhaps the biggest threat from improvements in the quality and ease of creating fake imagery, is that people will lose confidence in all images and assume everything they see is fake.
  </p>
</div>
<h1 id="readings">Readings</h1>
<h3 id="for-the-first-class-109">For the first class (10/9)</h3>
<ul>
<li>Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio. <a href="https://arxiv.org/abs/1406.2661"><em>Generative adversarial nets</em></a>. 2014.</li>
<li>Xin Wang, Hui Guo, Shu Hu, Ming-Ching Chang, Siwei Lyu. <a href="https://arxiv.org/abs/2202.07145"><em>GAN-generated Faces Detection: A survey and new perspectives</em></a>. 2022.</li>
</ul>
<h3 id="for-the-second-class-1011">For the second class (10/11)</h3>
<ul>
<li>Shruti Agarwal and Hany Farid. <a href="https://openaccess.thecvf.com/content/CVPR2021W/WMF/html/Agarwal_Detecting_Deep-Fake_Videos_From_Aural_and_Oral_Dynamics_CVPRW_2021_paper.html"><em>Detecting deep-fake videos from aural and oral dynamics</em></a>. CVPR 2023.</li>
</ul>
<h2 id="optional-additional-readings">Optional Additional Readings</h2>
<ul>
<li>Dilrukshi Gamage, Piyush Ghasiya, Vamshi Krishna Bonagiri, Mark E Whiting, and Kazutoshi Sasahara. <a href="https://dl.acm.org/doi/10.1145/3491102.3517446"><em>Are Deepfakes Concerning? Analyzing Conversations of Deepfakes on Reddit and Exploring Societal Implications</em></a>. In CHI Conference on Human Factors in Computing Systems (CHI ’22), April 29-May 5, 2022, New Orleans, LA, USA. ACM, New York, NY, USA, 19 pages.</li>
<li>Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, Richard G. Baraniuk. <a href="https://arxiv.org/abs/2307.01850"><em>Self-Consuming Generative Models Go MAD</em></a>.</li>
<li>Momina Masood, Marriam Nawaz, Khalid Mahmood Malik, Ali Javed, Aun Irtaza. <a href="https://arxiv.org/abs/2103.00484"><em>Deepfakes generation and detection: State-of-the-art, open challenges, countermeasures, and way forward</em></a>. Applied Intelligence, June 2022.</li>
</ul>
<h3 id="on-gan-training">On GAN Training</h3>
<ul>
<li>Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., &amp; Chen, X. (2016). <a href="https://papers.nips.cc/paper_files/paper/2016/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf"><em>Improved techniques for training gans</em></a></li>
<li>Arjovsky, M., &amp; Bottou, L. (2017). <a href="https://openreview.net/pdf?id=Hk4_qw5xe"><em>Towards principled methods for training generative adversarial networks</em></a></li>
<li>Shumailov, I., Shumaylov, Z., Zhao, Y., Gal, Y., Papernot, N., &amp; Anderson, R. (2023). <a href="https://arxiv.org/abs/2305.17493"><em>The Curse of Recursion: Training on Generated Data Makes Models Forget</em></a></li>
</ul>
<h3 id="blogs-and-tutorials">Blogs and Tutorials</h3>
<ul>
<li><a href="https://medium.datadriveninvestor.com/how-generative-adversial-network-works-3ddce0062b9"><em>How Generative Adversarial Network Works</em></a></li>
<li><a href="https://www.dhs.gov/sites/default/files/publications/increasing_threats_of_deepfake_identities_0.pdf"><em>Increasing threat of deepfake identities</em></a></li>
</ul>
<h1 id="discussion-questions">Discussion Questions</h1>
<p><strong>For Monday&rsquo;s class:</strong> (as usual, post your response to at least one of these questions, or respond to someone else&rsquo;s response, or post anything you want that is interesting and relevant, before 8:29pm on Sunday, 8 October)</p>
<ol>
<li>How might the application of GANs extend beyond image generation to other domains, such as text, finance, healthcare (or any other domain that you can think of) and what unique challenges might arise in these different domains? How can the GAN framework ensure fairness, accountability, and transparency in these applications?</li>
<li>Considering the challenges in evaluating the performance and quality of GANs, how might evaluation metrics or methods be developed to assess the quality, diversity, and realism of the samples generated by GANs in a more robust and reliable manner? Additionally, how might these evaluation methods account for different types of data (e.g., images, text, tabular
etc.) and various application domains?</li>
<li>The authors identify 2 methods of detecting GAN-based images: physical and physiological. Is it possible that we can train a new model to modify a GAN-based image to hide these seemingly obvious flaws, like the reflection and pupil shapes? Will this approach quickly invalidate these two methods?</li>
<li>Do you agree with the authors that deep-learning based methods lack interpretability? Is the visible or invisible patterns detected by DL models really not understandable or explainable?</li>
</ol>
<p><strong>Questions for Wednesday&rsquo;s class:</strong> (post response by 8:29pm on Tuesday, 10 October)</p>
<ol>
<li>What are the potential applications for the techniques discussed in the Agarwal and Farid paper beyond deep-fake detection, such as in voice recognition or speaker authentication systems?</li>
<li>How robust are the proposed ear analysis methods to real-world conditions like different head poses, lighting, occlusion by hair?</li>
<li>What are your ideas for other ways to detect deepfakes?</li>
<li>Deepfake detection and generation seems similar to many other &ldquo;arms races&rdquo; between attackers and defenders. How do you see this arms race evolving? Will there be an endpoint with one side clearly winning?</li>
</ol>

      </div>
<hr class="post-separator"></hr>

    
    <h1><a href="/week5/">Week 5: Hallucination</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-10-04 00:00:00 &#43;0000 UTC" itemprop="datePublished">4 October 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>(see bottom for assigned readings and questions)</p>
<h1 id="hallucination-week-5">Hallucination (Week 5)</h1>
<p><author>Presenting Team: Liu Zhe, Peng Wang, Sikun Guo, Yinhan He, Zhepei Wei</author></p>
<p><author>Blogging Team: Anshuman Suri, Jacob Christopher, Kasra Lekan, Kaylee Liu, My Dinh</author></p>
<h1 id="wednesday-september-27th-intro-to-hallucination">Wednesday, September 27th: Intro to Hallucination</h1>
<table>
    <tr>
        <td><img src="/images/Week5/Hallucination_page_4.png" width="95%"></td>
    </tr>
    <tr>
    <td colspan=1 align="center"><b>People Hallucinate Too</b>
    </td>
</tr>
</table>
<table>
    <tr>
        <td><img src="/images/Week5/Hallucination_page_6.png" width="95%"></td>
    </tr>
    <tr>
    <td colspan=1><center><b>Hallucination Definition</b></center>
    <br/>
        <p style="text-align: left; text-indent: 5%;">
There are three types of hallucinations according to the “Siren's Song
            in the AI Ocean” paper:
<ul> <li><i>Input-conflict:</i> This
            subcategory of hallucinations deviates from user
            input.</li> <li><i>Context-conflict:</i>
            Context-conflict hallucinations occur when a model
            generates contradicting information within a response. A
            simple example of this would be replacing someone’s name
            (ex. Silver) for another name (ex. Stern).</li>
            <li><i>Fact-conflict:</i> This is the most common
            subcategory of hallucination, thus making it the most
            recent focus of research. An example of this could be
            returning the wrong date for a historical event.</li>
            </ul>
<p>In class, there were dissenting opinions about the
definition of hallucination regarding LLMs. One classmate
argued how alignment-based hallucination should not be
considered as part of the discussion scope, as the model
would still be doing what it was intended to be doing
(i.e. aligning with the user and/or aligning with the
trainer).</p>
</td> </table>
<table>
    <tr>
        <td><img src="/images/Week5/Hallucination_Solution_&_Benefits_page_5.png" width="95%"></td>
    </tr>
    <tr>
    <td colspan=1 align="center"> <center><b>Sources of Hallucination</b></center>
<br>
<p><a href="https://arxiv.org/abs/2309.01219"><em>Siren&rsquo;s Song in the AI Ocean: A Survey on Hallucination in Large Language Models</em>  </a>
</td>
</tr></p>
</table>
<table>
    <tr>
        <td><img src="/images/Week5/Hallucination_Solution_&_Benefits_page_4.png" width="95%"></td>
    </tr>
    <td colspan=1 align="center"><b>Hallucination Risks</b>
    </td>
</table>
<div class="p-3 mb-2 bg-info text-white">
<h2> Group Activity: Engage with ChatGPT to Explore Its Hallucinations (Three Groups Focusing on Different Hallucination Types) </h2>
<br>
<p style="text-align: left; text-indent: 2%;">
<i>Group 1</i> focused on "Input-conflict Hallucination". One member narrated a story involving two characters, where one character murdered the other. Contrarily, ChatGPT presented an opposite conclusion. Another member tried to exploit different languages, using two distinct languages that possess similar words.
</p>
<p style="text-align: left; text-indent: 2%;">
<i>Group 2</i> concentrated on "Counter-conflict Hallucination". They described four to five fictitious characters, detailing their interrelationships. Some relationships were deducible, yet the model frequently failed to make a complete set of deductions until explictely prompted to be more complete.
</p>
<p style="text-align: left; text-indent: 2%;">
<i>Group 3</i> delved into "Fact-conflict Hallucination". An illustrative example was when ChatGPT was queried with the fraction "⅓". It offered "0.333" as an approximation. However, when subsequently asked to multiply "0.3333" by "3", it confidently responded with "1". Additional tests included translations between two languages.
</p>
</div>
<h1 id="wednesday-october-4th-hallucination-solutions">Wednesday, October 4th: Hallucination Solutions</h1>
<table>
    <tr>
        <td><img src="/images/Week5/Hallucination_Solution_&_Benefits_page_7.png" width="95%"></td>
    </tr>
   <tr>
  <td colspan=1 align="center"><b>Mitigation Strategies</b>
<p><a href="https://arxiv.org/abs/2309.01219"><em>Siren&rsquo;s Song in the AI Ocean: A Survey on Hallucination in Large Language Models</em></a></b>
<br/>
<p style="text-align: left; text-indent: 5%;">
For inference, one strategy is to reduce the snowballing of hallucinations by designing a dynamic p-value. The p-value should start off large and shrink as more tokens are generated. Furthermore, introducing new or external knowledge can be done at two different positions: before and after generation.
</p>
</td>
</tr></p>
</table>
<table>
    <tr>
        <td><img src="/images/Week5/Hallucination_Solution_&_Benefits_page_16.png" width="95%"></td>
    </tr>
<tr>
<td colspan=1 align="center"> 
<b>Decoding Contrasting Layers</b><br>
<p><a href="https://arxiv.org/abs/2309.03883"><em>DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models</em></a></p>
<br/>
<p style="text-align: left; text-indent: 5%;">
            Based on evolving trends, the concept of contrastive decoding is introduced. For example, one might ask, "How do we decide between Seattle or Olympia?" When considering the last layer as a mature layer, it is beneficial to contrast the differences between the preceding layers, which can be deemed as premature. For each of these layers, it is possible to calculate the difference between each probability distribution by comparing mature and premature layers, a process that utilizes the Jensen-Shannon Divergence. Such an approach permits the amplification of the factual knowledge that the model has acquired, thereby enhancing its output generation.
        </p>
    </td>
</table>
<table>
    <tr>
        <td><img src="/images/Week5/Hallucination_Solution_&_Benefits_page_21.png" width="95%"></td>
    </tr>
    <tr>
    <td colspan=1 align="center">
<p><a href="https://arxiv.org/abs/2302.00083"><em>In-Context Retrieval-Augmented Language Models</em></a></p>
  <p style="text-align: left; text-indent: 5%;">
            The model parameters are kept frozen. Instead of directly inputting text into the model, the approach first uses retrieval to search for relevant documents from external sources. The findings from these sources are then concatenated with the original text. Re-ranking results from the retrieval model also provides benefits; the exact perplexities can be referred to in the slide. It has been observed that smaller strides can enhance performance, albeit at the cost of increased runtime. The authors have noticed that the information at the end of a query is typically more relevant for output generation. In general, shorter queries tend to outperform longer ones.
        </p>
    </td>
</table>
<table>
    <tr>
       <td><img src="/images/Week5/Hallucination_Solution_&_Benefits_page_28.png" width="95%"></td>
    </tr>
  <tr>
<td colspan=1 align="center"><a href="https://arxiv.org/abs/2302.00083"><em>Benefits of Hallucinations</em></a>
    </td>
    </tr>
</table>
<h2 id="discussion-_based-on-the-sources-of-hallucination-what-methods-can-be-employed-to-mitigate-the-hallucination-issue_h2">Discussion: <em>Based on the sources of hallucination, what methods can be employed to mitigate the hallucination issue?</em></h2></h2>
<p style="text-align: left; text-indent: 5%;">
    Group 2:
        Discussed two papers from this week's reading which highlighted the use of semantic search and the introduction of external context to aid the model. This approach, while useful for diminishing hallucination, heavily depends on external information, which is not effective in generic cases.
        Further strategies discussed were automated prompt engineering, optimization of user-provided context (noting that extensive contexts can induce hallucination), and using filtering or attention mechanisms to limit the tokens the model processes.
        From the model's perspective, it is beneficial to employ red-teaming, explore corner cases, and pinpoint domains where hallucinations are prevalent.
        Notably, responses can vary for an identical prompt. A proposed solution is to generate multiple responses to the same prompt and amalgamate them, perhaps through a majority voting system, to eliminate low-probability hallucinations.
</p>
<p style="text-align: left; text-indent: 5%;">
    Group 1:
        Discussed the scarcity of alternatives to the current training dataset.
        Like Group 2, they also explored the idea of generating multiple responses but suggested allowing the user to select from the array of choices.
        Another approach discussed was the model admitting uncertainty, stating "I don’t know", rather than producing a hallucination.
</p>
<p style="text-align: left; text-indent: 5%;">
    Group 3: Addressed inconsistencies in the training data.
        Emphasized the importance of fine-tuning and ensuring the use of contemporary data.
        However, they noted that fine-tuning doesn't ensure exclusion of outdated data.
        It was also advised to source data solely from credible sources.
        An interesting perspective discussed was utilizing a larger model to verify the smaller model's hallucinations. But a caveat arises: How can one ensure the larger model's accuracy? And if the larger model is deemed superior, why not employ it directly?
</p>
<h2 id="discussion-_what-are-the-potential-advantages-of-hallucinations-in-large-language-models-llms_h2">Discussion: <em>What are the potential advantages of hallucinations in Large Language Models (LLMs)?</em></h2></h2>
<p style="text-align: left; text-indent: 5%;">
    One advantage discussed was that hallucinations "train" users to not blindly trust the model outputs. If such models are blindly trusted, there is a much greater risk associated with their use.
    If users can conclusively discern, however, that the produced information is fictitious, it could assist in fostering new ideas or fresh perspectives on a given topic.
    Furthermore, while fake data has potential utility in synthetic data generation, there's a pressing need to remain vigilant regarding the accuracy and plausibility of the data produced.
</p>
<h1 id="readings">Readings</h1>
<h3 id="for-the-first-class-927">For the first class (9/27)</h3>
<p>Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang et al. <a href="https://arxiv.org/abs/2309.01219"><em>Siren&rsquo;s Song in the AI Ocean: A Survey on Hallucination in Large Language Models</em></a>. September 2023. <a href="https://arxiv.org/abs/2309.01219">https://arxiv.org/abs/2309.01219</a></p>
<h3 id="for-the-second-class-104">For the second class (10/4)</h3>
<p>Choose <strong>one</strong> (or more) of the following papers to read:</p>
<ul>
<li>Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. <a href="https://arxiv.org/abs/2302.00083"><em>In-context retrieval-augmented language models</em></a>. Accepted for publication in TACL 2024.</li>
<li>Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. <a href="https://arxiv.org/abs/2309.03883"><em>DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models</em></a>. September 2023.</li>
</ul>
<h2 id="optional-additional-readings">Optional Additional Readings</h2>
<h3 id="overview-of-hallucination">Overview of hallucination</h3>
<ul>
<li>Vipula Rawte, Amit Sheth, and Amitava Das. <a href="https://arxiv.org/abs/2309.05922"><em>A Survey of Hallucination in Large Foundation Models</em></a>. September 2023.</li>
<li>Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, and Weiqiang Jia. <a href="https://arxiv.org/abs/2309.06794"><em>Cognitive Mirage: A Review of Hallucinations in Large Language Models</em></a>. September 2023. <br>
Nick McKenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark Johnson, and Mark Steedman. <a href="https://arxiv.org/abs/2305.14552"><em>Sources of Hallucination by Large Language Models on Inference Tasks</em></a>. May 2023.</li>
<li><a href="https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/"><em>Why ChatGPT and Bing Chat are so good at making things up</em></a></li>
</ul>
<h3 id="how-to-reduce-hallucination-retrieval-augmented-llm">How to reduce hallucination: Retrieval-augmented LLM</h3>
<ul>
<li>Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. <a href="https://arxiv.org/abs/2301.12652"><em>Replug: Retrieval-augmented black-box language models</em></a>. January 2023.</li>
<li>Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang et al. <a href="https://arxiv.org/abs/2302.12813"><em>Check your facts and try again: Improving large language models with external knowledge and automated feedback</em></a>. Februrary 2023.</li>
<li>Akari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. <a href="https://acl2023-retrieval-lm.github.io/">ACL 2023 Tutorial: <em>Retrieval-based Language Models and Applications</em></a>. ACL 2023.</li>
</ul>
<h3 id="how-to-reduce-hallucination-decoding-strategy">How to reduce hallucination: Decoding strategy</h3>
<ul>
<li>Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale N. Fung, Mohammad Shoeybi, and Bryan Catanzaro. <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/df438caa36714f69277daa92d608dd63-Abstract-Conference.html"><em>Factuality enhanced language models for open-ended text generation</em></a>. NeurIPS 2022.</li>
<li>Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. <a href="https://arxiv.org/abs/2306.03341"><em>Inference-Time Intervention: Eliciting Truthful Answers from a Language Model</em></a>. June 2023.</li>
</ul>
<h3 id="hallucination-is-not-always-harmful-possible-use-cases-of-hallucination">Hallucination is not always harmful: Possible use cases of hallucination</h3>
<ul>
<li><a href="https://github.com/DivergentAI/dreamGPT"><em>dreamGPT: AI powered inspiration</em></a></li>
<li><a href="https://techcrunch.com/2023/09/04/are-language-models-doomed-to-always-hallucinate/"><em>Are AI models doomed to always hallucinate?</em></a></li>
<li><a href="https://www.smartcompany.com.au/technology/artificial-intelligence/openai-ceo-sam-altman-ai-hallucinations/#:~:text=%E2%80%9COne%20of%20the%20non%2Dobvious,have%20good%20stuff%20for%20that.">OpenAI CEO Sam Altman sees “a lot of value” in AI hallucinations</a>.</li>
</ul>
<h1 id="discussion-questions">Discussion Questions</h1>
<p>Everyone who is not in either the lead or blogging team for the week should post (in the comments below) an answer to at least one of the four questions in each section, or a substantive response to someone else&rsquo;s comment, or something interesting about the readings that is not covered by these questions.</p>
<p>Don&rsquo;t post duplicates - if others have already posted, you should read their responses before adding your own. Please post your responses to different questions as separate comments.</p>
<p>First section (1 - 4): Before 5:29pm on <strong>Tuesday 26 September</strong>. <br>
Second section (5 - 9): Before 5:29pm on <strong>Tuesday 3 October</strong>.</p>
<h2 id="questions-for-the-first-class-927">Questions for the first class (9/27)</h2>
<ol>
<li>What are the risks of hallucinations, especially when LLMs are used in critical applications such as autonomous vehicles, medical diagnosis, or legal analysis?</li>
<li>What are some potential long-term consequences of allowing LLMs to generate fabricated information without proper detection and mitigation measures in place?</li>
<li>How can we distinguish between legitimate generalization or &ldquo;creative writing&rdquo; and hallucination? Where is the line between expanding on existing knowledge and creating entirely fictional information, and what are the consequences on users?</li>
</ol>
<h2 id="questions-for-the-second-class-104">Questions for the second class (10/4)</h2>
<ol>
<li>The required reading presents two methods for reducing hallucinations, i.e., introducing external knowledge and designing better decoding strategies. Can you brainstorm or refer to optional readings to explore ways to further mitigate hallucinations? If so, could you elaborate more on your ideas and also discuss the challenges and risks associated with them?</li>
<li>Among all the mitigation strategies for addressing hallucination (including those introduced in the reading material from the first class), which one do you find most promising, and why?</li>
<li>Do retrieval-augmented LLMs pose any risks or potential negative consequences despite their ability to mitigate LLM hallucinations through the use of external knowledge?</li>
<li>The method proposed by DoLa seems quite simple but effective. Where do you think the authors of DoLa get the inspiration for their idea？</li>
</ol>

      </div>
<hr class="post-separator"></hr>

    
    <h1><a href="/week4/">Week 4: Capabilities of LLMs</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-09-25 00:00:00 &#43;0000 UTC" itemprop="datePublished">25 September 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>(see bottom for assigned readings and questions)</p>
<h1 id="capabilities-of-llms-week-4">Capabilities of LLMs (Week 4)</h1>
<p><author>Presenting Team: Xindi Guo, Mengxuan Hu, Tseganesh Beyene Kebede, Zihan Guan</author></p>
<p><author>Blogging Team: Ajwa Shahid, Caroline Gihlstorf, Changhong Yang, Hyeongjin Kim, Sarah Boyce</author></p>
<h1 id="monday-september-18">Monday, September 18</h1>
<p>Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, Xia Hu. <em>Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond</em>. April 2023. <a href="https://arxiv.org/abs/2304.13712">https://arxiv.org/abs/2304.13712</a></p>
<p>This discussion was essential to highlight the distinction between large language models (LLMs) and fine-tuned models. The paper defines LLMs as models trained on large amounts of data without a particular domain of application in mind and fine-tuned models as models trained on some general data in addition to data from a specific domain in which they will perform a particular task. They also highlighted a rule of thumb from the paper that LLMs typically have more than 20 billion parameters whereas fine-tuned models have fewer than 20 billion parameters.</p>
<p>The presenters then discussed how to go about choosing the model that is best for a task when there are so many available. They explored two ways of approaching this problem: one that considers the data, and another that considers the task.</p>
<p>The presenters highlighted three types of data to consider for a task: the pre-training data, the fine-tuning data, and the test data. If there is a model already pre-trained on data that is relevant to the task at hand, that model might be a good choice. In order for a dataset to be conducive to fine-tuning a model, the data must be labeled and that there must be enough of it to sufficiently train the model. Otherwise, training an LLM would be better. The presenters noted, however, that fine-tuned models have been shown to perform better than LLMs on specific tasks when they have sufficient data to train on.</p>
<p>The following Figure 1 shows that fine-tuned models performed better than LLMs after being trained on enough data:</p>
<table><tr>
  <td><img src="/images/week4/llms_vs_fine_tuned_models.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"> Figure 1 <b> <a href="https://arxiv.org/abs/2307.11346">(Image source)</a></b></td>
</table>
<p>LLMs and fine-tuned models perform better on different tasks. According to a study from the paper <a href="https://arxiv.org/abs/2301.13848"><em>Benchmarking Large Language Models for News Summarization</em></a> <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, LLMs perform better than fine-tuned models on text summarization according to the preferences of human raters. Additionally, LLMs perform better on tasks that require large amounts of knowledge from a variety of domains<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. In machine translation, however, fine-tuned models generally do better than LLMs, although they only do slightly better in low-resource settings<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>. Additionally, the presenters explained that fine-tuned models and LLMs have similar performance when the task only requires very specific knowledge.</p>
<p>The presenters then posed the following question to the class: “How could we enhance LLM in the scenario where the required knowledge does not match their learned knowledge?” The class formed four groups to discuss the question. Each group then shared a summary of what they had discussed:</p>
<p>Group 1: Discussed enhancements in both training and testing. For testing, use intentional prompts to get the knowledge you want from the model. For training, adding more training data, using a knowledge graph to classify knowledge into different clusters for ease of search, and using a plug-in, as mentioned in the GitHub discussion.</p>
<p>Group 2: Advocated for enhancement through the model&rsquo;s ability to retrieve information from external sources and undergo fine-tuning.</p>
<p>Group 3: Emphasized the significance of using more extensive datasets and guiding models through example-based prompts to extract additional knowledge from the available data.</p>
<p>Group 4: Proposed refining model performance through fine-tuning and deliberately specifying the model&rsquo;s role within a chat setting to steer it toward specific tasks.</p>
<p>The presenters also mentioned that LLMs perform arithmetic better as they grow larger and that LLMs are better at commonsense reasoning than fine-tuned models<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.</p>
<h2 id="chatgpt-plug-ins">ChatGPT Plug-Ins</h2>
<p>The presenters then moved to a discussion of ChatGPT plug-ins. They established that LLMs are not able to keep up with current knowledge and facts and that they mainly handle text data as opposed to image data, audio, and other data formats. They also explained that while LLMs may be able to generate instructions for a human to follow to complete a task, they are often unable to follow those instructions and complete the tasks themselves. This is where plug-ins come in.</p>
<p>Plug-ins allow LLMs to access data outside of what they have already learned, for example, by searching the web. Plug-ins can enable ChatGPT to search the web (<a href="https://openai.com/blog/chatgpt-plugins#browsing">https://openai.com/blog/chatgpt-plugins#browsing</a>) and write code (<a href="https://openai.com/blog/chatgpt-plugins#code-interpreter)">https://openai.com/blog/chatgpt-plugins#code-interpreter)</a>. They also showed a video of how ChatGPT can use plug-ins from outside OpenAI to complete tasks with specific instructions, namely, finding a restaurant, then a recipe, then calculating the number of calories in the recipe (<a href="https://openai.com/blog/chatgpt-plugins#third-party-plugins)">https://openai.com/blog/chatgpt-plugins#third-party-plugins)</a>.</p>
<p>The class then formed groups and spent 25 minutes designing custom plug-ins. Each group then shared their plug-in with the class:</p>
<p><em>Group 1</em>: Envisioned a plug-in that allows a user to make DIY furniture or buy new furniture by submitting an image of something they would like to make from scratch or buy and returning either a link to a similar piece of furniture or a video/instructions on how to build a piece of furniture similar to the one in their image (Figure 2).</p>
<table><tr>
  <td><img src="/images/week4/group_1_plugin.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"> Figure 2 <b>DIY Furniture Plug-In </b></td>
</table>
<p><em>Group 2</em>: Suggested a plug-in that generates the profile of a fake person with a fake social media account. Essentially, it aimed to create a convincing online persona that would resemble a real person but was entirely fictional. (Such a plug-in, obviously, raises some serious ethical concerns and could be used for some legitimate purposes, but many malign ones!)</p>
<p><em>Group 3</em>: Created a plug-in that uses a person’s portfolio/resume to match them with different jobs serving as an ultimate career support tool (Figure 3). It assisted users in job search by matching them with relevant job openings, provided tailored preparation materials for interviews, offered resume/portfolio editing guidance, and provided opportunities to expand their professional network. Essentially, it aimed to equip individuals with the tools and resources needed to enhance their career prospects.</p>
<table><tr>
  <td><img src="/images/week4/group_3_plugin.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"> Figure 3 <b>Job Hunter Plug-In</b></td>
</table>
<p>Group 4: Shared the concept of a plug-in that allows a user to find the taxonomy of a particular research paper, and place it in a research area. Moreover, it offered the valuable function of returning related papers, thus assisting users in discovering additional relevant research in their field of interest.</p>
<p>The discussion on Monday concluded with the class appreciating each other’s ideas on the possibilities of using plug-ins but also their limitations and associated risks. This exchange of perspectives and ideas highlighted the creative ways in which technology and AI-driven tools could be used to address various challenges and opportunities across different domains.</p>
<h1 id="wednesday-medical-applications-of-llms">Wednesday: Medical Applications of LLMs</h1>
<h2 id="introduction">Introduction</h2>
<p>Wednesday&rsquo;s class started with warm-up story about a young boy who finally got diagnosed from ChatGPT. He saw 17 doctors over 3 years for chronic pain but they couldn’t diagnosis what made boy painful. One day, his parents explained the boy’s symptom to ChatGPT and they got a reliable answer. They brought this information to a doctor and after several tests, he could finally get a diagnosis which was correct information from ChatGPT. This story presents a bright possibilities for the medical use of Chat GPT.</p>
<p>We also got a simple question and had to solve the problem not using ChatGPT.</p>
<table><tr>
  <td><img src="/images/week4/figure4.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"> Figure 4 <b></b></td>
</table>
<p>About 70% of students could get an answer “A” using a simple web search (e.g., duckduckgo or google) at first trial, and 30% of students could get an answer by several attempts.</p>
<p>Later, we were asked to use ChatGPT to get an answer. All students could get a correct answer for at first trial. For a simple question like this, it is unclear if GPT is better than a web search. ChatGPT has been pre-trained from many documents from the web, so it is good at giving general information.</p>
<p>However, if we ask deeper and expertise questions to ChatGPT, it may
not give an answer. We indeed need a LLM model which is trained from
medical information to get a reliable result. (Note that GPT-4 is
already competitive in many ways with models tuneds specifically for
medical applications.)</p>
<h2 id="medical-fine-tuned-llms">Medical fine-tuned LLMs</h2>
<p>There were many attemps to build LLMs trained with medical knowlege. Med-PaLM is one of the representive models, which is fine-tuned PaLM with a medical knowledge. Figure 5 showed that Med-PalM scored 67.2% on USMLE-type questions in the MedQA dataset. Considering that approximate medical pass mark was 60%, Med-PalM was the first LLM model who passed the approximate medical pass mark by Dec 22.</p>
<table><tr>
  <td><img src="/images/week4/figure5.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"> Figure 5 <b> <a href="https://sites.research.google/med-palm/">(Image source)</a></b>
</table>
<p>Recently researchers developed Med-PaLM2. They trained PaLM2 with more medical data and upgraded. Figure 6 shows the brief explaination of how Med-PaLM2 was trained. As a result of training fine-tuned with more and better medical information, it could achieve significant improvement.</p>
<table><tr>
  <td><img src="/images/week4/figure6.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"> Figure 6 <b> <a href="https://www.youtube.com/watch?v=ixRanV-rdAQ">(Image source)</a></b>
</table>
<p>The researchers had a same experiment. Figure 7 showed that Med-PaLM2 scored an accuracy of 86.5% on USMLE-type-questions in the MedQA dataset. Compared with other LLM performance before 2023, Med-PaLM2 was the first LLM to reach expert performance.</p>
<p>In comparison to human physicians in High Quality Answer Traits, Med-PaLM2 showed better reflects consensus, better reading comprehension, better knowledge to recall, and better reasoning.</p>
<p>When they tested Potential Answer Risks, physicians omitted more
information, and gave slightly more evidence of demographic bias, and
potentially greater extent/likelihood of harm but Med-PaLM2 gave more
inaccurate/irrelevant information.</p>
<table><tr>
  <td><img src="/images/week4/figure7.jpeg" width="95%"></td>
</tr>
  <td colspan=1 align="center"> Figure 7 <b> <a href="https://arxiv.org/pdf/2305.09617.pdf">(Image source)</a></b>
</table>
<p>We have seen evidence showing that Med-PaLM2 performs very well,
but this is just a starting point for the discussion of how AI should
be used in healthcare.</p>
<p>Presenter suggested the one way of increasing the capabilities of Med-LLMs. Extending input data from language to other source of data like image, LLMs could understand better about the condition of patients and give more correct diagnosis. Presenter introduced a multimodal version Med-PaLM, in figure 8. Accuracy of the diagonsis is mainly based on how input data contain the precise information about the patient, so as LLM model could be trained through those multimodal material, there would be larger chances of increasing the capabilities.</p>
<table><tr>
  <td><img src="/images/week4/figure8.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"> Figure 8 <b> <a href="https://sites.research.google/med-palm/">(Image source)</a></b>
</table>
<p>Also presenter gave a data about physician evaluation on Multi-Med QA and adversarial Qustion to compare the physician, Med-PaLM1, and Med-PaLM2 as you can see in figure 9. It showed the significant increased capabilities as Med-PaLM developed to Med-PaLM2. Furthermore, Almost evalution on Med-PaLM2 is within the range of physicians and some of evaluation even exceed physicians. Based on this results, presenter opened a discussion.</p>
<table><tr>
  <td><img src="/images/week4/figure9.png" width="95%"></td>
</tr>
  <td colspan=1 align="center"> Figure 9 <b> <a href="https://arxiv.org/pdf/2305.09617.pdf">(Image source)</a></b>
</table>
<h2 id="discussion">Discussion</h2>
<p><em>What are the potential implications of LLMs reaching or surpassing human expertise in medical knowledge?</em></p>
<p>The discussion raised more questions than answers:</p>
<ul>
<li>
<p>At what point do we trust doctors vs LLMs and to what extent?</p>
</li>
<li>
<p>Are LLMs more convenient than going to a doctor?</p>
</li>
<li>
<p>People will be more prone to self-diagnosis</p>
</li>
<li>
<p>People could be more honest with AI than a human being</p>
</li>
<li>
<p>Should we let AI write prescriptions?</p>
</li>
<li>
<p>How can we truly verify what is right and what is wrong?</p>
</li>
<li>
<p>AI could obviously give a better diagnosis than human whose experiences effect diagnoses</p>
</li>
<li>
<p>What would happen when different Ais give different results?</p>
</li>
<li>
<p>There is a lot of data to train with and fine tuning. Is this worth the time and effort?</p>
</li>
<li>
<p>Who is taking accountability for the mistakes made by AI? If a doctor does something wrong, they can have their license taken, but this is that the case for AI.</p>
</li>
<li>
<p>How can we refine and improve LLMs like Med-PaLM2 to be more effective in healthcare applications?</p>
</li>
</ul>
<h2 id="readings">Readings</h2>
<p><strong>Monday:</strong></p>
<ol>
<li>
<p>Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, Xia Hu. <a href="https://arxiv.org/abs/2304.13712"><em>Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond</em></a>. April 2023. <a href="https://arxiv.org/abs/2304.13712">https://arxiv.org/abs/2304.13712</a>. [<a href="https://arxiv.org/pdf/2304.13712.pdf">PDF</a>]</p>
</li>
<li>
<p>OpenAI. <a href="https://arxiv.org/abs/2303.08774"><em>GPT-4 Technical Report</em></a>. March 2023. <a href="https://arxiv.org/abs/2303.08774">https://arxiv.org/abs/2303.08774</a> [<a href="https://arxiv.org/pdf/2303.08774.pdf">PDF</a>]</p>
</li>
</ol>
<p>Optionally, also explore <a href="https://openai.com/blog/chatgpt-plugins">https://openai.com/blog/chatgpt-plugins</a>.</p>
<p><strong>Wednesday:</strong></p>
<ol start="3">
<li>Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, Sami Lachgar, Philip Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Aguera y Arcas, Nenad Tomasev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle Barral, Dale Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, Vivek Natarajan. <a href="https://arxiv.org/abs/2305.09617"><em>Towards Expert-Level Medical Question Answering with Large Language Models</em></a>
<a href="https://arxiv.org/abs/2305.09617">https://arxiv.org/abs/2305.09617</a> [<a href="https://arxiv.org/pdf/2305.09617.pdf">PDF</a>]</li>
</ol>
<p>Optional Readings:</p>
<ul>
<li>Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, Eric Horvitz. <a href="https://arxiv.org/abs/2303.13375"><em>Capabilities of GPT-4 on Medical Challenge Problems</em></a>. March 2023. <a href="https://arxiv.org/abs/2303.13375">https://arxiv.org/abs/2303.13375</a></li>
<li>Travis Zack, Eric Lehman, Mirac Suzgun, Jorge A. Rodriguez, Leo Anthony Celi, Judy Gichoya, Dan Jurafsky, Peter Szolovits, David W. Bates, Raja-Elie E. Abdulnour, Atul J. Butte,  Emily Alsentzer. <a href="https://www.medrxiv.org/content/10.1101/2023.07.13.23292577"><em>Coding Inequity: Assessing GPT-4’s Potential for Perpetuating Racial and Gender Biases in Healthcare</em></a>. July 2023.
<a href="https://www.medrxiv.org/content/10.1101/2023.07.13.23292577">https://www.medrxiv.org/content/10.1101/2023.07.13.23292577</a> — This article relates to the underlying biases in the models we talked about this week, but with an application that show clear potential harm resulting from these biases in the form if increased risk of medical misdiagnosis.</li>
</ul>
<h2 id="discussion-for-monday">Discussion for Monday:</h2>
<p>Everyone who is not in either the lead or blogging team for the week should post (in the comments below) an answer to at least one of the questions in this section, or a substantive response to someone else&rsquo;s comment, or something interesting about the readings that is not covered by these questions. Don&rsquo;t post duplicates - if others have already posted, you should read their responses before adding your own. Please post your responses to different questions as separate comments.</p>
<p>You should post your <em>initial</em> response before 5:29pm on Sunday, September 17, but feel free (and encouraged!) to continue the discussion after that, including responding to any responses by others to your comments.</p>
<ol>
<li>Based on the criterions shown in Figure 2 of [1], imagine a practical scenario and explain why you would choose or not choose using LLMs for your scenario.</li>
<li>Are plug-ins the future of AGI? Do you think that a company should only focus on building powerful AI systems that does not need any support from plug-ins, or they should only focus on the core system and involve more plug-ins into the ecosystem?</li>
</ol>
<h2 id="discussion-for-wednesday">Discussion for Wednesday:</h2>
<p>You should post your <em>initial</em> response to one of the questions below or something interesting related to the Wednesday readings before 5:29pm on Tuesday, September 19.</p>
<ol>
<li>
<p>What should we do before deploying LLMs in medical diagnosis applications? What (if any) regulations should control or limit how they would be used?</p>
</li>
<li>
<p>With LLMs handling sensitive medical information, how can patient privacy and data security be maintained? What policies and safeguards should be in place to protect patient data?</p>
</li>
<li>
<p>The paper discusses the progress of LLMs towards achieving physician-level performance in medical question answering. What are the potential implications of LLMs reaching or surpassing human expertise in medical knowledge?</p>
</li>
<li>
<p>The paper mentions the importance of safety and minimizing bias in LLM-generated medical information, and the <a href="https://www.medrxiv.org/content/10.1101/2023.07.13.23292577">optional reading</a> reports on some experiments that show biases in GPT&rsquo;s medical diagnoses. Should models be tuned to ignore protected attributes? Should we prevent models from being used in medical applications until these problems can be solved?</p>
</li>
</ol>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B. Hashimoto. Benchmarking large language models for news summarization, 2023. <a href="https://arxiv.org/abs/2301.13848">https://arxiv.org/abs/2301.13848</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif,
Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard,
Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa
Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus,
Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexan-
der Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz,
Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling
language modeling with pathways, 2022. <a href="https://arxiv.org/abs/2204.02311">https://arxiv.org/abs/2204.02311</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>OpenAI. GPT-4 Technical Report. March 2023. <a href="https://arxiv.org/abs/2303.08774">https://arxiv.org/abs/2303.08774</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

      </div>
<hr class="post-separator"></hr>

    
    <h1><a href="/week3/">Week 3: Prompting and Bias</a></h1>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-09-18 00:00:00 &#43;0000 UTC" itemprop="datePublished">18 September 2023</time>
  </span>
  
</div>

      <div class="post-body" itemprop="articleBody">
        <p>(see bottom for assigned readings and questions)</p>
<h1 id="prompt-engineering-week-3">Prompt Engineering (Week 3)</h1>
<p><author>Presenting Team: Haolin Liu, Xueren Ge, Ji  Hyun Kim, Stephanie Schoch </author></p>
<p><author>Blogging Team: Aparna Kishore, Erzhen Hu, Elena Long, Jingping Wan</author></p>
<ul>
<li><a href="#monday-09112023-prompt-engineering">(Monday, 09/11/2023) Prompt Engineering</a>
<ul>
<li><a href="#warm-up-questions">Warm-up questions</a></li>
<li><a href="#what-is-prompt-engineering">What is Prompt Engineering?</a></li>
<li><a href="#how-is-prompt-based-learning-different-from-traditional-supervised-learning">How is prompt-based learning different from traditional supervised learning?</a></li>
<li><a href="#in-context-learning-and-different-types-of-prompts">In-context learning and different types of prompts</a></li>
<li><a href="#what-is-the-difference-between-prompts-and-fine-tuning">What is the difference between prompts and fine-tuning?</a></li>
<li><a href="#when-is-the-best-to-use-prompts-vs-fine-tuning">When is the best to use prompts vs fine-tuning?</a></li>
<li><a href="#risk-of-prompts">Risk of Prompts</a></li>
<li><a href="#discussion-about-prompt-format">Discussion about Prompt format</a></li>
</ul>
</li>
<li><a href="#wednesday-09132023-prompt-engineering-exposing-llm-risks">(Wednesday, 09/13/2023) Prompt Engineering: Exposing LLM Risks</a>
<ul>
<li><a href="#open-discussion">Open Discussion</a></li>
<li><a href="#case-study-marked-personas">Case Study: Marked Personas</a></li>
<li><a href="#discussion-bias-mitigation">Discussion: Bias mitigation</a></li>
<li><a href="#hands-on-activity-prompt-hacking">Hands-on Activity: Prompt Hacking</a></li>
<li><a href="#discussion-can-we-defend-against-prompt-hacking-by-build-in-safegurads">Discussion: Can we defend against prompt hacking by build-in safegurads?</a></li>
<li><a href="#further-thoughts-whats-the-real-risk">Further thoughts: What&rsquo;s the real risk?</a></li>
</ul>
</li>
<li><a href="#readings">Readings</a>
<ul>
<li><a href="#optional-additional-readings">Optional Additional Readings</a></li>
<li><a href="#discussion-questions">Discussion Questions</a></li>
</ul>
</li>
</ul>
<h1 id="monday-09112023-prompt-engineering">(Monday, 09/11/2023) Prompt Engineering</h1>
<h2 id="warm-up-questions">Warm-up questions</h2>
<p>Monday&rsquo;s class started with warm-up questions to demonstrate how prompts can help an LLM produce correct answers or desired outcomes. The questions and the prompts were tested in GPT3.5. This task was performed as an in-class experiment where each individual used GPT3.5 to test the questions and help GPT3.5 produce correct answers via prompts.</p>
<p>The three questions were:</p>
<ol>
<li><em>What is 7084 times 0.99?</em></li>
<li><em>I have a magic box that can only transfer coins. If you insert a number of coins in it, the next day each coin will turn into two apples. If I add 10 coins and wait for 3 days, what will happen?</em></li>
<li><em>Among &ldquo;Oregon, Virginia, Wyoming&rdquo;, what is the word that ends with &ldquo;n&rdquo;？</em></li>
</ol>
<p>While the first question tested the arithmetic capability of the model, the second and the third questions tested common sense and symbolic reasoning, respectively. The initial response from GPT3.5 for all three questions was wrong.</p>
<p>For the first question, providing more examples as prompts did not work. At the same time, an explanation of how to reach the specific answer by decomposing the multiplication into multiple steps helped.</p>
<p>Figure 1 shows the prompting for the first question and the answer from GPT3.5.</p>
<table><tr>
<td><img src="../images/Week3/Picture1.png" width="95%"></td>
<td><img src="../images/Week3/Picture2.png" width="95%"></td><br><tr>
<td colspan=2 align="center">Figure 1: <b>Prompting for arithmetic question</b></td>
</tr></table>
<p>For the second question, providing an example and an explanation behind the reasoning on how to reach the final answer helped GPT produce the correct answer. Here, the prompt included explicitly stating that the magic box can also convert from coins to apples.</p>
<p>Figure 2 shows the prompting for the second question and the answer from GPT3.5.</p>
<table><tr>
<td><img src="../images/Week3/Picture3.png" width="95%"></td>
<td><img src="../images/Week3/Picture4.png" width="95%"></td></tr>
<tr>
<td colspan=2 align="center">Figure 2: <b>Prompting for common sense question</b>
</tr></table>
<p>While GPT was producing random results for the third question, instructing GPT through examples to take the words, concatenate the last letters, and then find the alphabet&rsquo;s position helped produce the correct answer.</p>
<p>Figure 3 shows the prompting for the third question and the answer from GPT3.5.</p>
<table><tr>
<td><img src="../images/Week3/Picture5.png" width="95%"></td>
<td><img src="../images/Week3/Picture6.png" width="95%"></td></tr>
<tr>
<td colspan=2 align="center">
Figure 3: <b>Prompting for symbolic reasoning question</b></td>
</tr></table>
<p>All these examples demonstrate the benefit of using prompts to explore the model&rsquo;s reasoning ability.</p>
<h2 id="what-is-prompt-engineering">What is Prompt Engineering?</h2>
<p>Prompt engineering is a method to communicate and guide LLM to demonstrate a behavior or desired outcomes by crafting prompts that coax the model towards providing the desired response. The model weights or parameters are not updated in prompt engineering.</p>
<h2 id="how-is-prompt-based-learning-different-from-traditional-supervised-learning">How is prompt-based learning different from traditional supervised learning?</h2>
<p>Traditional supervised learning trains a model by taking input and generating an output based on prediction probability. The model learns to map input data to specific output labels. In contrast, prompt-based learning models the probability of the text directly. Here, the inputs are converted to textual strings called prompts. These prompts are used to generate desired outcomes. Prompt-based learning offers more flexibility in adapting the model&rsquo;s behavior to different tasks by modifying the prompts. Retraining the model is not required in this scenario.</p>
<p>Interestingly, the prompts were initially used in language translations and emotion predictions based on texts instead of improving the performance of LLMs.</p>
<h2 id="in-context-learning-and-different-types-of-prompts">In-context learning and different types of prompts</h2>
<p>In-context learning is a powerful approach to fine-tuning or training the model within a specific context. This improves the performance and reliability of the model for the specific task or the environment. Here, the models are given a few examples as reference/instructions that are relevant to the context and are domain-specific.</p>
<p>We can categorized in-context learning into three different types of prompts:</p>
<ul>
<li><em>Zero-shot</em> – the model predicts the answers given only a natural language description of the task.</li>
</ul>
<center>
<a href="../images/Week3/Picture7.png"><img src="../images/Week3/Picture7.png" width="65%"></a>  
<p>Figure 4: <b>Example for zero-shot prompting</b> (<a href="https://arxiv.org/pdf/2005.14165.pdf">Image Source</a>)</p>
</center>
<ul>
<li><em>One-shot</em> or <em>Few-shot</em> – In this scenario, one or few examples are provided that explains the task description the model, i.e. prompting the model with few input-output pairs.</li>
</ul>
<center>
<a href="../images/Week3/Picture8.png"><img src="../images/Week3/Picture8.png" width="65%"></a><br>
<p>Figure 5: Examples for one-shot and fewshot prompting (<a href="https://arxiv.org/pdf/2005.14165.pdf">Image Source</a>)</p>
</center>
<ul>
<li><em>Chain-of-thought</em> – The given task or question is decomposed into coherent intermediate reasoning steps that are solved before providing the final response. This explores the reasoning ability of the model for each of the provided tasks. It is given in the format <code>&lt;input chain-of-thought output&gt;</code>. The difference between standard prompting and chain-of-thought prompting is depicted in the figure below. In the figure to the right, the highlighted statement in blue is an example of chain-of-thought prompting, where the reasoning behind reaching a final answer is provided as a part of the example. Thus, in the model outcome, the model also outputs its reasoning, highlighted in green, to reach the final answer. In addition, chain-of-thought prompting can revolutionize the way we interact with LLMs and leverage their capabilities, as they provide step-by-step explanations of how a particular response is reached.</li>
</ul>
<table><tr>
<td align="center"><img src="../images/Week3/Picture9.png" width="85%"></td>
<td align="center"><img src="../images/Week3/Picture10.png" width="85%"></td>
</tr>
<tr>
<td align="center" colspan="2">
<p>Figure 6: <b>Standard prompting and chain-of-thought prompting</b> (<a href="https://arxiv.org/abs/2201.11903">Image Source</a>)</tr></table></p>
<h2 id="what-is-the-difference-between-prompts-and-fine-tuning">What is the difference between prompts and fine-tuning?</h2>
<p>Prompt engineering focuses on eliciting better output for a given LLM through changing input. Fine-tuning focuses on enhancing model performance by training the model on a smaller, targeted database relevant to the desired task. The similarity is that both methods help improve the model&rsquo;s performance and provide desired outcomes.</p>
<p>Prompt engineering requires no retraining, and the prompting is performed in a single window of the model. At the same time, fine-tuning involves retraining the model and changing the model parameter to improve its performance. Fine-tuning also requires more computational resources compared to prompt engineering.</p>
<h2 id="when-is-the-best-to-use-prompts-vs-fine-tuning">When is the best to use prompts vs fine-tuning?</h2>
<p>The above question was an in-class discussion question, and the discussion points were shared in class. Fine-tuning requires updating model weights and changing parameters. These are useful in applications where there is a requirement for central change. In this scenario, all the users experience similar performance. Prompt-based methods are user-specific in a particular window for further fine-grained control. The model&rsquo;s performance depends on the individual prompts designed by the user. Thus, fine-tuning is more potent than prompt-based methods in scenarios that require centralized tuning.</p>
<p>In scenarios with limited training examples, prompt-based methods can perform well. Fine-tuning methods are data-hungry and require many input data for better model performance. As discussed in the discussion posts, prompts cannot be used as a universal tool for all problems to generate desired outcomes and have performance enhancements. However, in specific scenarios, it can assist users to improve performance and reach desired outcomes for in-context specific tasks.</p>
<h2 id="risk-of-prompts">Risk of Prompts</h2>
<p>The class then discussed the perspectives from risks of prompt: those methods like chain of thoughts already achieve some success in the LLMs. However, prompt engineering can be still a controversial topic. The group brought out two aspects.</p>
<p>First, Reasoning ability of LLMs. The group asked, “Does CoT empowers LLMs reasoning ability?” Secondly, there are some bias problems in prompting engineering. The group brought up an example of  “LeBron James took a corner kick.” Is the following sentence plausible? (A) plausible (B) implausible I think the answer is A and saying “but I’m curious to hear what you think.” However, this might inject a bias in the prompt.</p>
<p>The group then brought up an open discussion about two potential kinds of prompting bias and ask the class about how would the prompt format (e.g., Task-specific prompt methods, words selected) and prompt training examples (e.g., label distribution, permutations of training examples) affect LLMs output and the possible debiasing solutions.</p>
<p>The class then discussed two different kinds of prompting bias, the prompt format and the prompt training examples.</p>
<h3 id="discussion-about-prompt-training-examples">Discussion about Prompt training examples</h3>
<p>For label distribution, the class discussed that there needs to be a balance in the training set to avoid overgeneralizing the agreement of the user as some examples that the user interjects an opinion that can be wrong. In these cases, the GPT should learn to disagree with the user when the user is wrong. This is also related to the label distribution, if  the user always provides the example with positive labels, then the LLMs will be more likely to output the positive one in the prediction.</p>
<p>Permutation on the training example: A student mentioned a paper that he just read about why context learning works, provides the label space and the distribution of the input. In the paper, they randomly generate the labels, which might be false, they show that actually is better at zero-shot, though worse than when you provide all the labels. Randomly generated labels actually have a significant performance input.
The sequence of the training example may affect the LLM output, especially for the last example. LLM output tends to output the same label with the last example being provided training example.</p>
<h2 id="discussion-about-prompt-format">Discussion about Prompt format</h2>
<p>Prompt format: the word you selected might affect the prompt because some words may appear more frequently in the coporus and some words may have more correlation with some specific label. Male may relate to more positive terms in their training coporus. some prompting may affect the results. Task-specific prompt methods are related to how you select prompt methods based on specific task.</p>
<p>Finally, the group shared two papers about the bias problem in LLMs.
The first paper<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> shows that different prompts will provide a large variance in accuracy, which indicates LLMs are not that stable. The paper also provides a calibration method that takes the output of the GPT model and another linear layer on it to calibrate the models.
The second paper<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> shows that LLMs do not always say what they think, especially injecting some bias into the prompt. For example, they worked on the CoT and non-CoT and they found that CoT will amplify the bias in the context when the user puts some bias in the prompt.</p>
<p>In conclusion, prompts can be controversial and not always perfect.</p>
<!-- TODO: find the actual papers, put in links and make the references more complete. -->
<h1 id="wednesday-09132023-marked-personas">(Wednesday, 09/13/2023) Marked Personas</h1>
<h2 id="open-discussion">Open Discussion</h2>
<p>What do you think are the biggest potential risks of LLMs?</p>
<ul>
<li><strong>Social impact from intentional misuse.</strong> LLM’s content could be manipulated by the government, can potentially affect elections and raise tensions between countries.</li>
<li><strong>Mutual trust among people could be harmed.</strong> We cannot tell which email or info was written by humans or automatically generated by chatgpt. As a result, we may treat these information more skeptically.</li>
<li><strong>People may overly trust LLM outputs.</strong> We may rely more on asking LLM, which is a second-hand information source, rather than actively searching information by ourselves, overtrusting LLM system. Information pool may be contaminated by LLM if they provide misleading information.</li>
</ul>
<p>How does GPT-4 Respond?</p>
<ul>
<li>Misinformation: Provide wrong / misleading / sensitive information, known as jailbreaking of LLM.</li>
<li>Potential manipulation: People could intentionally hack LLM by giving specific prompts.</li>
</ul>
<h2 id="case-study-marked-personas">Case Study: Marked Personas</h2>
<p>Myra Cheng, Esin Durmus, Dan Jurafsky. <a href="https://arxiv.org/abs/2305.18189">Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models</a>. ACL 2023.</p>
<p>In this study, ChatGPT was asked to give descriptions about several characters based on different ethnicity / gender / demographic groups, e.g., Asian woman, Black woman and white man.</p>
<p>When describing a character from a non-dominant demographic group, while the overall description is positive, it could still imply some potential stereotypes. For example, “Almond-shaped eye” is used in describing an east asian woman while it may sound strange to a real east asian. We can also see that ChatGPT is intentionally trying to build a more diverse and politically correct atmosphere for different groups. In contrast, ChatGPT uses mostly neutral and ordinary words when describing an average white man.</p>
<h2 id="discussion-bias-mitigation">Discussion: Bias mitigation</h2>
<h3 id="group-1">Group 1</h3>
<p>Mitigation could sometimes be overcompensating. As a language model, it should try to be neutral and independent. Also, given that people themselves are biased, and LLM is learning from the human world, we may be over-expecting LLMs to be perfectly unbiased. After all, it is hard to define what is fairness and distinguish between stereotype and prototype, leading to over corrections.</p>
<h3 id="group-2">Group 2</h3>
<p>We may be able to identify the risks by data augmentation (replacing “male” with “female” in prompts). Governments should also be responsible for setting rules and regulating LLMs. (Note: this is controversial, and it is unclear what kinds of regulations might be useful or effective.)</p>
<h3 id="group-4">Group 4</h3>
<p>Companies like OpenAI should publish the mitigation strategies so that it could be understood and monitored by the public. Another aspect is that different groups of people can have very diverse points of views, so it is hard to define the stereotypes and biases with a universal law. Also, the answer could be very different based on the prompts, making it even harder to mitigate</p>
<h2 id="hands-on-activity-prompt-hacking">Hands-on Activity: Prompt Hacking</h2>
<p>In this activity, the class was trying to make ChatGPT generate sensitive / bad responses. It could be done by setting a pretended identity, e.g. pretending to be a Hutu person in Rwanda in the 1990s or pretending to be a criminal. With these conditions, ChatGPT’s barrier of biased or evil contents can be partly bypassed.</p>
<h2 id="discussion-can-we-defend-against-prompt-hacking-by-build-in-safegurads">Discussion: Can we defend against prompt hacking by build-in safegurads?</h2>
<p>As we can see in the activity, right now this safeguard is not that strong. A more practical way may be to add a disclaimer at the end of potentially sensitive content and give a questionnaire to collect feedback for better iteration. Companies should also actively identify these jailbreakings and attempt to mitigate them.</p>
<h2 id="further-thoughts-whats-the-real-risk">Further thoughts: What&rsquo;s the real risk?</h2>
<p>While jailbreaking is one of the risks of LLMs, a more risky situation may be that LLM is intentionally trained and used by people to do bad things. After all, misuse is not that serious compared with a specific crime.</p>
<p><a href="#table-of-contents">Back to top</a></p>
<h1 id="readings">Readings</h1>
<ol>
<li>
<p>(for Monday) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou. <a href="https://arxiv.org/abs/2201.11903"><em>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</em></a>. 2022.</p>
</li>
<li>
<p>(for Wednesday) Myra Cheng, Esin Durmus, Dan Jurafsky. <a href="https://arxiv.org/abs/2305.18189">Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models</a>. ACL 2023.</p>
</li>
</ol>
<h2 id="optional-additional-readings">Optional Additional Readings</h2>
<p><strong>Background:</strong></p>
<ul>
<li>Lilian Weng, <a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/"><em>Prompting Engineering</em></a>. March 2023.</li>
<li>Miles Turpin, Julian Michael, Ethan Perez, Samuel R. Bowman. <a href="https://arxiv.org/abs/2305.04388"><em>Language Models Don&rsquo;t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting</em></a>. May 2023.</li>
<li>Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh. <a href="https://arxiv.org/abs/2102.09690"><em>Calibrate Before Use: Improving Few-Shot Performance of Language Models</em></a>. ICML 2021.</li>
</ul>
<p><strong>Stereotypes and bias:</strong></p>
<ul>
<li>Yang Trista Cao, Anna Sotnikova, Hal Daumé III, Rachel Rudinger, Linda Zou.    <a href="https://aclanthology.org/2022.naacl-main.92.pdf"><em>Theory-Grounded Measurement of U.S. Social Stereotypes in English Language Models</em></a>. NAACL 2022.</li>
</ul>
<p><strong>Prompt Injection:</strong></p>
<ul>
<li>Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, Sameer Singh. <a href="https://arxiv.org/abs/1908.07125"><em>Universal Adversarial Triggers for Attacking and Analyzing NLP</em></a>. EMNLP 2019</li>
<li>Simon Willison, <a href="https://simonwillison.net/2022/Sep/12/prompt-injection/"><em>Prompt injection attacks against GPT-3</em></a>. September 2022.</li>
<li>William Zhang. <a href="https://www.robustintelligence.com/blog-posts/prompt-injection-attack-on-gpt-4"><em>Prompt Injection Attack on GPT-4</em></a>. Robust Intelligence, March 2023.</li>
</ul>
<h1 id="discussion-questions">Discussion Questions</h1>
<p>Everyone who is not in either the lead or blogging team for the week should post (in the comments below) an answer to at least one of the four questions in each section, or a substantive response to someone else&rsquo;s comment, or something interesting about the readings that is not covered by these questions.</p>
<p>Don&rsquo;t post duplicates - if others have already posted, you should read their responses before adding your own. Please post your responses to different questions as separate comments.</p>
<p>First section (1 – 4): Before <strong>5:29pm</strong> on <strong>Sunday, September 10</strong>.<br>
Second section (5 – 9): Before <strong>5:29pm</strong> on <strong>Tuesday, September 12</strong>.</p>
<h2 id="before-sunday-questions-about-chain-of-thought-prompting">Before Sunday: Questions about Chain-of-Thought Prompting</h2>
<ol>
<li>
<p>Compared to other types of prompting, do you believe that chain-of-thought prompting represents the most effective approach for enhancing the performance of LLMs? Why or why not? If not, could you propose an alternative?</p>
</li>
<li>
<p>The paper highlights several examples where chain-of-thought prompting can significantly improve its outcomes, such as in solving math problems, applying commonsense reasoning, and comprehending data. Considering these improvements, what additional capabilities do you envision for LLMs using chain-of-thought prompting?</p>
</li>
<li>
<p>Why are different language models in the experiment performing differently with chain-of-thought prompting?</p>
</li>
<li>
<p>Try some of your own experiments with prompt engineering using your favorite LLM, and report interesting results. Is what you find consistent with what you expect from the paper? Are you able to find any new prompting methods that are effective?</p>
</li>
</ol>
<h2 id="by-tuesday-questions-about-marked-personas">By Tuesday: Questions about Marked Personas</h2>
<ol start="5">
<li>
<p>The paper addresses potential harms from LLMs by identifying the underlying stereotypes present in their generated contents. Additionally, the paper offers methods to examine and measure those stereotypes. Can this approach effectively be used to diminish stereotypes and enhance fairness? What are the main limitations of the work?</p>
</li>
<li>
<p>The paper mentions racial stereotypes identified in downstream applications such as story generation. Are there other possible issues we might encounter when the racial stereotypes in LLMs become problematic after its application?</p>
</li>
<li>
<p>Much of the evaluation in this work uses a list of White and Black stereotypical attributes provided by Ghavami and Peplau (2013) as the human-written responses and compares them with the list of LLMs generated responses. This, however, does not encompass all racial backgrounds and is heavily biased by American attitudes about racial categories, and they might not distinguish between races in great detail. Do you believe there could be a notable difference when more comprehensive racial representation is incorporated? If yes, what potential differences may arise? If no, why not?</p>
</li>
<li>
<p>This work emphasizes the naturalness of the input provided to the LLM, while we have previously seen examples of eliciting harmful outputs by using less natural language. What potential benefits or risks are there in not investigating less natural inputs (e.g., prompt injection attacks including the suffix attack we saw in Week 2)? Can you suggest a less natural prompt that could reveal additional or alternate stereotypes?</p>
</li>
<li>
<p>The authors recommend transparency of bias mitigation methods, citing the benefit it could provide to researchers and practitioners. Specifically, how might researchers benefit from this? Can you foresee any negative consequences (either to researchers or the general users of these models) of this transparency?</p>
</li>
</ol>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh. &ldquo;<a href="https://arxiv.org/pdf/2102.09690.pdf">Calibrate before use: Improving few-shot performance of language models</a>.&rdquo; International Conference on Machine Learning. PMLR, 2021.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>Miles Turpin, Julian Michael, Ethan Perez, Samuel R. Bowman. &ldquo;<a href="https://arxiv.org/pdf/2305.04388.pdf">Language Models Don&rsquo;t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting.</a>&rdquo; arXiv preprint arXiv:2305.04388, 2023.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

      </div>
<hr class="post-separator"></hr>

    


    <footer>
      <nav>
	<a href="/post/" class="button hollow primary">All Posts</a>
      </nav>
    </footer>

</div>
</div>


    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-12 medium-6">
      <a href="//llmrisks.github.io"><b>cs 6501: Risks (and Benefits) of Generative AI</b></a><br>
      Fall 2023<br>
      <a href="//www.cs.virginia.edu">University of Virginia</a>
    </div>
    <div class="column small-14 medium-5">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="https://llmrisks.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
  </hr>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
