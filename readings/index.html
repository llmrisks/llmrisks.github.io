<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>Readings and Topics | Risks (and Benefits) of Generative AI and Large Language Models</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="https://llmrisks.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/fonts.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/finite.css">
    <link rel="shortcut icon" href="/images/uva.png">  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="https://llmrisks.github.io/">Risks (and Benefits) of Generative AI and Large Language Models</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      


	    </div>
	  </div>
	    
	  
	    <div class="top-bar" id="site-menu" >	      
	      <div class="top-bar-title show-for-medium site-title">
		<a href="https://llmrisks.github.io/">Risks (and Benefits) of Generative AI and Large Language Models</a>
	      </div>
	      <div class="top-bar-left">
		<ul class="menu vertical medium-horizontal">
		  
		  
		</ul>
	      </div>
	      <div class="top-bar-right show-for-medium">
		


	      </div>
	    </div>
	  
	</nav>
      
    </header>
    
    <main>
      

<div class="content">
    <article class="article" itemscope itemtype="http://schema.org/Article">
      <h1 itemprop="name" class="pagetitle">Readings and Topics</h1>
      <div class="post-body" itemprop="articleBody">
        <p>This page collects some potential topics and readings for the seminar.</p>
<h2 id="introduction-week-1">Introduction (Week 1)</h2>
<p><a href="https://stanford-cs324.github.io/winter2022/lectures/introduction/">Introduction to Large Language Models</a> (from Stanford course)</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. <a href="https://arxiv.org/abs/1706.03762"><em>Attention Is All You Need</em></a>. <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>. NeurIPS 2017.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. <a href="https://aclanthology.org/N19-1423/"><em>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</em></a>. ACL 2019.</p>
<p>(optional) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever. <a href="/docs/language-models.pdf"><em>Language Models are Unsupervised Multitask Learners</em></a>. OpenAI, 2019.</p>
<p>These two blog posts by Jay Alammar are helpful for understanding attention and Transformers:</p>
<ul>
<li><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)</a></li>
<li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
</ul>
<p>Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan
Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa
Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton,
Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne
Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, Iason
Gabriel. <a href="https://arxiv.org/abs/2112.04359"><em>Ethical and social risks of harm from Language
Models</em></a> DeepMind, 2021. <a href="https://arxiv.org/abs/2112.04359">https://arxiv.org/abs/2112.04359</a></p>
<p>Simon Willison. <a href="https://simonwillison.net/2023/Aug/3/weird-world-of-llms/"><em>Catching up on the weird world of LLMs</em></a>, August 2023.</p>
<h3 id="viewpoint-essays">Viewpoint Essays</h3>
<p>Douglas Hofstadter. <a href="https://archive.is/2VtiC"><em>Gödel, Escher, Bach, and AI</em></a>. The Atlantic. 8 July 2023.</p>
<p>Marc Andreessen. <a href="https://www.thefp.com/p/why-ai-will-save-the-world">AI Will Save the World</a>. 11 July 2023</p>
<p>Paul Kingsnorth. <a href="https://www.thefp.com/p/rage-against-the-machine-ai-paul-kingsnorth"><em>Rage Against the Machine</em></a>. 12 July 2023</p>
<h3 id="broad-overviews">Broad Overviews</h3>
<p>Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, Robert McHardy. <a href="https://arxiv.org/abs/2307.10169"><em>Challenges and Applications of Large Language Models</em></a>. <a href="https://arxiv.org/abs/2307.10169">https://arxiv.org/abs/2307.10169</a></p>
<p>Center for Research on Foundation Models (CRFM) at the Stanford
Institute for Human-Centered Artificial Intelligence (HAI). <a href="https://arxiv.org/abs/2108.07258"><em>On the
Opportunities and Risks of Foundation
Models</em></a> <a href="https://arxiv.org/abs/2108.07258">https://arxiv.org/abs/2108.07258</a>. <a href="https://crfm.stanford.edu/report.html">Report Page</a></p>
<h2 id="copyright-and-law">Copyright and Law</h2>
<p>Katherine Lee, A. Feder Cooper, James Grimmelmann, and Daphne Ippolito. <a href="https://genlaw.github.io/explainers/explainers.pdf"><em>AI and Law: The Next Generation</em></a>. July 2023. <a href="https://genlaw.github.io/explainers/">https://genlaw.github.io/explainers/</a></p>
<p>Inyoung Cheong, Aylin Caliskan, Tadayoshi Kohno. <a href="https://arxiv.org/abs/2308.15906"><em>Is the U.S. Legal System Ready for AI&rsquo;s Challenges to Human Values?</em></a>. 30 August 2023.</p>
<p><a href="https://s3.documentcloud.org/documents/23869693/silverman-openai-complaint.pdf">Sarah Silverman, Christopher Golden, and Richard Kadrey vs. OpenAI</a>. Legal Complaint against ChatGPT, file 7 July 2023.</p>
<p>Nikhil Vyas, Sham Kakade, Boaz Barak. <a href="https://arxiv.org/abs/2302.10870"><em>On Provable Copyright Protection for Generative Models</em></a>. <a href="https://arxiv.org/abs/2302.10870">https://arxiv.org/abs/2302.10870</a>.</p>
<p>Federal Register. <a href="https://www.federalregister.gov/documents/2023/08/30/2023-18624/artificial-intelligence-and-copyright"><em>Artificial Intelligence and Copyright</em></a>. Request for Comment (Published 30 August 2023, Comments due by 15 November 2023)</p>
<h2 id="governance-and-regulation">Governance and Regulation</h2>
<p>Michael Veale, Kira Matus, Robert Gorwa. <a href="https://discovery.ucl.ac.uk/id/eprint/10171121/1/Veale%20Matus%20Gorwa%202023.pdf"><em>AI and Global Governance: Modalities, Rationales, Tensions</em></a>. Annual Review of Law and Social Science, 2023.</p>
<h2 id="amplification-techniques">Amplification Techniques</h2>
<p>Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awada. <a href="https://arxiv.org/abs/2306.02707"><em>Orca: Progressive Learning from Complex Explanation Traces of GPT-4</em></a>. <a href="https://arxiv.org/abs/2306.02707">https://arxiv.org/abs/2306.02707</a></p>
<h2 id="programming-with-llms">Programming with LLMs</h2>
<h2 id="performance-of-llms">Performance of LLMs</h2>
<p>Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin,
Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan
Perez, Evan Hubinger, Kamilė Lukošiūtė, Karina Nguyen, Nicholas
Joseph, Sam McCandlish, Jared Kaplan, Samuel R. Bowman. <a href="https://arxiv.org/abs/2308.03296"><em>Studying
Large Language Model Generalization with Influence
Functions</em></a>. <a href="https://arxiv.org/abs/2308.03296">https://arxiv.org/abs/2308.03296</a></p>
<h2 id="integrations">Integrations</h2>
<p>Ernest Davis, Scott Aaronson. <a href="https://arxiv.org/abs/2308.05713"><em>Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math and science problems</em></a>. <a href="https://arxiv.org/abs/2308.05713">https://arxiv.org/abs/2308.05713</a></p>
<h2 id="hallucination">Hallucination</h2>
<p>Andrew Slavin Ross, Michael C. Hughes, Finale Doshi-Velez. <a href="https://arxiv.org/abs/1703.03717"><em>Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations</em></a>. IJCAI 2017.</p>
<p>Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, Igor Mordatch. <a href="https://arxiv.org/abs/2305.14325"><em>Improving Factuality and Reasoning in Language Models through Multiagent Debate</em></a> <a href="https://arxiv.org/abs/2305.14325">https://arxiv.org/abs/2305.14325</a>.</p>
<h2 id="abuses-of-llms">Abuses of LLMs</h2>
<p>Nicholas Carlini. <a href="https://arxiv.org/abs/2307.15008"><em>A LLM Assisted Exploitation of AI-Guardian</em></a>.</p>
<p>Andy Zou, Zifan Wang, J. Zico Kolter, Matt Fredrikson. <a href="https://arxiv.org/abs/2307.15043"><em>Universal and Transferable Adversarial Attacks on Aligned Language Models</em></a>. <a href="https://arxiv.org/abs/2307.15043">https://arxiv.org/abs/2307.15043</a>.
<a href="https://llm-attacks.org/">Project Website: https://llm-attacks.org/</a>.</p>
<p>Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, Mario Fritz. <a href="https://arxiv.org/abs/2302.12173"><em>Not what you&rsquo;ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection</em></a>. <a href="https://arxiv.org/abs/2302.12173">https://arxiv.org/abs/2302.12173</a>.</p>
<h2 id="fairness-and-bias">Fairness and Bias</h2>
<p>Shangbin Feng, Chan Young Park, Yuhan Liu, Yulia Tsvetkov. <a href="https://arxiv.org/abs/2305.08283"><em>From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models</em></a>. ACL 2023.</p>
<p>Myra Cheng, Esin Durmus, Dan Jurafsky. <a href="https://arxiv.org/abs/2305.18189"><em>Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models</em></a>. ACL 2023.</p>
<h2 id="alignment">&ldquo;Alignment&rdquo;</h2>
<p>Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, Hang Li. <a href="https://arxiv.org/abs/2308.05374"><em>Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models&rsquo; Alignment</em></a>. <a href="https://arxiv.org/abs/2308.05374">https://arxiv.org/abs/2308.05374</a>.</p>
<h2 id="agi">AGI</h2>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang. <a href="https://www.microsoft.com/en-us/research/publication/sparks-of-artificial-general-intelligence-early-experiments-with-gpt-4/"><em>Sparks of Artificial General Intelligence: Early experiments with GPT-4</em></a>. Microsoft, March 2023. <a href="https://arxiv.org/abs/2303.12712">https://arxiv.org/abs/2303.12712</a></p>
<p>Yejin Choi. <a href="https://www.amacad.org/publication/curious-case-commonsense-intelligence"><em>The Curious Case of Commonsense Intelligence</em></a>. Daedalus, Spring 2022.</p>
<p>Konstantine Arkoudas. <a href="https://arxiv.org/abs/2308.03762"><em>GPT-4 Can&rsquo;t Reason</em></a>. <a href="https://arxiv.org/abs/2308.03762">https://arxiv.org/abs/2308.03762</a>.</p>
<p>Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, Vered Shwartz. <a href="https://arxiv.org/abs/2305.14763"><em>Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models</em></a>.</p>
<p>Melanie Sclar, Sachin Kumar, Peter West, Alane Suhr, Yejin Choi, Yulia
Tsvetkov. <a href="https://arxiv.org/abs/2306.00924"><em>Minding Language Models&rsquo; (Lack of) Theory of Mind: A
Plug-and-Play Multi-Character Belief
Tracker</em></a>. ACL 2023</p>
<p>Boaz Barak. <a href="https://windowsontheory.org/2023/07/17/the-shape-of-agi-cartoons-and-back-of-envelope/"><em>The shape of AGI: Cartoons and back of envelope</em></a>. July 2023.</p>
<h2 id="art">Art</h2>
<h2 id="writing">Writing</h2>
<h2 id="human-dignity-and-job-loss">Human Dignity and Job Loss</h2>
<h2 id="cheating">&ldquo;Cheating&rdquo;</h2>
<h2 id="scaling">Scaling</h2>
<h2 id="embeddings">Embeddings</h2>
<h2 id="prompt-engineering-and-jailbreaking">Prompt Engineering and &ldquo;Jailbreaking&rdquo;</h2>
<p>Alexander Wei, Nika Haghtalab, Jacob Steinhardt. <a href="https://arxiv.org/abs/2307.02483"><em>Jailbroken: How Does LLM Safety Training Fail?</em></a>. July 2023.</p>
<h2 id="safety">&ldquo;Safety&rdquo;</h2>
<h2 id="sentience">Sentience</h2>
<p>Patrick Butlin, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan Birch, Axel Constant, George Deane, Stephen M. Fleming, Chris Frith, Xu Ji, Ryota Kanai, Colin Klein, Grace Lindsay, Matthias Michel, Liad Mudrik, Megan A. K. Peters, Eric Schwitzgebel, Jonathan Simon, Rufin VanRullen. <a href="https://arxiv.org/abs/2308.08708"><em>Consciousness in Artificial Intelligence: Insights from the Science of Consciousness</em></a>. August 2023.</p>
<h2 id="memorization-and-inference-privacy">Memorization and Inference Privacy</h2>
<p>R. Thomas McCoy, Paul Smolensky, Tal Linzen, Jianfeng Gao, Asli Celikyilmaz. <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00567/116616/How-Much-Do-Language-Models-Copy-From-Their"><em>How Much Do Language Models Copy From Their Training Data? Evaluating Linguistic Novelty in Text Generation Using RAVEN</em></a>. TACL 2023.</p>
<h2 id="preventing-learning">Preventing Learning</h2>
<p>Shawn Shan, Jenna Cryan, Emily Wenger, Haitao Zheng, Rana Hanocka, Ben
Y. Zhao. <a href="http://people.cs.uchicago.edu/~ravenben/publications/pdf/glaze-usenix23.pdf"><em>Glaze: Protecting Artists from Style Mimicry by
Text-to-Image
Models</em></a>. USENIX Security 2023.  <a href="https://glaze.cs.uchicago.edu/">Glaze Project Website</a> <a href="https://arxiv.org/abs/2302.04222">https://arxiv.org/abs/2302.04222</a></p>
<p>Pedro Sandoval-Segura, Vasu Singla, Jonas Geiping, Micah Goldblum, Tom
Goldstein. <a href="https://arxiv.org/abs/2305.19254"><em>What Can We Learn from Unlearnable
Datasets?</em></a>. <a href="https://arxiv.org/abs/2305.19254">https://arxiv.org/abs/2305.19254</a></p>
<p>Giannis Daras, Kulin Shah, Yuval Dagan, Aravind Gollakota, Alexandros
G. Dimakis, Adam Klivans.  <a href="https://arxiv.org/abs/2305.19256"><em>Ambient Diffusion: Learning Clean
Distributions from Corrupted Data</em></a>. <a href="https://arxiv.org/abs/2305.19256">https://arxiv.org/abs/2305.19256</a></p>
<h2 id="training-on-generated-data">Training on Generated Data</h2>
<p>Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, Ross Anderson. <a href="https://arxiv.org/abs/2305.17493"><em>The Curse of Recursion: Training on Generated Data Makes Models Forget</em></a>. <a href="https://arxiv.org/abs/2305.17493">https://arxiv.org/abs/2305.17493</a></p>
<p>Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, Richard G. Baraniuk. <a href="https://arxiv.org/abs/2307.01850"><em>Self-Consuming Generative Models Go MAD</em></a>. <a href="https://arxiv.org/abs/2307.01850">https://arxiv.org/abs/2307.01850</a>.</p>
<h2 id="environmental-harms">Environmental Harms</h2>
<h2 id="evaluating-llms">Evaluating LLMs</h2>
<p>Percy Liang, et al. <a href="https://arxiv.org/abs/2211.09110"><em>Holistic Evaluation of Language Models</em></a>. <a href="https://crfm.stanford.edu/helm/v0.2.2/">HELM Project</a>.</p>
<h2 id="useful-guides">Useful Guides</h2>
<p><a href="https://www.oneusefulthing.org/p/how-to-use-ai-to-do-stuff-an-opinionated"><em>How to Use AI to Do Stuff: An Opinionated Guide</em></a> (Ethan Mollick)</p>
<p><a href="https://github.com/openai/openai-cookbook">OpenAI Cookbook</a></p>
<h3 id="more-sources">More Sources</h3>
<p><a href="https://github.com/Hannibal046/Awesome-LLM">Awesome-LLM: a curated list of Large Language Model Papers and Links</a></p>
<p><a href="https://llmsecurity.net/">LLM Security</a> — collection of papers on LLM security</p>
<p><a href="https://www.cs.princeton.edu/courses/archive/fall22/cos597G/">COS 597G (Fall 2022): Understanding Large Language Models</a> (Princeton Course taught by <a href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a></p>
<p><a href="https://stanford-cs324.github.io/winter2022/">CS324 - Large Language Models (Winter 2022)</a> (Stanford Course taught by <a href="https://cs.stanford.edu/~pliang/">Percy Liang</a>, <a href="https://thashim.github.io/">
Tatsunori Hashimoto</a>, and <a href="https://cs.stanford.edu/~chrismre/">Christopher Ré</a>)</p>

      </div>

      <meta itemprop="wordCount" content="1132">
      <meta itemprop="datePublished" content="2023-08-21">
      <meta itemprop="url" content="https://llmrisks.github.io/readings/">
    </article>
</div>


    </main>
    
    
<footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-12 medium-6">
      <a href="//llmrisks.github.io"><b>cs 6501: Risks (and Benefits) of Generative AI</b></a><br>
      Fall 2023<br>
      <a href="//www.cs.virginia.edu">University of Virginia</a>
    </div>
    <div class="column small-14 medium-5">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="https://llmrisks.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
  </hr>
</footer>


    
    <div class="endofpage">
    </div>

    <script src="/js/jquery.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
