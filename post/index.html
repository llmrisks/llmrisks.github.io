<!doctype html>
<html class="no-js" lang="en-us">
  <head>
    <meta charset="utf-8">
    <title>Posts | Risks (and Benefits) of Generative AI and Large Language Models</title>
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <link rel="stylesheet" href="https://llmrisks.github.io/css/foundation.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/fonts.css">
    <link rel="stylesheet" href="https://llmrisks.github.io/css/finite.css">
    <link rel="shortcut icon" href="/images/uva.png">  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>

    
  </head>
  <body>
      
    <header>
      <nav class="nav-bar">
	
	  <div class="title-bar" data-responsive-toggle="site-menu" data-hide-for="medium">	      
	    <button class="site-hamburger" type="button" data-toggle>
	      <i class="fa fa-bars fa-lg" aria-hidden="true"></i>
	    </button>
	    <div class="title-bar-title site-title">
	      <a href="https://llmrisks.github.io/">Risks (and Benefits) of Generative AI and Large Language Models</a>
	    </div>
	    <div class="title-bar-right pull-right">
	      
	      
	    </div>
	  </div>
	    
	  
	    <div class="top-bar" id="site-menu" >	      
	      <div class="top-bar-title show-for-medium site-title">
		<a href="https://llmrisks.github.io/">Risks (and Benefits) of Generative AI and Large Language Models</a>
	      </div>
	      <div class="top-bar-left">
		<ul class="menu vertical medium-horizontal">
		  
		  
		</ul>
	      </div>
	      <div class="top-bar-right show-for-medium">
		
	         <p class="groupstyle">Computer Science</br>University of Virginia</p>
		
	      </div>
	    </div>
	  
	</nav>
      
    </header>
    
    <main>
      

<div class="container">
<div class="content">
<div class="row">
    
    
    <h2><a href="/week14b/">Week 14b: Ethical AI</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-12-04 00:00:00 &#43;0000 UTC" itemprop="datePublished">4 December 2023</time>
  </span>
  
</div>


Presenting Team: Aparna Kishore, Elena Long, Erzhen Hu, Jingping Wan
Blogging Team: Haolin Liu, Haochen Liu, Ji Hyun Kim, Stephanie Schoch, Xueren Ge
Note: since the topics were unrelated, Week 14 is split into two posts:
Monday, November 27: Multimodal Models Wednesday, November 29: Ethical AI Wednesday, November 29: Ethical AI Ben Shneiderman. Bridging the Gap Between Ethics and Practice: Guidelines for Reliable, Safe, and Trustworthy Human-centered AI Systems. ACM Transactions on Interactive Intelligent Systems, October 2020.
<p class="text-right"><a href="/week14b/">Read More…</a></p>
	

    
    <h2><a href="/week14a/">Week 14a: Multimodal Models</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-12-03 00:00:00 &#43;0000 UTC" itemprop="datePublished">3 December 2023</time>
  </span>
  
</div>


Presenting Team: Aparna Kishore, Elena Long, Erzhen Hu, Jingping Wan
Blogging Team: Haolin Liu, Haochen Liu, Ji Hyun Kim, Stephanie Schoch, Xueren Ge
Note: since the topics were unrelated, Week 14 is split into two posts:
Monday, November 27: Multimodal Models Wednesday, November 29: Ethical AI Monday, November 27: Multimodal Models Today&rsquo;s topic is how to improve model performance by combining multiple modes.
We will first introduce the multimodal foundations and then center around CLIP, which is the most famous vision-language model.
<p class="text-right"><a href="/week14a/">Read More…</a></p>
	

    
    <h2><a href="/week13/">Week 13: Regulating Dangerous Technologies</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-11-20 00:00:00 &#43;0000 UTC" itemprop="datePublished">20 November 2023</time>
  </span>
  
</div>


The slides are here: Regulating Dangerous Technologies (I&rsquo;ve included some slides in the posted slides that I didn&rsquo;t present in class but you might find interesting, including some excerpts from a talk I gave in 2018 on Mutually Assured Destruction and the Impending AI Apocalypse.)
Since one of the groups made the analogy to tobacco products, I also will take the liberty of pointing to a talk I gave at Google making a similar analogy: The Dragon in the Room.
<p class="text-right"><a href="/week13/">Read More…</a></p>
	

    
    <h2><a href="/week12/">Week 12: LLM Agents</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-11-16 00:00:00 &#43;0000 UTC" itemprop="datePublished">16 November 2023</time>
  </span>
  
</div>


Presenting Team: Liu Zhe, Peng Wang, Sikun Guo, Yinhan He, Zhepei Wei
Blogging Team: Anshuman Suri, Jacob Christopher, Kasra Lekan, Kaylee Liu, My Dinh
Monday, November 13: LLM Agents LLM agents are the &ldquo;next big thing&rdquo;, with the potential to directly impact important fields like healthcare and education. Essentially, they are LLM-based systems that have the ability to use external tools, such as Internet browsing access and calculators, to augment their abilities.
<p class="text-right"><a href="/week12/">Read More…</a></p>
	

    
    <h2><a href="/week11/">Week 11: Watermarking on Generative Models</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-11-13 00:00:00 &#43;0000 UTC" itemprop="datePublished">13 November 2023</time>
  </span>
  
</div>


Presenting Team: Tseganesh Beyene Kebede, Zihan Guan, Xindi Guo, Mengxuan Hu
Blogging Team: Ajwa Shahid, Caroline Gihlstorf, Changhong Yang, Hyeongjin Kim, Sarah Boyce
Monday, November 6: Watermarking LLM Outputs Recent instances of AI-generated text passing for human text and the writing of students being misattributed to AI suggest the need for a tool to distinguish between human-written and AI-generated text. The presenters also noted that the increase in the amount of AI-generated text online is a risk for training future LLMs on this data.
<p class="text-right"><a href="/week11/">Read More…</a></p>
	

    
    <h2><a href="/week10/">Week 10: Data Selection for LLMs</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-11-03 00:00:00 &#43;0000 UTC" itemprop="datePublished">3 November 2023</time>
  </span>
  
</div>


(see bottom for assigned readings and questions)
Presenting Team: Haolin Liu, Xueren Ge, Ji Hyun Kim, Stephanie Schoch Blogging Team: Aparna Kishore, Elena Long, Erzhen Hu, Jingping Wan
Monday, 30 October: Data Selection for Fine-tuning LLMs Question: Would more models help? We&rsquo;ve discussed so many risks and issues of GenAI so far and one question is that it can be difficult for us to come up with a possible solution to these problems.
<p class="text-right"><a href="/week10/">Read More…</a></p>
	

    
    <h2><a href="/week9/">Week 9: Interpretability</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-10-30 00:00:00 &#43;0000 UTC" itemprop="datePublished">30 October 2023</time>
  </span>
  
</div>


(see bottom for assigned readings and questions)
Presenting Team: Anshuman Suri, Jacob Christopher, Kasra Lekan, Kaylee Liu, My Dinh
Blogging Team: Hamza Khalid, Liu Zhe, Peng Wang, Sikun Guo, Yinhan He, Zhepei Wei
Monday, 23 October: Interpretability: Overview, Limitations, &amp; Challenges Definition of Interpretability Interpretability in the context of artificial intelligence (AI) and machine learning refers to the extent to which a model&rsquo;s decisions, predictions, or internal workings can be understood and explained by humans.
<p class="text-right"><a href="/week9/">Read More…</a></p>
	

    
    <h2><a href="/week8/">Week 8: Machine Translation</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-10-22 00:00:00 &#43;0000 UTC" itemprop="datePublished">22 October 2023</time>
  </span>
  
</div>


(see bottom for assigned readings and questions)
Machine Translation (Week 8) Presenting Team: Ajwa Shahid, Caroline Gihlstorf, Changhong Yang, Hyeongjin Kim, Sarah Boyce
Blogging Team: Xindi Guo, Mengxuan Hu, Tseganesh Beyene Kebede, Zihan Guan
Monday, 16 Oct: Diving into the History of Machine Translation Let&rsquo;s kick off this topic with an activity that involves translating an English sentence into a language of your choice and subsequently composing pseudocode to describe the process.
<p class="text-right"><a href="/week8/">Read More…</a></p>
	

    
    <h2><a href="/week7/">Week 7: GANs and DeepFakes</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-10-16 00:00:00 &#43;0000 UTC" itemprop="datePublished">16 October 2023</time>
  </span>
  
</div>


(see bottom for assigned readings and questions)
Presenting Team: Aparna Kishore, Elena Long, Erzhen Hu, Jingping Wan Blogging Team: Haochen Liu, Haolin Liu, Ji Hyun Kim, Stephanie Schoch, Xueren Ge Monday, 9 October: Generative Adversarial Networks and DeepFakes Today's topic is how to utilize generative adversarial networks to create fake images and how to identify the images generated by these models.
Generative Adversarial Network (GAN) is a revolutionary deep learning framework that pits two neural networks against each other in a creative showdown.
<p class="text-right"><a href="/week7/">Read More…</a></p>
	

    
    <h2><a href="/week5/">Week 5: Hallucination</a></h2>
<div class="post-metadata">
  <span class="post-date">
    <time datetime="2023-10-04 00:00:00 &#43;0000 UTC" itemprop="datePublished">4 October 2023</time>
  </span>
  
</div>


(see bottom for assigned readings and questions)
Hallucination (Week 5) Presenting Team: Liu Zhe, Peng Wang, Sikun Guo, Yinhan He, Zhepei Wei
Blogging Team: Anshuman Suri, Jacob Christopher, Kasra Lekan, Kaylee Liu, My Dinh
Wednesday, September 27th: Intro to Hallucination People Hallucinate Too Hallucination Definition There are three types of hallucinations according to the “Siren's Song in the AI Ocean” paper: Input-conflict: This subcategory of hallucinations deviates from user input. Context-conflict: Context-conflict hallucinations occur when a model generates contradicting information within a response.
<p class="text-right"><a href="/week5/">Read More…</a></p>
	

    
    <div class="row">
  <div class="column small-12">
    <ul class="pagination" role="navigation" aria-label="Pagination">
      
      <li class="arrow" aria-disabled="true"><a href="/post/page/2/">&laquo; <em>Older<span class="show-for-sr"> blog entries</span></em></a></li>
      
      <li><span>Page 1 of 2</span></li>      
      
    </ul>    
    All Posts by <a href="https://llmrisks.github.io/categories">Category</a> or <a href="https://llmrisks.github.io/tags">Tags</a>.

  </div>
</div>

</div>
</div>
</div>

    </main>
    
    
    <footer class="whatisthis">
  <hr />
  <div class="row">
    <div class="column small-12 medium-6">
      <a href="//llmrisks.github.io"><b>cs 6501: Risks (and Benefits) of Generative AI</b></a><br>
      Fall 2023<br>
      <a href="//www.cs.virginia.edu">University of Virginia</a>
    </div>
    <div class="column small-14 medium-5">
      <font size="-1">
      Subscribe to
	the <a href="/index.xml"><i class="fa fa-rss-square"></i>&nbsp;RSS feed</a>.
      <a id="searchsite">
	<form method="get" action="https://duckduckgo.com/">
	  <label for="search-field" class="show-for-sr">Search with DuckDuckGo</label>
	  <input type="search" name="q" maxlength="255" placeholder="Search with DuckDuckGo" id="search-field">
	  <input type="hidden" name="sites" value="https://llmrisks.github.io/"/>
	  <input type="hidden" name="k7" value="#faf8f8"/>
	  <input type="hidden" name="kj" value="#b33"/>
	  <input type="hidden" name="ky" value="#fafafa"/>
	  <input type="hidden" name="kx" value="b"/>
	  <input type="hidden" name="ko" value="-1"/>
	  <input type="hidden" name="k1" value="-1"/>
	  <input type="submit" value="DuckDuckGo Search" style="visibility: hidden;" />
	</form>
      </a>
</font>
    </div>
  </div>
  </hr>
</footer>

    
    
    <div class="endofpage">
    </div>

    <script src="/js/jquery.js"></script>
    <script src="/js/what-input.js"></script>
    <script src="/js/foundation.min.js"></script>
    <script src="/js/finite.js"></script>

    
    <script src="/js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    
    
  </body>
</html>
